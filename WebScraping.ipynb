{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "jWMRXtMFh2do",
        "outputId": "ae52c2ce-0928-4b3d-a8dd-6d2da4062122"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"0542c77e-1e3e-4757-8747-4a314124933e-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"0542c77e-1e3e-4757-8747-4a314124933e-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            const element = document.getElementById(\"0542c77e-1e3e-4757-8747-4a314124933e-circle\");\n            element.style.border = \"3px solid limegreen\";\n            element.style.animation = \"none\";\n\n            const text = document.getElementById(\"0542c77e-1e3e-4757-8747-4a314124933e-text\");\n            text.innerText = \"Initialized Chromedriver\";\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install -q google-colab-selenium\n",
        "import google_colab_selenium as gs\n",
        "driver = gs.Chrome()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbO93B9AMWaO",
        "outputId": "a3d45935-c0b6-40a4-ebd1-b9e1f56a7cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.10/dist-packages (4.27.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Requirement already satisfied: trio~=0.17 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.28.0)\n",
            "Requirement already satisfied: trio-websocket~=0.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (0.11.1)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Requirement already satisfied: outcome in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Requirement already satisfied: wsproto>=0.14 in /usr/local/lib/python3.10/dist-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KJDi4sm3xGW1",
        "outputId": "b29b6e24-0348-4c72-e433-4b38d2040c7a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div class=\"spinner-container\">\n",
              "                <div class=\"spinner\" id=\"d80894bf-813e-4ec0-a14d-7bc2e6e5478e-circle\"></div>\n",
              "                <div class=\"spinner-text\" id=\"d80894bf-813e-4ec0-a14d-7bc2e6e5478e-text\">Initializing Chromedriver</div>\n",
              "            </div>\n",
              "            <style>\n",
              "                @keyframes spin {\n",
              "                    from { transform: rotate(0deg); }\n",
              "                    to { transform: rotate(360deg); }\n",
              "                }\n",
              "\n",
              "                .spinner-container {\n",
              "                    display: flex;\n",
              "                    align-items: center;\n",
              "                    margin-bottom: 3px;\n",
              "                }\n",
              "\n",
              "                .spinner {\n",
              "                    border: 3px solid rgba(0, 0, 0, 0.1);\n",
              "                    border-left-color: lightblue;\n",
              "                    border-radius: 50%;\n",
              "                    width: 12px;\n",
              "                    height: 12px;\n",
              "                    animation: spin 1s linear infinite;\n",
              "                }\n",
              "\n",
              "                .spinner-text {\n",
              "                    padding-left: 6px;\n",
              "                }\n",
              "            </style>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "\n            const element = document.getElementById(\"d80894bf-813e-4ec0-a14d-7bc2e6e5478e-circle\");\n            element.style.border = \"3px solid limegreen\";\n            element.style.animation = \"none\";\n\n            const text = document.getElementById(\"d80894bf-813e-4ec0-a14d-7bc2e6e5478e-text\");\n            text.innerText = \"Initialized Chromedriver\";\n        ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Link: https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581\n",
            "Replies: 0\n",
            "Views: 811\n",
            "Activity: Created: May 6, 2022 1:06 am\n",
            "Topic Owner: msamogh\n",
            "Topic Content: I’ve exported a custom PyTorch-based Transformer model into ONNX to run it on NodeJS. However, the exported model seems to expect input_ids directly (and not raw text).\n",
            "Is there any way I can perform tokenization in JS?\n",
            "Topic Timestamp: May 6, 2022 1:06 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Make correct padding for text generation with GPT-NEO\n",
            "Link: https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800\n",
            "Replies: 0\n",
            "Views: 795\n",
            "Activity: Created: Jul 5, 2023 9:57 pm\n",
            "Latest: Jul 5, 2023 10:16 pm\n",
            "Topic Owner: junoriosity\n",
            "Topic Content: In order to make generate text sequences with GPT-NEO, I first load all the relevant components for sequence generation for GPTNeoForCausalLM.\n",
            "from transformers import AutoTokenizer, GPTNeoForCausalLM\n",
            "import torch\n",
            "from torch.nn import functional as F\n",
            "\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
            "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125m\")\n",
            "There are two ways how I can generate input_ids and attention_mask.\n",
            "I take the standard approach without padding\n",
            "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
            "I use padding instead\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "tokenizer.padding_side = 'left'\n",
            "tokenizer.truncation_side = 'left'\n",
            "no_items_for_history = 30\n",
            "\n",
            "inputs = tokenizer.encode_plus(\"Hello, my dog is cute\", max_length=no_items_for_history, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
            "Then for both approaches, I iteratively loop through everything in order generate the sequence on token at a time.\n",
            "input_ids = inputs['input_ids']\n",
            "attention_mask = inputs['attention_mask']\n",
            "\n",
            "\n",
            "for i in range(10):\n",
            "    if i == 0:\n",
            "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=inputs[\"input_ids\"])\n",
            "    else:\n",
            "        outputs = model(input_ids=new_input_ids, attention_mask=attention_mask, past_key_values=past_key_values)\n",
            "    loss = outputs.loss\n",
            "    logits = outputs.logits[:, -1, :]\n",
            "\n",
            "    logits = F.softmax(logits, dim=1)\n",
            "\n",
            "    topk_values, topk_indices = torch.topk(logits, 5)\n",
            "    inputs_in_topk = torch.multinomial(topk_values, num_samples=1, replacement=True)\n",
            "    new_input_ids = torch.gather(topk_indices, 1, inputs_in_topk)\n",
            "\n",
            "    past_key_values = outputs.past_key_values\n",
            "    attention_mask = torch.concat((attention_mask, torch.ones(1, 1).to(attention_mask.device)), dim=1)\n",
            "    input_ids = torch.concat((input_ids, new_input_ids), dim=1)\n",
            "\n",
            "\n",
            "print(tokenizer.decode(input_ids.tolist()[0], skip_special_tokens=True))\n",
            "Here is the problem:\n",
            "The starting input_ids and attention_mask for the first approach look like:\n",
            "input_ids = tensor([[15496,    11,   616,  3290,   318, 13779]])\n",
            "attention_mask = tensor([[1, 1, 1, 1, 1, 1]])\n",
            "The output looks very sensible:\n",
            "Hello, my dog is cute! This post is about dogs and cats\n",
            "However, for the second approach the starting input_ids and attention_mask look like\n",
            "input_ids = tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 15496,    11,   616,  3290,   318, 13779]])\n",
            "attention_mask = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]])\n",
            "and it always generates nonsense like\n",
            "Hello, my dog is cute pet is my pet pet pet is my dog is\n",
            "Question: Do you know how to make it work with padding, i.e., the second approach?\n",
            "Topic Timestamp: Jul 5, 2023 9:57 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to convert HuggingFace tokenizers into ONNX format?\n",
            "Link: https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272\n",
            "Replies: 1\n",
            "Views: 540\n",
            "Activity: Created: Dec 5, 2022 3:14 pm\n",
            "Latest: Dec 5, 2022 9:42 pm\n",
            "Topic Owner: yerma\n",
            "Topic Content: I have faced with a problem, I am trying to convert BertClassification model into ONNX format but, I also have to carry out about preprocessing step , which means my Tokenizer also must be converted to ONNX , is it even possible ?\n",
            "Topic Timestamp: Dec 5, 2022 3:14 pm\n",
            "Replies:\n",
            "  - Username: lianghsun\n",
            "    Answer: According to Export to ONNX , I think it only allow you transform the model to ONNX format not tokenizer.\n",
            "    Timestamp: Dec 5, 2022 9:42 pm\n",
            "----------------------------------------\n",
            "Title: Period ID in RobertaTokenizer with is_split_into_words\n",
            "Link: https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142\n",
            "Replies: 1\n",
            "Views: 531\n",
            "Activity: Created: Oct 8, 2022 2:23 pm\n",
            "Latest: Oct 27, 2022 1:06 pm\n",
            "Topic Owner: shon711\n",
            "Topic Content: Hello,\n",
            "I have an issue when I using is_split_into_words flag.\n",
            "This is a two sentences text. The period ID is different.\n",
            "import spacy\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True, add_prefix_space=True)\n",
            "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tagger\", \"parser\", \"lemmatizer\", \"ner\", \"textcat\"])\n",
            "\n",
            "text = \"This is a two sentences text. The period ID is different.\"\n",
            "\n",
            "spacy_tokenized = [tok.text for tok in nlp(text)]\n",
            "\n",
            "print(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
            "> [713, 16, 10, 80, 11305, 2788, 4, 20, 675, 4576, 16, 430, 4]\n",
            "\n",
            "print(tokenizer(spacy_tokenized, add_special_tokens=False, is_split_into_words=True)['input_ids'])\n",
            "> [152, 16, 10, 80, 11305, 2788, 479, 20, 675, 4576, 16, 430, 479]\n",
            "As you can see, the period ID is different, when is_split_into_words=True the ID is 479 and 4 otherwise.\n",
            "Now I know that this is expected behavior, the tokenizer adds a space before each word when is_split_into_words=True so technically these are 2 different entries in the vocabulary.\n",
            "But I don’t need this extra space when decoding, Any workaround or suggestion to solve it?\n",
            "Thanks in advance,\n",
            "Shon\n",
            "Topic Timestamp: Oct 8, 2022 2:23 pm\n",
            "Replies:\n",
            "  - Username: lianghsun\n",
            "    Answer: Hi @shon711, I think the problem may be the Spacy, please take a look at the following result\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained('roberta-base',add_prefix_space=True)\n",
            "\n",
            "text = \"This is a two sentences text. The period ID is different.\"\n",
            "_text = text.split() # Replace spacy for testing\n",
            "\n",
            "print(tokenizer(text).input_ids)\n",
            "print(tokenizer(_text, is_split_into_words=True).input_ids)\n",
            "# > the results is same!\n",
            "So, I think you should modify spacy_tokenized to be same format as _text, then you will get the same result\n",
            "    Timestamp: Oct 27, 2022 1:06 pm\n",
            "----------------------------------------\n",
            "Title: Tokenizer tend to choose added tokens first rather than token in vocab\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965\n",
            "Replies: 1\n",
            "Views: 530\n",
            "Activity: Created: Nov 29, 2023 8:02 am\n",
            "Latest: Nov 30, 2023 4:13 am\n",
            "Topic Owner: Hakase-Noonna\n",
            "Topic Content: Hello, I am working on customizing a tokenizer.\n",
            "Tokenizer.json structure looks like this.\n",
            "{version,\n",
            "truncation,\n",
            "padding,\n",
            "added_tokens : [\n",
            "    {added_token1},\n",
            "    {added_token2},\n",
            "    ],\n",
            "normalizer,\n",
            "pre_tokenizer,\n",
            "post_processor,\n",
            "model : {..., ..., \n",
            "    vocab : {a : 0,\n",
            "                  b : 1,\n",
            "                  c : 2, ...}\n",
            "         }\n",
            "}\n",
            "So I build it up from scratch using BPE model. Below is the code.\n",
            "class SettingTokenizer:\n",
            "    \n",
            "    @staticmethod\n",
            "    def set_tokenizer_and_trainer():\n",
            "        tokenizer = Tokenizer(models.BPE())\n",
            "        tokenizer.normalizer = normalizers.Sequence(\n",
            "            [normalizers.BertNormalizer(strip_accents=True), normalizers.Replace(\"\\\\r\\\\n\", \" \")]\n",
            "        )\n",
            "        tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
            "        tokenizer.decoder = decoders.ByteLevel()\n",
            "        trainer = trainers.BpeTrainer(\n",
            "            vocab_size=100000,\n",
            "            min_frequency=10,\n",
            "            initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),\n",
            "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"],\n",
            "        )\n",
            "        return tokenizer, trainer\n",
            "Everything seems fine to me at first train step.\n",
            "Let’s say I trained a new empty tokenizer with 10,000 train data.\n",
            "And I got the vocab with size 300. (No added tokens yet)\n",
            "The next day I trained with other 5,000 train data.\n",
            "The new tokens are updated in added_tokens.\n",
            "The following image explains the logic I implemented.\n",
            "The problem I am having now is that loaded tokenizer seems to use tokens in added_tokens first rather than tokens in vocab.\n",
            "For example, a word “request” is tokenized into “requ” and “est” (input_ids = [1336, 400] )\n",
            "But the tokenizer could use the token “reques”,“t” or “request”.\n",
            "The input_ids would be [265,88] or [266].\n",
            "As you can see the above image, the token “request” is already in vocab. (“requ” and “est” are in the added_tokens)\n",
            "Why tokenizer prefer added_tokens to vocab?\n",
            "How can I set this tokenizer to use vocab tokens first?\n",
            "Thx!!\n",
            "Topic Timestamp: Nov 29, 2023 8:02 am\n",
            "Replies:\n",
            "  - Username: Hakase-Noonna\n",
            "    Answer: I find a trick.\n",
            "Instead of just using list of new tokens for the function add_token, Use with AddedToken class\n",
            "like this.\n",
            "at3 = AddedToken('est', single_word=True)\n",
            "You can use some arguments in there. I choose single_word as True.\n",
            "If it is False, then it would tokenize “West” and “Request” into [“w”, “est”] and [“requ”, “est”].\n",
            "But if it is True, then it would not tokenize into seperate tokens.\n",
            "This is not the solution, but I am good with this.\n",
            "GLTA!\n",
            "    Timestamp: Nov 30, 2023 4:13 am\n",
            "----------------------------------------\n",
            "Title: Version incompatibility between transformers and tokenizers\n",
            "Link: https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843\n",
            "Replies: 0\n",
            "Views: 742\n",
            "Activity: Created: May 22, 2024 1:23 pm\n",
            "Topic Owner: PeteBleackley\n",
            "Topic Content: I keep getting the following error message.\n",
            "    from transformers import AutoTokenizer\n",
            "  File \"/home/peter/.local/lib/python3.10/site-packages/transformers/__init__.py\", line 30, in <module>\n",
            "    from . import dependency_versions_check\n",
            "  File \"/home/peter/.local/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 41, in <module>\n",
            "    require_version_core(deps[pkg])\n",
            "  File \"/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py\", line 122, in require_version_core\n",
            "    return require_version(requirement, hint)\n",
            "  File \"/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py\", line 116, in require_version\n",
            "    _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)\n",
            "  File \"/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py\", line 49, in _compare_versions\n",
            "    raise ImportError(\n",
            "ImportError: tokenizers>=0.11.1,!=0.11.3,<0.13 is required for a normal functioning of this module, but found tokenizers==0.19.1.\n",
            "Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git main\n",
            "I’ve tried reinstalling both transformers and tokenizers, but this doesn’t help. It seems strange that, given that both transformers and tokenizers come from HuggingFace, one should be incompatible with the other. Can anyone tell me what’s wrong and how to fix it?\n",
            "Topic Timestamp: May 22, 2024 1:23 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to add a new token without expanding the vocabulary\n",
            "Link: https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536\n",
            "Replies: 0\n",
            "Views: 739\n",
            "Activity: Created: Mar 24, 2023 4:19 am\n",
            "Topic Owner: jingyey\n",
            "Topic Content: Hi all, I had a question regarding making changes to the tokenizer’s vocabulary. I want to register a new token’s id in the vocab, but I don’t want to add a new token and expand the size of the vocab.\n",
            "Specifically, assume we are working with tokenizer from bert-base-cased. Now I want to register “qazwsx” to id 1, which is occupied by “[unused1]”. So I want to replace “[unused1]” : 1 by “qazwsx” : 1. Do you know how to achieve this?\n",
            "Another question is how to synchronize two tokens with the same id, instead of overwrite it as above. For example, instead of replacing “[unused1]” : 1 by “qazwsx” : 1, I want to keep both in the new vocab, so that both of them will be tokenized as 1.\n",
            "Thank you so much for the help!\n",
            "Topic Timestamp: Mar 24, 2023 4:19 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Option to load only tokenizer and model configuration into “token-classification” pipeline\n",
            "Link: https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697\n",
            "Replies: 0\n",
            "Views: 738\n",
            "Activity: Created: Nov 25, 2022 2:55 pm\n",
            "Topic Owner: Leoprat\n",
            "Topic Content: Hi everyone I stumbled into this issue integrating Nvidia triton and a NER model I trained.\n",
            "For practical purpose the triton inference server is receiving the tokenized text and returning the logits.\n",
            "So on the client I want to use a token classification pipeline, in particular:\n",
            "1 - I want to use the pipeline.preprocess() to encode the text\n",
            "2 - I don’t need to use the pipeline.forward() because the triton server is doing the inference and return the logits to the client.\n",
            "3 - I want to use the pipeline.postprocess() to retrieve the entites\n",
            "The issue with this configuration is that I must load the full model locally even though I’m not using it.\n",
            "I would like to load a pipeline composed only of preprocess and postprocess.\n",
            "Calling:\n",
            "token_classifier = pipeline(\n",
            "        \"token-classification\", model=model_checkpoint)\n",
            "without giving it the model (only the tokenizer config) results in an error.\n",
            "Topic Timestamp: Nov 25, 2022 2:55 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Simple Transformers Multilabelclassification\n",
            "Link: https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568\n",
            "Replies: 1\n",
            "Views: 520\n",
            "Activity: Created: Oct 17, 2022 4:30 pm\n",
            "Latest: Oct 18, 2022 8:18 am\n",
            "Topic Owner: Jamesyyjj\n",
            "Topic Content: Hi there, I am having trouble importing DistilBertForMultilabelSequenceClassification\n",
            "In the attached two pictures you can see I am having no issues importing the AutoModelForSequenceClassification but when I try and import the DistilBertForMultilabelSequenceClassification I get:\n",
            "ImportError Traceback (most recent call last)\n",
            "Cell In [3], line 1\n",
            "----> 1 from transformers import AutoTokenizer, DistilBertForMultilabelSequenceClassification\n",
            "3 tokenizer = AutoTokenizer.from_pretrained(“bhadresh-savani/bert-base-go-emotion”)\n",
            "5 model = DistilBertForMultilabelSequenceClassification.from_pretrained(“bhadresh-savani/bert-base-go-emotion”)\n",
            "ImportError: cannot import name ‘DistilBertForMultilabelSequenceClassification’ from ‘transformers’ (C:\\Users\\name\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers_init_.py)\n",
            "Topic Timestamp: Oct 17, 2022 4:30 pm\n",
            "Replies:\n",
            "  - Username: nielsr\n",
            "    Answer: Hi,\n",
            "the correct way to instantiate a DistilBERT model for multi-label text classification is as follows:\n",
            "from transformers import DistilBertForSequenceClassification\n",
            "\n",
            "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", problem_type=\"multi_label_classification\", num_labels=10)\n",
            "    Timestamp: Oct 18, 2022 8:18 am\n",
            "----------------------------------------\n",
            "Title: Adding tokens, but tokenizer doesn’t use them\n",
            "Link: https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674\n",
            "Replies: 1\n",
            "Views: 292\n",
            "Activity: Created: Mar 25, 2024 11:18 am\n",
            "Latest: Aug 14, 2024 12:15 pm\n",
            "Topic Owner: WonderYear1905\n",
            "Topic Content: Hi, Im trying to add tokens to a pretrained tokenizer.\n",
            "First I initialized the tokenizer:\n",
            "tokenizer = AutoTokenizer.from_pretrained(“mistralai/Mistral-7B-Instruct-v0.2”)\n",
            "Next I created a training iterator and trained a new tokenizer:\n",
            "training_corpus = get_training_corpus()\n",
            "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus,vocab_size=10000)#,10000)\n",
            "Taking the diff:\n",
            "tokens_to_add = list(set(new_tokenizer.vocab.keys())- set(tokenizer.vocab.keys()))\n",
            "output = tokenizer.add_tokens(tokens_to_add)\n",
            "tokenizer is updated, I can see its now in the correct size (i.e original size=32000+ added tokens), I can also see the new token under added_tokens_decoder and added_tokens_encoder. Everything seems great.\n",
            "But, when Im trying to tokenize my input data:\n",
            "tokenizer.tokenize(x, return_tensors=“pt”)\n",
            "The tokenizer just doesnt use the new tokens.\n",
            "Any idea what Im doing wrong?!\n",
            "Thanks!\n",
            "Topic Timestamp: Mar 25, 2024 11:18 am\n",
            "Replies:\n",
            "  - Username: Butanium\n",
            "    Answer: Just experienceed the same issue with llama2-7b, did you find a solution to that?\n",
            "In my case adding normalized=False fixed the issue I had on my single example.\n",
            "    Timestamp: Aug 14, 2024 11:57 am\n",
            "----------------------------------------\n",
            "Title: Trained tokenizer API as PretrainedTokenizer\n",
            "Link: https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639\n",
            "Replies: 1\n",
            "Views: 520\n",
            "Activity: Created: Sep 26, 2022 4:35 pm\n",
            "Latest: Oct 26, 2022 9:21 am\n",
            "Topic Owner: AfonsoSousa\n",
            "Topic Content: Hi. I have trained a BPE Tokenizer successfully using:\n",
            " tokenizer = Tokenizer(BPE(unk_token=unk_token))\n",
            " trainer = BpeTrainer(special_tokens=special_tokens)\n",
            " tokenizer.train(text_files, trainer)\n",
            "But I lack things like tokenizer.pad_token to access the padding token or len(tokenizer) to get the vocabulary size. Is this the best way to train a BPE tokenizer? How can I get the same API as PretrainedTokenizer? Thanks in advance for any help you can provide.\n",
            "Topic Timestamp: Sep 26, 2022 4:35 pm\n",
            "Replies:\n",
            "  - Username: lianghsun\n",
            "    Answer: Not pretty sure what questions you met, so I just explain how to deal with padding and get vocab size here:\n",
            "Padding: the reason we set [PAD] in training phase is to tell tokenizer that we might have a special token there, but we do not tell the function about each special token at moment. Then after the training you finally get a trained tokenizer instance, then you can specify tokenizer.enable_padding(pad_id=tokenizer.tokens_to_id('[PAD]')) to tell the tokenizer that we want to set [PAD] to the roll of the padding token which we have already setup in vocab list before. After setting up .enable_padding(), the tokenized sentense might contain padding.\n",
            "Vocab size: tokenizer.get_vocab_size()\n",
            "    Timestamp: Oct 25, 2022 8:29 pm\n",
            "----------------------------------------\n",
            "Title: Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model\n",
            "Link: https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829\n",
            "Replies: 0\n",
            "Views: 726\n",
            "Activity: Created: Jul 14, 2023 1:49 pm\n",
            "Topic Owner: RCarroll\n",
            "Topic Content: I’m currently working on a project that involves adding new tokens (specifically, domain-specific terms) to a pre-trained biomed_RoBERTa model. I have come across two main strategies for this:\n",
            "Using the add_tokens method provided by the Transformers library to directly add new tokens to the tokenizer’s vocabulary.\n",
            "Training a new Byte Pair Encoding (BPE) tokenizer on the new data and merging it with the existing vocabulary.\n",
            "I’m interested in hearing your thoughts on the advantages and disadvantages of each method, particularly in the context of adding domain-specific vocabulary to a pre-existing model.\n",
            "Add_tokens is a more straightforward approach and allows the model to be fine-tuned on the new data, rather than being trained from scratch. However, BPE has advantages, such as handling out-of-vocabulary words and producing more efficient tokenization.\n",
            "I’d appreciate your insights on these points:\n",
            "How does each method impact the model’s generalization of new, unseen data?\n",
            "How do these methods influence computational efficiency during training and inference?\n",
            "How does each strategy handle out-of-vocabulary words?\n",
            "Are there any potential drawbacks or challenges in implementing these methods?\n",
            "Thank you in advance for your insights!\n",
            "Topic Timestamp: Jul 14, 2023 1:49 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Best solution for train tokenizer and MLM from scratch\n",
            "Link: https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579\n",
            "Replies: 0\n",
            "Views: 725\n",
            "Activity: Created: Dec 6, 2021 4:13 pm\n",
            "Topic Owner: Stefano\n",
            "Topic Content: Hi guys,\n",
            "I searched over different discussions but I was not able to find an effective answer.\n",
            "I’d like to train from scratch an MLM model (roberta) wherein the original paper concatenate full sentences in the same line up to 512 tokens. This means that they have split the text of the documents into sentences, but actually I was not sure what is the best tool to do this.\n",
            "Moreover, on the web, there are thousands of different approaches to training an MLM from scratch.\n",
            "Some people put a document into a single line, others split a document into sentences and put a sentence in each line.\n",
            "So my questions are:\n",
            "what is the best solution to train the tokenizer and the model from scratch? split a document into lines or keep the same document in one line?\n",
            "What is in your opinion the best choice to split a document into sentences? (different languages)\n",
            "Thanks\n",
            "Topic Timestamp: Dec 6, 2021 4:13 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Adding token to t5-base vocab does not respect space\n",
            "Link: https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662\n",
            "Replies: 0\n",
            "Views: 718\n",
            "Activity: Created: Jan 13, 2022 6:48 pm\n",
            "Latest: Jan 13, 2022 6:48 pm\n",
            "Topic Owner: rahular\n",
            "Topic Content: I am adding { and } to the t5-base tokenizer with\n",
            "tokenizer.add_tokens(['{', '}'])\n",
            "This successfully adds the tokens, but with a slight problem. When I try to tokenize the following sentence, the space before the { and } is gone\n",
            ">>> tokenizer.tokenize('hello { this is a sentence } bye')\n",
            "['▁hello', '{', '▁this', '▁is', '▁', 'a', '▁sentence', '}', '▁by', 'e']\n",
            "This will result in an imperfect reconstruction of the sentence when decoding\n",
            ">>> tokenizer.batch_decode(tokenizer([\"hello { this is a sentence } bye\"])['input_ids'], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
            "['hello{ this is a sentence} bye']\n",
            "I also tried adding the tokens with different configs using AddedToken without success. Any ideas on how I can make { and } their own words?\n",
            "Topic Timestamp: Jan 13, 2022 6:48 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: LongFormer tokenizer has the same token_type_ids for sequence pairs\n",
            "Link: https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992\n",
            "Replies: 0\n",
            "Views: 712\n",
            "Activity: Created: Dec 20, 2021 11:44 pm\n",
            "Topic Owner: Matigol\n",
            "Topic Content: Hey Guys,\n",
            "I was trying to process a sequence pair of sentences together using the tokenizer for the longformer model and the problem is that the token_type_ids is a list always with zero elements.\n",
            "This is a snippet what the data looks like\n",
            "This is the code to create my CustomDataset:\n",
            "class PlagiarismDetectorDataset(Dataset):\n",
            "    \n",
            "    def __init__(self, data: pd.DataFrame, tokenizer, max_token_len: int = 4096):\n",
            "        self.data = data\n",
            "        self.tokenizer = tokenizer\n",
            "        self.max_token_len = max_token_len\n",
            "        \n",
            "    def __len__(self):\n",
            "        return len(self.data[self.data['Datatype'] == 'train'])\n",
            "    \n",
            "    def __getitem__(self, item: int):\n",
            "        \n",
            "        data_row = self.data.iloc[item]\n",
            "        \n",
            "        review = data_row.Text\n",
            "        original = self.data[(self.data['Task'] == data_row.Task) & (self.data.Datatype == 'orig')]['Text'].iloc[0]\n",
            "        label = data_row.Class\n",
            "        \n",
            "        encoding = self.tokenizer(\n",
            "            text=review,\n",
            "            text_pair=original,\n",
            "            add_special_tokens=True,\n",
            "            max_length=self.max_token_len,\n",
            "            return_token_type_ids=True,\n",
            "            truncation=True,\n",
            "            return_attention_mask=True,\n",
            "            return_tensors=\"pt\")\n",
            "        \n",
            "        return dict(\n",
            "            review=review,\n",
            "            original=original,\n",
            "            label=label,\n",
            "            input_ids=encoding[\"input_ids\"].flatten(),\n",
            "            token_type_ids=encoding[\"token_type_ids\"].flatten(),\n",
            "            attention_mask=encoding[\"attention_mask\"].flatten(),\n",
            "            labels=torch.DoubleTensor(label)\n",
            "        )\n",
            "Finally, this the way to get a sample of the processed data:\n",
            "from transformers import AutoTokenizer, LongformerForSequenceClassification\n",
            "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
            "\n",
            "train_dataset = PlagiarismDetectorDataset(data=aux, tokenizer=tokenizer)\n",
            "sample_data = train_dataset[0]\n",
            "Based on the video of preprocessing I was expecting to have a token_type_ids with 0 and 1 elements. However, the result I get is a list of zero elements. What am I doing wrong?\n",
            "Topic Timestamp: Dec 20, 2021 11:44 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Fast tokenizer for marianMTModel\n",
            "Link: https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638\n",
            "Replies: 1\n",
            "Views: 499\n",
            "Activity: Created: Sep 26, 2022 3:58 pm\n",
            "Latest: Sep 26, 2022 5:06 pm\n",
            "Topic Owner: Matthieu\n",
            "Topic Content: Hi,\n",
            "I use Helsinki-NLP/opus-mt-fr-en model for translation from french to english.\n",
            "When I load the tokenizer, I see that the tokenizer isn’t fast even if I use the use_fast=True flag:\n",
            "tokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-fr-en, use_fast=True)\n",
            "PreTrainedTokenizer(name_or_path='Helsinki-NLP/opus-mt-fr-en', vocab_size=59514, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})\n",
            "Doesn’t it exist fast tokenizer for MarianMTModel?\n",
            "Topic Timestamp: Sep 26, 2022 3:58 pm\n",
            "Replies:\n",
            "  - Username: sgugger\n",
            "    Answer: No, there is no fast tokenizer for Marian models.\n",
            "    Timestamp: Sep 26, 2022 5:06 pm\n",
            "----------------------------------------\n",
            "Title: Tokenizer.train() running out of memory\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355\n",
            "Replies: 0\n",
            "Views: 702\n",
            "Activity: Created: Feb 9, 2023 1:15 am\n",
            "Topic Owner: 5GreatApes\n",
            "Topic Content: I am trying to train a tokenizer using the following code:\n",
            "tokenizer= ByteLevelBPETokenizer()\n",
            "\n",
            "\n",
            "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
            "    \"<s>\",\n",
            "    \"<pad>\",\n",
            "    \"</s>\",\n",
            "    \"<unk>\",\n",
            "    \"<mask>\",\n",
            "])\n",
            "\n",
            "# Save files to disk\n",
            "tokenizer.save_model(\"tokenizer\")\n",
            "It works fine when using a small dataset but when using my full dataset it reads the following error:\n",
            "[00:00:00] Pre-processing files (485 Mo) ████████████████████████████████████████████████ 100%\n",
            "memory allocation of 21474836480 bytes failed\n",
            "My system has 16gb of RAM. Is there a way around this issue that isn’t upgrading RAM? I’m not finding solutions online. Thanks\n",
            "Topic Timestamp: Feb 9, 2023 1:15 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Lower case with input ids\n",
            "Link: https://discuss.huggingface.co/t/lower-case-with-input-ids/18490\n",
            "Replies: 0\n",
            "Views: 692\n",
            "Activity: Created: May 29, 2022 6:50 pm\n",
            "Topic Owner: shon711\n",
            "Topic Content: Hello,\n",
            "It is possible to do lower case to given input ids without decode and then encode again ?\n",
            "for example\n",
            "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
            "\n",
            "text1 = tokenizer.decode([713, 16, 10, 3645, 4])\n",
            "print(text1)\n",
            ">>> This is a sentence.\n",
            "\n",
            "text2 = tokenizer.decode([9226, 16, 10, 3645, 4])\n",
            "print(text2)\n",
            ">>> this is a sentence.\n",
            "I would like to know if there is some fast way to convert the id 713 to 9226, without decode, do lower and then encode again.\n",
            "Thanks,\n",
            "Shon\n",
            "Topic Timestamp: May 29, 2022 6:50 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Performance difference between ByteLevelBPE and Wordpiece tokenizers\n",
            "Link: https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203\n",
            "Replies: 0\n",
            "Views: 682\n",
            "Activity: Created: Sep 22, 2021 2:13 am\n",
            "Topic Owner: dexhrestha\n",
            "Topic Content: I pretrained two models for Nepali language which is in devnagari script and similar to hindi, first model was a DistillBertmodel with Wordpiece tokenizer and second was Roberta model with ByteLevelBPE tokenizer. For the first model i used oscar nepali dataset which is relatively small dataset. Even for first 50000 optimization steps the model was performing really well, it could predict words based on the context. However for the second model with ByteLevelBPE tokenizer I used a bigger dataset with almost 700k lines. But for this model it is not performing as well as the DistillBert model. So coming to my question. I have few questions about the tokenizer and model.\n",
            "Wordpiece tokenizer normalized the words in the sentence. Why did this happen?\n",
            "Is Distillbert a better model for pre training than roberta or is it due to the tokenizer that i am getting bad results for roberta model? Because while studying about models Roberta was said to have higher parameters and better model than distillbert.\n",
            "Topic Timestamp: Sep 22, 2021 2:13 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to handle parenthesis, quotation marks, \\n etc when creating tokenizer from scratch\n",
            "Link: https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628\n",
            "Replies: 0\n",
            "Views: 677\n",
            "Activity: Created: Jun 26, 2022 6:15 am\n",
            "Topic Owner: jonathanalis\n",
            "Topic Content: Hello,\n",
            "I have a large corpus from a specific domain.\n",
            "We decided that it was better to create a tokenizer from scratch, there was too many important tokens that was’nt in the general language tokenizer vocabulary.\n",
            "We use SentensePieceBPETokenizer.\n",
            "However, in my first tests, odd things happened.\n",
            "Lets consider the generic token “token”. It and its capitalized variations apear on the vocabulary.\n",
            "However, also “(token”, ““token”, “\\ntoken”, “\\n\\ntoken”, “\\n\\n\\ntoken”, “\\n(token”, “.\\nToken” (parentesis, quotation mark, a sequence of 1 or more new line (\\n), and their combination. Even period, new line and the token ) apear on the vocabulary.\n",
            "Therefore, the tokenizer is not segmenting the \\n, quotation marks, parenthesis, etc from the tokens. The tokenizer is adding more words with the variations of the tokens with these annoying characters. So, words that have the same meaning are considered other words in the vocabulary.\n",
            "How should I handle this problem? And how to do it using Tokenizer class?\n",
            "For now, my code is going like this:\n",
            "Blockquote\n",
            "tokenizer = SentencePieceBPETokenizer()\n",
            "tokenizer.normalizer=normalizers.Sequence( [normalizers.NFD() ] )\n",
            "tokenizer.train_from_iterator(\n",
            "dataset[‘text’],\n",
            "vocab_size=60_000\n",
            "min_frequency=5,\n",
            "show_progress=True,\n",
            "limit_alphabet=500,\n",
            ")\n",
            "I am aware that I could include the special tokens as the parameter special_tokens on the train_from_iterator function of the tokenizer, for instance:\n",
            "Blockquote\n",
            "special_tokens=[‘[PAD]’, ‘[UNK]’, ‘[CLS]’, ‘[SEP]’, ‘[MASK]’]\n",
            "But how to include more special tokens, and attribute specific characters (like newline, quotation mark, parenthesis) to them?\n",
            "Also, if doing so, these tokens are treated as segmented from other tokens when building a tokenizer? Will it solve my problem?\n",
            "Thank you\n",
            "Topic Timestamp: Jun 26, 2022 6:15 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Bypassing tokenizers\n",
            "Link: https://discuss.huggingface.co/t/bypassing-tokenizers/2162\n",
            "Replies: 2\n",
            "Views: 390\n",
            "Activity: Created: Nov 22, 2020 11:08 am\n",
            "Latest: Nov 23, 2020 1:49 pm\n",
            "Topic Owner: thenoone\n",
            "Topic Content: Hi everyone,\n",
            "Is it possible to bypass the tokenizer and directly provide the input embeddings to train the BERT model?\n",
            "Thanks!\n",
            "Topic Timestamp: Nov 22, 2020 11:08 am\n",
            "Replies:\n",
            "  - Username: BramVanroy\n",
            "    Answer: You can just create a subclass of the model that you want and modify its forward pass.\n",
            "    Timestamp: Nov 22, 2020 4:51 pm\n",
            "  - Username: sgugger\n",
            "    Answer: You can also feed input_embeds instead of input_ids to your mdel.\n",
            "    Timestamp: Nov 23, 2020 1:49 pm\n",
            "----------------------------------------\n",
            "Title: Tokenizer Trainer Crashing\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645\n",
            "Replies: 0\n",
            "Views: 664\n",
            "Activity: Created: Apr 15, 2023 3:46 am\n",
            "Latest: Apr 15, 2023 3:46 am\n",
            "Topic Owner: jeromeku\n",
            "Topic Content: Trying to train a tokenizer on a corpus of protein sequences ~8 GB in size (32M examples).\n",
            "However, the memory usage grows incrementally until the process is killed when training from iterator:\n",
            "from tokenizers import SentencePieceBPETokenizer\n",
            "from tokenizers.processors import BertProcessing\n",
            "\n",
            "import datasets\n",
            "from datasets import load_dataset\n",
            "\n",
            "import itertools\n",
            "\n",
            "from datetime import datetime\n",
            "from absl import flags, app\n",
            "\n",
            "flags.DEFINE_integer(\"num_batches\", 1, help=\"Number of batches to sample\")\n",
            "flags.DEFINE_integer(\"batch_size\", 100, help=\"Batch size\")\n",
            "FLAGS = flags.FLAGS\n",
            "\n",
            "dataset_path = \"agemagician/uniref30\"\n",
            "\n",
            "def batch_iterator(ds: datasets.Dataset, batch_size = 1000, num_batches = -1, split='train', feature='text'):\n",
            "    \n",
            "    iterator = iter(ds[split])\n",
            "    \n",
            "    def batch_generator(iterable):\n",
            "        while (batch := [ex[feature] for ex in itertools.islice(iterable, batch_size)]):\n",
            "            yield batch \n",
            "         \n",
            "    return batch_generator(iterator) if num_batches == -1 else itertools.chain(itertools.islice(batch_generator(iterator), num_batches))\n",
            "    \n",
            "def train(batch_size, num_batches):\n",
            "    uniref = load_dataset(dataset_path)\n",
            "    tokenizer = SentencePieceBPETokenizer()\n",
            "    \n",
            "    it = batch_iterator(uniref, batch_size=batch_size, num_batches=num_batches)\n",
            "    tokenizer.train_from_iterator(it, vocab_size=1000, min_frequency=2, special_tokens=[\n",
            "        \"<s>\",\n",
            "        \"<pad>\",\n",
            "        \"</s>\",\n",
            "        \"<unk>\",\n",
            "        \"<mask>\",\n",
            "    ])\n",
            "    tokenizer.save('proteins-tmp')\n",
            "\n",
            "def main(args):\n",
            "    start = datetime.now()\n",
            "    print(f\"Starting tokenizer training on {FLAGS.num_batches} of batch size {FLAGS.batch_size}\")\n",
            "    train(FLAGS.batch_size, FLAGS.num_batches)\n",
            "    end = datetime.now()\n",
            "    dur = end - start\n",
            "    print(f\"Took {dur.total_seconds() / 60.:.1f} minutes\")    \n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "   app.run(main)\n",
            "I included batch_size and num_batches commandline args to enable testing. However, when I try to train the tokenizer on the entire dataset (setting num_batches=-1), the memory usage during the “tokenizing” phase starts to grow until the entire process crashes.\n",
            "Are there any suggested workarounds for this? Is it possible to train the tokenizer on chunks of data (i.e., call tokenizer.train_from_iterator multiple times), or does it need to see the entire dataset at once for the BPE algorithm to work correctly?\n",
            "Thanks!\n",
            "Topic Timestamp: Apr 15, 2023 3:46 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Dialogue classification\n",
            "Link: https://discuss.huggingface.co/t/dialogue-classification/18466\n",
            "Replies: 0\n",
            "Views: 656\n",
            "Activity: Created: May 28, 2022 5:01 pm\n",
            "Topic Owner: gillan\n",
            "Topic Content: I want to make a classification of a dialogue (client and assistant). What is the best way to encode the chat so that model knows whose speech where? Maybe to use some custom special tokens?\n",
            "Topic Timestamp: May 28, 2022 5:01 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Using the Tokenizers library in a Unity project\n",
            "Link: https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533\n",
            "Replies: 0\n",
            "Views: 655\n",
            "Activity: Created: May 4, 2022 6:50 pm\n",
            "Topic Owner: cj-mills\n",
            "Topic Content: Hi everyone! Any advice on the best way to leverage the Tokenizers library when using a transformer model in a Unity project (C#)? I’m considering creating a Rust plugin, embedding Python in a C++ plugin, or using a server. Ideally, I could run everything locally, model size allowing.\n",
            "Topic Timestamp: May 4, 2022 6:50 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: What are the equivalent manner for using texts_to_sequences?\n",
            "Link: https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220\n",
            "Replies: 0\n",
            "Views: 644\n",
            "Activity: Created: Dec 29, 2021 11:19 pm\n",
            "Topic Owner: toyl\n",
            "Topic Content: what are the equivalent manner for using\n",
            "seq = tokenizer.texts_to_sequences([desc])[0]\n",
            "in bert\n",
            "Topic Timestamp: Dec 29, 2021 11:19 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Error finetuning XLM-RoBERTa-Large when training\n",
            "Link: https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412\n",
            "Replies: 2\n",
            "Views: 372\n",
            "Activity: Created: Jul 15, 2022 9:03 am\n",
            "Latest: Jul 15, 2022 9:07 am\n",
            "Topic Owner: JeunesseAfricaine\n",
            "Topic Content: I followed the guide on the documentation HERE while pre-training a zero-shot classifier. These are screenshots of Tokenization, Compute Metrics, Training Arguments, and Trainer.\n",
            "Topic Timestamp: Jul 15, 2022 9:03 am\n",
            "Replies:\n",
            "  - Username: JeunesseAfricaine\n",
            "    Answer: \n",
            "    Timestamp: Jul 15, 2022 9:07 am\n",
            "  - Username: JeunesseAfricaine\n",
            "    Answer: \n",
            "    Timestamp: Jul 15, 2022 9:07 am\n",
            "----------------------------------------\n",
            "Title: Space token ’ ’ cannot be add when is_split_into_words = True\n",
            "Link: https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305\n",
            "Replies: 1\n",
            "Views: 455\n",
            "Activity: Created: Mar 11, 2021 9:27 am\n",
            "Latest: Mar 11, 2021 9:35 am\n",
            "Topic Owner: Boltzmachine\n",
            "Topic Content: for example,\n",
            ">>> tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
            ">>> tokenizer.add_tokens(' ')\n",
            "1\n",
            ">>> tokenizer.encode('你好 世界', add_special_tokens=False)\n",
            "[872, 1962, 21128, 686, 4518]\n",
            ">>> tokenizer.encode(['你','好',' ', '世', '界'], is_split_into_words=True, add_special_tokens=False)\n",
            " [872, 1962, 686, 4518]\n",
            "Obviously, the blank token is ignored. But if you change it to another token like ‘[balabala]’, it works.\n",
            "So what is the proper way to do this?\n",
            "Topic Timestamp: Mar 11, 2021 9:27 am\n",
            "Replies:\n",
            "  - Username: Boltzmachine\n",
            "    Answer: I found that one way is to use convert_tokens_to_ids, yet by which I cannot use the convenient features in encode and __call__ such as padding and automatically generating attention_mask\n",
            "    Timestamp: Mar 11, 2021 9:35 am\n",
            "----------------------------------------\n",
            "Title: Chunk tokens into desired chunk length without simply getting rid of rest of tokens\n",
            "Link: https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399\n",
            "Replies: 0\n",
            "Views: 624\n",
            "Activity: Created: Jun 15, 2023 4:01 pm\n",
            "Topic Owner: bennicholl\n",
            "Topic Content: Is there a way to break up a tokens based on max token length? For example if I tokenize the sentence\n",
            "toked_sent = tokenizer([\"I have two rottweilers and a black lab\"])\n",
            "I’ll get\n",
            "{'input_ids': [[27, 43, 192, 3, 14369, 15337, 277, 11, 3, 9, 1001, 7690, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
            "If I want to set some parameter max_length to 8, the output would be something like\n",
            "{'input_ids': [[27, 43, 192, 3, 14369, 15337, 277, 11], [3, 9, 1001, 7690, 1] ], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1,1,1,1,1] ]}\n",
            "Is this possible?\n",
            "Topic Timestamp: Jun 15, 2023 4:01 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to return custom `token_type_ids` or other values from a tokenizer?\n",
            "Link: https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525\n",
            "Replies: 0\n",
            "Views: 625\n",
            "Activity: Created: May 3, 2023 9:07 am\n",
            "Latest: May 3, 2023 9:09 am\n",
            "Topic Owner: lonewar\n",
            "Topic Content: I have two text features, ‘title’ and ‘abstract’. I want to tokenize these two columns in a Dataset.\n",
            "# https://huggingface.co/jjzha/jobbert-base-cased\n",
            "checkpoint = 'jjzha/jobbert-base-cased'\n",
            "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
            "tokenizer.model_max_len = 512\n",
            "\n",
            "def tokenize(batch):\n",
            "    tokenized_text1 = tokenizer(batch[\"title\"], truncation=True, padding='max_length', max_length=512)\n",
            "    tokenized_text2 = tokenizer(batch[\"abstract\"], truncation=True, padding='max_length', max_length=512)\n",
            "\n",
            "    return {'input_ids1': tokenized_text1['input_ids'], 'attention_mask1': tokenized_text1['attention_mask'],\n",
            "        'token_type_ids1': tokenized_text1['token_type_ids'],\n",
            "        'input_ids2': tokenized_text2['input_ids'], 'attention_mask2': tokenized_text2['attention_mask'],\n",
            "        'token_type_ids2': [1] * len(tokenized_text2['token_type_ids']) #100 * [[1] * 512]\n",
            "            }\n",
            "\n",
            "dataset_dict.map(tokenize, batched=True)['train'].to_pandas()#['input_ids1']\n",
            "In the above code, in this line ‘token_type_ids2’, I want to create the token_type_ids full of 1s for the second text feature (like next sentence prediction in BERT). But it is not working as expected. I assume ‘token_type_ids2’ should be a list of size (max_len). Is it correct?\n",
            "By examining the tokenized dataset, the column “token_type_ids2” for each row, is a single value 1. But it is supposed to be a list of 1s with length 512.\n",
            "How to solve this problem?\n",
            "BTW, If I fix the value there to be “100 * [[1] * 512]”, it shows the correct token_type_ids. 100 is the size of the original dataset. But shouldn’t the map function applies the tokenize function to each row?\n",
            "Topic Timestamp: May 3, 2023 9:07 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Huggingface t5 models seem to not download a tokenizer file\n",
            "Link: https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935\n",
            "Replies: 0\n",
            "Views: 623\n",
            "Activity: Created: Dec 16, 2022 12:14 pm\n",
            "Topic Owner: jantuitman\n",
            "Topic Content: If I run this code i get an error:\n",
            "model_name = 'yhavinga/t5-base-dutch'\n",
            "#model_name = 'flax-community/t5-base-dutch'\n",
            "model: PreTrainedModel = T5ForConditionalGeneration.from_pretrained(model_name)\n",
            "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
            "I have debugged the code and i see there is no resolved filename that is passed in to the underlying SentencePiece tokenizer. But this tokenizer crashes if it is not initialized with a file.\n",
            "So is this a bug in the model files? How do i otherwise get the same tokens as the model was trained with on my local machine?\n",
            "Topic Timestamp: Dec 16, 2022 12:14 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Multilang bert vs translating to english\n",
            "Link: https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454\n",
            "Replies: 0\n",
            "Views: 603\n",
            "Activity: Created: May 28, 2022 8:32 am\n",
            "Topic Owner: jl22\n",
            "Topic Content: I am using sentence transformers (i tried many multilang tokenizers) but they do not perform as well vs english.\n",
            "For now I translare first using the deepl api and tokenize the translated version. Good results so far.\n",
            "I was wondering what the downside of this method is? The translation is very fast and costs are manageable.\n",
            "Topic Timestamp: May 28, 2022 8:32 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Questions on model’s tokens\n",
            "Link: https://discuss.huggingface.co/t/questions-on-models-tokens/5023\n",
            "Replies: 0\n",
            "Views: 592\n",
            "Activity: Created: Mar 24, 2021 1:46 am\n",
            "Topic Owner: ape\n",
            "Topic Content: Hi, I have a few questions regarding tokenizing word/characters/emojis for different huggingface models.\n",
            "From my understanding, a model would only perform best during inference if the token of the input sentence are within the tokens that the model’s tokenizer was trained on.\n",
            "My questions are:\n",
            "is there a way to easily find out if a particular word/emoji is compatible (included during model training) with the model?\n",
            "if this word/emoji is not was not included during model training, what are the best ways to deal with these words/emojis, such that model inference would give best possible output considering the inclusion of these word/emoji as input. (for 2. it would be nice if it could be answered in the context of my setup below, if possible)\n",
            "My current setup is as follows:\n",
            "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
            "pre_trained_model = 'facebook/bart-large-mnli'\n",
            "task = 'zero-shot-classification'\n",
            "candidate_labels = ['happy', 'sad', 'angry', 'confused']\n",
            "tokenizer = AutoTokenizer.from_pretrained(pre_trained_model)\n",
            "model = AutoModelForSequenceClassification.from_pretrained(pre_trained_model)\n",
            "zero_shot_classifier = pipeline(model=model, tokenizer=tokenizer, task=task)\n",
            "\n",
            "zero_shot_classifier('today is a good day 😃', candidate_labels=candidate_labels)\n",
            "Any help is appreciated\n",
            "Topic Timestamp: Mar 24, 2021 1:46 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How do I remove tokens from a BPE Tokenizer’s vocabulary?\n",
            "Link: https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593\n",
            "Replies: 2\n",
            "Views: 341\n",
            "Activity: Created: Jun 28, 2024 12:27 pm\n",
            "Latest: Jul 3, 2024 8:28 am\n",
            "Topic Owner: ForBo7\n",
            "Topic Content: Hi there!\n",
            "I need to remove specific tokens from my tokenizer’s vocabulary, and I am not quite sure how to do so. Specifically, I am using Qwen2Tokenizer, a BPE tokenizer, and I would like to remove specific Chinese tokens from its vocabulary. I have tried various methods, shown below, but to no avail.\n",
            "Deleting Tokens from Vocabulary\n",
            "for tok in long_toks: vocab.pop(tok)\n",
            "tokz.vocab = vocab\n",
            "This results in unknown tokens being output. The length of vocabulary also differs in certain places. That is, `len(vocab) ≠ len(tokz.vocab).\n",
            "Tweaking vocab.json and merges.txt\n",
            "vocab.json\n",
            "import json\n",
            "with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f)\n",
            "for tok in long_toks: vocab.pop(tok)\n",
            "with open('tokenizer/vocab.json', 'w') as f: json.dump(vocab, f, indent=2)\n",
            "merges.txt\n",
            "with open('tokenizer/merges.txt', 'r') as f: merges = L(f.readlines()[1:])\n",
            "While I am able to update vocab.json, I do not know how to work with the contents of merges.txt. This file stores pairs as follows:\n",
            "(#151387) ['Ġ Ġ\\n','ĠĠ ĠĠ\\n','i n\\n','Ġ t\\n','ĠĠĠĠ ĠĠĠĠ\\n','e r\\n','ĠĠ Ġ\\n','o n\\n','Ġ a\\n','r e\\n'...]\n",
            "I am unable to determine what special characters represent Chinese characters. Is there any way I can decode or figure out what the characters such as ‘Ġ’ and ‘â½ Ĺ \\n’ represent?\n",
            "Training a Tokenizer\n",
            "I feel training a tokenizer might not be feasible, because of the amount of data required? I want to have the exact same Qwen2Tokenizer, save for certain tokens removed.\n",
            "Backend\n",
            "import json\n",
            "tokz_state = json.loads(tokz.backend_tokenizer.model.__getstate__())\n",
            "for tok in long_toks: del tokz_state['vocab'][tok]\n",
            "\n",
            "from tokenizers import models\n",
            "model_class = getattr(models, tokz_state.pop('type'))\n",
            "tokz.backend_tokenizer.model = model_class(**tokz_state)\n",
            "However, this approach results in the following error:\n",
            "---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)Cell In[67], line 3\n",
            "      1 from tokenizers import models\n",
            "      2 model_class = getattr(models, tokz_state.pop('type'))\n",
            "----> 3 tokz.backend_tokenizer.model = model_class(**tokz_state)\n",
            "TypeError: argument 'merges': failed to extract enum PyMerges ('Merges | Filename')\n",
            "- variant Merges (Merges): TypeError: failed to extract field PyMerges::Merges.0, caused by TypeError: 'str' object cannot be converted to 'PyTuple'\n",
            "- variant Filename (Filename): TypeError: failed to extract field PyMerges::Filename.0, caused by TypeError: 'list' object cannot be converted to 'PyString'\n",
            "This approach references this GitHub issue, where others also noted the same error.\n",
            "I would really appreciate any pointers regarding how correctly remove tokens!\n",
            "Topic Timestamp: Jun 28, 2024 12:27 pm\n",
            "Replies:\n",
            "  - Username: ForBo7\n",
            "    Answer: I figured out a way to do so to remove tokens from a BPE Tokenizer.\n",
            "I took each pair in the merges.txt file and combined them to produce the resulting token. I then looked up the resulting token in the tokenizer’s vocabulary to obtain its input ID. I then decoded the obtained input ID to determine what each resulting merge represented. From that, I was able to filter out the undesired tokens from the merge file.\n",
            "with open('tokenizer/merges.txt', 'r') as f: mrules = L(f.read().split('\\n')[1:-1])\n",
            "import json\n",
            "with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f)\n",
            "\n",
            "merged = L(merge.replace(' ', '') for merge in mrules)\n",
            "merged_ids = L(vocab[merge] for merge in merged)\n",
            "pairs = dict(zip(merged, merged_ids))\n",
            "\n",
            "for i, _merge in enumerate(mrules):\n",
            "  merge = _merge.replace(' ', '')\n",
            "  idx = vocab[merge]\n",
            "  if is_long(tokz.decode(idx))[0]: mrules.remove(_merge)\n",
            "Then I simply removed the same undesired tokens from the tokenizer’s vocabulary.\n",
            "with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f)\n",
            "long_toks = {tok: idx for tok, idx in vocab.items() if is_long(tokz.decode(idx))[0]}\n",
            "for tok in long_toks: vocab.pop(tok)\n",
            "    Timestamp: Jul 3, 2024 8:28 am\n",
            "----------------------------------------\n",
            "Title: Avoid creating certain tokens when training a tokenizer\n",
            "Link: https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864\n",
            "Replies: 0\n",
            "Views: 583\n",
            "Activity: Created: Jul 26, 2022 2:49 am\n",
            "Topic Owner: jonathanalis\n",
            "Topic Content: Hello.\n",
            "There is a way to train a tokenizer from scratch, but force the algorithm (lets say, WordPiece) to not include in the vocabulary some tokens based on a given rule?\n",
            "For instance, I would like to\n",
            "Lets say we have the words:\n",
            "['rondônia', 'Rondônia', 'RONDÔNIA']\n",
            "Training a lowercase wordpiece tokenizer on my dataset tokenizes them as ['rondôn', '##ia']\n",
            "However, i tryed something different. I preprocess them (using a custom pretokenizer) into, respectively:\n",
            "['rondônia', 'rondônia_FU', 'rondônia_U']\n",
            "I tryed tokenizing, but I got\n",
            "['rondôn', '##ia'], ['rondôn', '##ia_FU'], ['rondôn', '##ia_U']\n",
            "Is not what I expected. I expect :\n",
            "['rondôn', '##ia'], ['rondôn', '##ia', '##_FU'], ['rondôn', '##ia', '##_FU']\n",
            "(or either ['rondônia'], ['rondônia', '##_FU'], ['rondônia', '##_FU'])\n",
            "I want to train the tokenizer in such way the ‘_FU’ and ‘_U’ to not being part of tokens, to be an exception, only when finding these terms in the words it ignores and break them separately, like they were something like ponctuations…\n",
            "It is possible? There is such option when training a (wordpiece) tokenizer?\n",
            "Plan B\n",
            "If not, it is possible to remove words from vocabulary?\n",
            "I mean, I could remove every token that ends in _FU (except ‘##_FU’). By doing this, it will force the tokenizer to tokenize as I want because it wont find any tokens larger than ‘##_FU’ when tokenizing words that ends with _FU, because they wont be on the vocabulary anymore.\n",
            "Makes sense?\n",
            "Is it possible to remove words from vocabulary?\n",
            "Side note: I didnt save a vocabulary in disk, it seems it is on the memory while running the code.\n",
            "As it is a customized tokenizer, it wont let me save it:\n",
            "tok_wpc.save(\"tokenizer.json\")\n",
            "Returns the error:\n",
            "Exception: Custom PreTokenizer cannot be serialized\n",
            "So I cant save externally, neither see the generated vocabulary…\n",
            "Tks in advance.\n",
            "Topic Timestamp: Jul 26, 2022 2:49 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Issue with Tokenizer\n",
            "Link: https://discuss.huggingface.co/t/issue-with-tokenizer/33796\n",
            "Replies: 0\n",
            "Views: 580\n",
            "Activity: Created: Mar 14, 2023 5:16 pm\n",
            "Latest: Mar 14, 2023 5:16 pm\n",
            "Topic Owner: pnandhini\n",
            "Topic Content: I am trying to download audio files using streaming mode. The steps go like this Download the dataset, processor feature extractor, prepare the dataset, seq2seqtrainer, compute metrics and then trainer.train().\n",
            "When the training steps starts, after few downloads getting an error in tokenizer.utils.base as “You need to specify either ’ text’ or ’ text_target’”.\n",
            "Have two questions here, one is not getting this error while downloading non streaming mode. Secondly after passing the parameter as \" tex\" / \" text_target\" still getting this error.\n",
            "Any help would be appreciated!!\n",
            "Topic Timestamp: Mar 14, 2023 5:16 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Adding a special language token to MBART\n",
            "Link: https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967\n",
            "Replies: 0\n",
            "Views: 555\n",
            "Activity: Created: Nov 12, 2022 1:09 pm\n",
            "Topic Owner: BramVanroy\n",
            "Topic Content: Is there a straightforward way of adding a new language to MBART’s tokenizer? The implementation seems quite intricate so it does not seem straightforward to add a new language code.\n",
            "        self.sp_model_size = len(self.sp_model)\n",
            "        self.lang_code_to_id = {\n",
            "            code: self.sp_model_size + i + self.fairseq_offset for i, code in enumerate(FAIRSEQ_LANGUAGE_CODES)\n",
            "        }\n",
            "        self.id_to_lang_code = {v: k for k, v in self.lang_code_to_id.items()}\n",
            "        self.fairseq_tokens_to_ids[\"<mask>\"] = len(self.sp_model) + len(self.lang_code_to_id) + self.fairseq_offset\n",
            "\n",
            "        self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n",
            "        self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n",
            "        self._additional_special_tokens = list(self.lang_code_to_id.keys())\n",
            "\n",
            "        if additional_special_tokens is not None:\n",
            "            # Only add those special tokens if they are not already there.\n",
            "            self._additional_special_tokens.extend(\n",
            "                [t for t in additional_special_tokens if t not in self._additional_special_tokens]\n",
            "            )\n",
            "\n",
            "        self._src_lang = src_lang if src_lang is not None else \"en_XX\"\n",
            "        self.cur_lang_code_id = self.lang_code_to_id[self._src_lang]\n",
            "Even with subclassing it is not immediately clear to me if and how one would add a custom language code that will be correctly recognized when using tokenizer(target_text=...) or other target language related things. Any tips?\n",
            "Topic Timestamp: Nov 12, 2022 1:09 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to train target tokenizer\n",
            "Link: https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291\n",
            "Replies: 0\n",
            "Views: 554\n",
            "Activity: Created: Aug 30, 2022 2:48 am\n",
            "Topic Owner: nubol23\n",
            "Topic Content: I’m trying to retrain t5-small with a japanese to spanish dataset, I want to retrain the tokenizer to handle the words in those languages\n",
            "Currently I’ve done this:\n",
            "def get_training_corpus(lang: str):\n",
            "    ds = dataset[\"train\"]\n",
            "    for start_idx in range(0, len(ds), 1000):\n",
            "        samples = ds[start_idx : start_idx + 1000]\n",
            "        yield samples[lang]\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
            "new_tokenizer = tokenizer.train_new_from_iterator(\n",
            "    get_training_corpus(\"ja\"), \n",
            "    52000,\n",
            ")\n",
            "but I don’t know how to also train the target side of the tokenizer, I would like to be able to tokenize in japanese like\n",
            "model_inputs = tokenizer(examples[\"ja\"], max_length=max_input_length, truncation=True)\n",
            "and in spanish using\n",
            "with tokenizer.as_target_tokenizer():\n",
            "        labels = tokenizer(examples[\"es\"], max_length=max_target_length, truncation=True)\n",
            "Topic Timestamp: Aug 30, 2022 2:48 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer for German lang\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222\n",
            "Replies: 0\n",
            "Views: 547\n",
            "Activity: Created: Jun 22, 2023 7:34 pm\n",
            "Topic Owner: whatisslove11\n",
            "Topic Content: Hi!\n",
            "I try to implement neural machine translition model from scratch and now I choose tokenizer for languages.\n",
            "I read what tokenizer has special Unicode Normalizer (NFC, NFD, etc.). I have a few questions for this normalizers\n",
            "Do I need to use Unicode normalizers for German or will any other one be suitable?\n",
            "Is there any additional information on unicode normalizers?\n",
            "Thanks a lot for your help!\n",
            "Topic Timestamp: Jun 22, 2023 7:34 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers?\n",
            "Link: https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639\n",
            "Replies: 0\n",
            "Views: 545\n",
            "Activity: Created: Jun 26, 2023 6:49 pm\n",
            "Latest: Jun 27, 2023 10:58 am\n",
            "Topic Owner: mohit-sv\n",
            "Topic Content: Context of question: (scroll down to get to the Real question)\n",
            "I need to find start_position and end_position of the sequence that I input to QA model (eg., RoBERTa, LLMv3) from the start-end character positions of the answer in a context. @NielsRogge in his notebook (Transformers-Tutorials/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb at master · NielsRogge/Transformers-Tutorials · GitHub) does this by using word_ids information of the tokenized context and start-end token indices of the answer in the given context.\n",
            "eg.,\n",
            "context_token_indicies = [0, 1, 2, 3, 4, 5, 6, 7, || 8, 9, 10, 11, 12, 13 ||, 14, 15, 16, 17, 18] # pipes represent the scope of answer in tokenized context\n",
            "ans_start_token_idx_in_context = 8\n",
            "ans_start_token_idx_in_context = 13\n",
            "context_word_ids = [0, 1, 2, 3, 3, 4, 5, 6, 7, || 8, 8, 9, 10, 10, 10, 11, 12, 13 ||, 14, 15, 16, 17, 18, 18] # pipes represent the scope of answer in tokenized context\n",
            "(FYI: Repeated indices are created when a word like “bookworm” is broken down into “book” and “worm” by the tokenizer.)\n",
            "we infer that…\n",
            "start_position = 9\n",
            "end_position = 17\n",
            "Therefore, the success of the whole process is dependent on how we tokenize the context.\n",
            "@NielsRogge used match, word_idx_start, word_idx_end = subfinder(words, answer.split()) (gets first match; doesn’t work for me) which is the same as doing context.split(), but this fails to split at “,”, “.” etc. as done by AutoTokenizer.\n",
            "eg., “Great! Keep it up.” must be tokenized as [“Great”, “!”, “Keep”, “it”, “up”, “.”]\n",
            "=> context_token_indicies = [0, 1, 2, 3, 4, 5] not [0, 1, 2, 3]\n",
            "So I tried using nltk.word_tokenize. Though it does better, again this fails to split at “-”.\n",
            "eg., “Great! Keep-it-up.” must be tokenized as [“Great”, “!”, “Keep”, “-”, “it”, “-”, “up”, “.”]\n",
            "(Using offset_mapping instead of word_ids gives some other problems. RoBERTa, LLMv3 offsets look very different.)\n",
            "I wonder how many other such edge cases exist. Thus the important point to be clarified is…\n",
            "Real question:\n",
            "“How do we need to tokenize context to find start_position and end_position for Question answering task?” This can be understood if we know “how huggingface tokenizers generate the integers in word_ids”.\n",
            "Can anyone please answer one of the questions in the quotes?\n",
            "Topic Timestamp: Jun 26, 2023 6:49 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: GPT2 long text approach\n",
            "Link: https://discuss.huggingface.co/t/gpt2-long-text-approach/28165\n",
            "Replies: 0\n",
            "Views: 542\n",
            "Activity: Created: Dec 20, 2022 10:54 am\n",
            "Topic Owner: misterkilgore\n",
            "Topic Content: Hi everyone. I was wondering, what’s the best way to approach a long text for gpt2 training?\n",
            "Suppose the text is 10000 tokens long and the max length for the tokenizer is 2048.\n",
            "It’s better to split the corpus in 5 sequential samples (first sample from token 0 to 2047, second sample from token 2048 to 4095…) or in 7952 overlapping samples (first sample form token 0 to token 2047, second sample from token 1 to 2048… last sample from token 7952 to token 10000)?\n",
            "I’ve seen people using both approaches.\n",
            "Topic Timestamp: Dec 20, 2022 10:54 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Does Deberta tokenizer use wordpiece?\n",
            "Link: https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307\n",
            "Replies: 0\n",
            "Views: 539\n",
            "Activity: Created: Aug 6, 2022 9:05 am\n",
            "Topic Owner: Trunway\n",
            "Topic Content: From the docs of hugging face:\n",
            "Constructs a DeBERTa tokenizer, which runs end-to-end tokenization: punctuation splitting + workpiece\n",
            "The answer is positive. However, when I checked results tokenized by other models’ tokenizers, the results were confusing. I checked four models in total, respectively deberta, bert, roberta and albert. The tokenized results of the string “Hugging face is of great help to implement models!” by the four models’ tokenizers are respectively shown as follows.\n",
            "My code:\n",
            "tokenizer1 = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
            "tokenizer2 = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
            "tokenizer3 = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
            "tokenizer4 = AutoTokenizer.from_pretrained(\"albert-xxlarge-v2\")\n",
            "\n",
            "test_str = \"Hugging face is of great help to implement models!\"\n",
            "print(tokenizer1.tokenize(test_str))\n",
            "print(tokenizer2.tokenize(test_str))\n",
            "print(tokenizer3.tokenize(test_str))\n",
            "print(tokenizer4.tokenize(test_str))\n",
            "Results:\n",
            "['Hug', 'ging', 'Ġface', 'Ġis', 'Ġof', 'Ġgreat', 'Ġhelp', 'Ġto', 'Ġimplement', 'Ġmodels', '!']\n",
            "['hugging', 'face', 'is', 'of', 'great', 'help', 'to', 'implement', 'models', '!']\n",
            "['Hug', 'ging', 'Ġface', 'Ġis', 'Ġof', 'Ġgreat', 'Ġhelp', 'Ġto', 'Ġimplement', 'Ġmodels', '!']\n",
            "['▁hugging', '▁face', '▁is', '▁of', '▁great', '▁help', '▁to', '▁implement', '▁models', '!']\n",
            "As you can see, the results of deberta is the same as roberta, which, as described in the hugging face docs, is derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding. So, does deberta use byte-level Byte-Pair-Encoding, instead of wordpiece like bert (the results of deberta and bert are different from each other .)? Besides, I also checked the codes for deberta tokenizer, and found that the _tokenize() method is derived from GPT2 tokenizer as well (class DebertaTokenizer(GPT2Tokenizer):).\n",
            "So, is there any problem with the content of the docs, or if anyone could help to explain this? Thank you for your reply!\n",
            "Topic Timestamp: Aug 6, 2022 9:05 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Building a custom Java tokenizer\n",
            "Link: https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823\n",
            "Replies: 0\n",
            "Views: 531\n",
            "Activity: Created: Feb 4, 2024 5:40 pm\n",
            "Topic Owner: nimanthacooray\n",
            "Topic Content: Hi,\n",
            "I am trying to build a custom tokenizer for tokenizing Java code using the tokenizers library. I have a set of tokens that should not be splitted into subwords (For example: Java keywords, operators, separators, common class names, etc). But I want identifiers in the Java token to split into subword tokens (For example: getAge, setName, etc).\n",
            "To achieve this, I have added the tokens that don’t need to be splitted to the tokenizer before training. Then the tokenizer is trained on a large Java code file. Below is the code for this:\n",
            "tokenizer = Tokenizer(models.WordPiece(unk_token='<unk>'))\n",
            "tokenizer.pre_tokenizer = PreTokenizer.custom(JavaPreTokenizer())\n",
            "\n",
            "with open(IMPORTANT_TOKENS_FILE) as fp:\n",
            "    important_tokens = json.load(fp)\n",
            "\n",
            "special_tokens = [\"</s>\", \"<unk>\", \"<pad>\"]\n",
            "num = tokenizer.add_tokens(important_tokens + special_tokens)\n",
            "# print(f\"{num} tokens added\")\n",
            "\n",
            "trainer = WordPieceTrainer(vocab_size=32000, special_tokens=special_tokens)\n",
            "tokenizer.decoder = decoders.WordPiece()\n",
            "tokenizer.train([DATASET_FILE], trainer=trainer)\n",
            "print(f\"Vocab size: {tokenizer.get_vocab_size()}\")\n",
            "The custom pre-tokenizer is implemented using the javalang library. It is a python library for parsing and tokenizing Java code. Below is the custom pre-tokenizer:\n",
            "class JavaPreTokenizer:\n",
            "\n",
            "    def java_tokenize(self, i, normalized_string):\n",
            "        string = str(normalized_string)\n",
            "        javalang_tokens = list(javalang.tokenizer.tokenize(string))\n",
            "        splits = []\n",
            "        original_pos = 0\n",
            "        for javalang_token in javalang_tokens:\n",
            "            length = len(javalang_token.value)\n",
            "            while str(javalang_token.value) != string[original_pos:original_pos+length] and original_pos < len(string):\n",
            "                original_pos += 1\n",
            "            if original_pos >= len(string):\n",
            "                raise ValueError(f\"Could not find token \\\"{javalang_token.value}\\\" in string \\\"{string}\\\"\")\n",
            "            splits.append(normalized_string[original_pos:original_pos+length])\n",
            "            original_pos += length\n",
            "        return splits\n",
            "\n",
            "    def pre_tokenize(self, pretok: PreTokenizedString):\n",
            "        pretok.split(self.java_tokenize)\n",
            "The issue I am facing is, when I try to tokenize “class Person implements Comparable<Person> {\\n String name = \\\"Hello World!\\\";”, the pre-tokenizer is getting only the \"Hello Worl to pre-tokenize and fails with an error message Exception: LexerError: Unterminated character/string literal at \" \", line 1: \"Hello Worl.\n",
            "The failure is understandable because javalang can not tokenize a unterminated string. But why this happens? I mean why pre-tokenizer is given only part of the input?\n",
            "After some debugging, I saw that the pre-tokenizer is applied only on some parts of the input. Those parts are [' Person ', ' ', 'Person', ' ', '\\n ', ' name ', ' \"Hello World']. I thought the pre-tokenizer is applied to the whole input.\n",
            "When I checked my vocabulary, I saw those tokens which were passed to pre-tokenizer are not in the added tokens section of the saved tokenizer (I saved the tokenizer after wrapping it using the transformers library). It seems like the tokens only which are not in the added tokens are passed to the pre-tokenizer.\n",
            "How to fix this issue?\n",
            "Topic Timestamp: Feb 4, 2024 5:40 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Are the slow and fast tokenizer results the same output for the same input?\n",
            "Link: https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743\n",
            "Replies: 0\n",
            "Views: 528\n",
            "Activity: Created: Aug 30, 2023 4:01 am\n",
            "Latest: Aug 30, 2023 5:50 am\n",
            "Topic Owner: mengban\n",
            "Topic Content: I found some information but couldn’t find a definitive statement.\n",
            "But I found an example where tokenization results are different:\n",
            "from transformers import LlamaTokenizer, LlamaTokenizerFast\n",
            "\n",
            "model_name_or_path = \"ziqingyang/chinese-alpaca-2-7b\"\n",
            "sentence = \"吃了吗</s>\"\n",
            "\n",
            "llama_t = LlamaTokenizer.from_pretrained(model_name_or_path)\n",
            "llama_t_fast = LlamaTokenizerFast.from_pretrained(model_name_or_path)\n",
            "\n",
            "print(llama_t(sentence))\n",
            "print(llama_t_fast(sentence))\n",
            "the output is:\n",
            "{‘input_ids’: [1, 29871, 35302, 32013, 2], ‘attention_mask’: [1, 1, 1, 1, 1]}\n",
            "{‘input_ids’: [1, 29871, 32050, 34353, 2], ‘attention_mask’: [1, 1, 1, 1, 1]}\n",
            "and here is my env:\n",
            "transformers                  4.31.0\n",
            "tokenizers                    0.13.3\n",
            "Topic Timestamp: Aug 30, 2023 4:01 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: I set up a different batch_size, but the time of data processing has not changed\n",
            "Link: https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668\n",
            "Replies: 0\n",
            "Views: 528\n",
            "Activity: Created: Sep 1, 2021 1:10 pm\n",
            "Topic Owner: ccfeidao\n",
            "Topic Content: def tokenize_function(example):\n",
            "    return tokenizer(example[\"sentence1\"], truncation=True, max_length = 512)\n",
            "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, batch_size = 1024)\n",
            "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\"])\n",
            "tokenized_datasets.set_format(\"torch\")\n",
            "Topic Timestamp: Sep 1, 2021 1:10 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Should cls_token be [CLS] or <cls>?\n",
            "Link: https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140\n",
            "Replies: 3\n",
            "Views: 265\n",
            "Activity: Created: Oct 11, 2023 7:48 am\n",
            "Latest: Oct 11, 2023 9:23 am\n",
            "Topic Owner: raptorkwok\n",
            "Topic Content: If I want to create a tokenizer from scratch, should the special tokens use square brackets with uppercase letters, or angle brackets with lowercase letters?\n",
            "That is, should cls_token be [CLS] or <cls>?\n",
            "BertTokenizerFast uses the former one:\n",
            "github.com\n",
            "huggingface/transformers/blob/599db139f921f3af535052c860cb685cadae6fcd/src/transformers/tokenization_bert.py#L547\n",
            "def __init__(\n",
            "    self,\n",
            "    vocab_file,\n",
            "    do_lower_case=True,\n",
            "    do_basic_tokenize=True,\n",
            "    never_split=None,\n",
            "    unk_token=\"[UNK]\",\n",
            "    sep_token=\"[SEP]\",\n",
            "    pad_token=\"[PAD]\",\n",
            "    cls_token=\"[CLS]\",\n",
            "    mask_token=\"[MASK]\",\n",
            "    tokenize_chinese_chars=True,\n",
            "    max_length=None,\n",
            "    pad_to_max_length=False,\n",
            "    stride=0,\n",
            "    truncation_strategy=\"longest_first\",\n",
            "    add_special_tokens=True,\n",
            "    **kwargs\n",
            "):\n",
            "    super(BertTokenizerFast, self).__init__(\n",
            "while many other examples on the Internet uses the latter one.\n",
            "Topic Timestamp: Oct 11, 2023 7:48 am\n",
            "Replies:\n",
            "  - Username: Sandy1857\n",
            "    Answer: It could be anything you want it to be since you’re making the tokenizer from scratch.\n",
            "    Timestamp: Oct 11, 2023 8:34 am\n",
            "  - Username: raptorkwok\n",
            "    Answer: thanks. Also, is bos_token and eos_token necessary? In Bert’s implementation, these two tokens are not defined.\n",
            "    Timestamp: Oct 11, 2023 8:44 am\n",
            "  - Username: Sandy1857\n",
            "    Answer: It’s not needed for Bert, which does not generate text auto-regressively. So it depends on your application I guess.\n",
            "    Timestamp: Oct 11, 2023 9:23 am\n",
            "----------------------------------------\n",
            "Title: Using a BertTokenizer when training a RobertaForMaskedLM\n",
            "Link: https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456\n",
            "Replies: 0\n",
            "Views: 512\n",
            "Activity: Created: Dec 8, 2022 10:51 am\n",
            "Topic Owner: Chrode\n",
            "Topic Content: Hello, I want to train from scrath a RobertaForMaskedLM model. But I need a character level tokenizer and I found one already, perfect for me. So I am wondering,\n",
            "can I re-use it?\n",
            "This is my RobertaForMaskedLM config:\n",
            "{\"architectures\": [\"RobertaForMaskedLM\"], \n",
            "\"attention_probs_dropout_prob\": 0.1, \n",
            "\"hidden_act\": \"gelu\",\n",
            " \"hidden_dropout_prob\": 0.1,\n",
            " \"hidden_size\": 768, \"initializer_range\": 0.02, \n",
            "\"intermediate_size\": 3072, \n",
            "\"layer_norm_eps\": 1e-05, \n",
            "\"max_position_embeddings\": 40000, \n",
            "\"model_type\": \"roberta\",\n",
            " \"num_attention_heads\": 12,\n",
            " \"num_hidden_layers\": 12,\n",
            " \"type_vocab_size\": 1,\n",
            " \"vocab_size\": 30} # edited to be the same as the tokenizer\n",
            "This is the BertTokenizer config:\n",
            "{\n",
            "  \"version\": \"1.0\",\n",
            "  \"truncation\": {\n",
            "    \"direction\": \"Right\",\n",
            "    \"max_length\": 512,\n",
            "    \"strategy\": \"LongestFirst\",\n",
            "    \"stride\": 0\n",
            "  },\n",
            "  \"padding\": null,\n",
            "  \"added_tokens\": [\n",
            "    {\n",
            "      \"id\": 0,\n",
            "      \"content\": \"[PAD]\",\n",
            "      \"single_word\": false,\n",
            "      \"lstrip\": false,\n",
            "      \"rstrip\": false,\n",
            "      \"normalized\": false,\n",
            "      \"special\": true\n",
            "    },\n",
            "    {\n",
            "      \"id\": 1,\n",
            "      \"content\": \"[UNK]\",\n",
            "      \"single_word\": false,\n",
            "      \"lstrip\": false,\n",
            "      \"rstrip\": false,\n",
            "      \"normalized\": false,\n",
            "      \"special\": true\n",
            "    },\n",
            "    {\n",
            "      \"id\": 2,\n",
            "      \"content\": \"[CLS]\",\n",
            "      \"single_word\": false,\n",
            "      \"lstrip\": false,\n",
            "      \"rstrip\": false,\n",
            "      \"normalized\": false,\n",
            "      \"special\": true\n",
            "    },\n",
            "    {\n",
            "      \"id\": 3,\n",
            "      \"content\": \"[SEP]\",\n",
            "      \"single_word\": false,\n",
            "      \"lstrip\": false,\n",
            "      \"rstrip\": false,\n",
            "      \"normalized\": false,\n",
            "      \"special\": true\n",
            "    },\n",
            "    {\n",
            "      \"id\": 4,\n",
            "      \"content\": \"[MASK]\",\n",
            "      \"single_word\": false,\n",
            "      \"lstrip\": false,\n",
            "      \"rstrip\": false,\n",
            "      \"normalized\": false,\n",
            "      \"special\": true\n",
            "    }\n",
            "  ],\n",
            "  \"normalizer\": {\n",
            "    \"type\": \"BertNormalizer\",\n",
            "    \"clean_text\": true,\n",
            "    \"handle_chinese_chars\": true,\n",
            "    \"strip_accents\": null,\n",
            "    \"lowercase\": false\n",
            "  },\n",
            "  \"pre_tokenizer\": {\n",
            "    \"type\": \"BertPreTokenizer\"\n",
            "  },\n",
            "  \"post_processor\": {\n",
            "    \"type\": \"TemplateProcessing\",\n",
            "    \"single\": [\n",
            "      {\n",
            "        \"SpecialToken\": {\n",
            "          \"id\": \"[CLS]\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"Sequence\": {\n",
            "          \"id\": \"A\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"SpecialToken\": {\n",
            "          \"id\": \"[SEP]\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      }\n",
            "    ],\n",
            "    \"pair\": [\n",
            "      {\n",
            "        \"SpecialToken\": {\n",
            "          \"id\": \"[CLS]\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"Sequence\": {\n",
            "          \"id\": \"A\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"SpecialToken\": {\n",
            "          \"id\": \"[SEP]\",\n",
            "          \"type_id\": 0\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"Sequence\": {\n",
            "          \"id\": \"B\",\n",
            "          \"type_id\": 1\n",
            "        }\n",
            "      },\n",
            "      {\n",
            "        \"SpecialToken\": {\n",
            "          \"id\": \"[SEP]\",\n",
            "          \"type_id\": 1\n",
            "        }\n",
            "      }\n",
            "    ],\n",
            "    \"special_tokens\": {\n",
            "      \"[CLS]\": {\n",
            "        \"id\": \"[CLS]\",\n",
            "        \"ids\": [\n",
            "          2\n",
            "        ],\n",
            "        \"tokens\": [\n",
            "          \"[CLS]\"\n",
            "        ]\n",
            "      },\n",
            "      \"[SEP]\": {\n",
            "        \"id\": \"[SEP]\",\n",
            "        \"ids\": [\n",
            "          3\n",
            "        ],\n",
            "        \"tokens\": [\n",
            "          \"[SEP]\"\n",
            "        ]\n",
            "      }\n",
            "    }\n",
            "  },\n",
            "  \"decoder\": {\n",
            "    \"type\": \"WordPiece\",\n",
            "    \"prefix\": \"##\",\n",
            "    \"cleanup\": true\n",
            "  },\n",
            "  \"model\": {\n",
            "    \"type\": \"WordPiece\",\n",
            "    \"unk_token\": \"[UNK]\",\n",
            "    \"continuing_subword_prefix\": \"##\",\n",
            "    \"max_input_chars_per_word\": 100,\n",
            "    \"vocab\": {\n",
            "      \"[PAD]\": 0,\n",
            "      \"[UNK]\": 1,\n",
            "      \"[CLS]\": 2,\n",
            "      \"[SEP]\": 3,\n",
            "      \"[MASK]\": 4,\n",
            "      \"L\": 5,\n",
            "      \"A\": 6,\n",
            "      \"G\": 7,\n",
            "      \"V\": 8,\n",
            "      \"E\": 9,\n",
            "      \"S\": 10,\n",
            "      \"I\": 11,\n",
            "      \"K\": 12,\n",
            "      \"R\": 13,\n",
            "      \"D\": 14,\n",
            "      \"T\": 15,\n",
            "      \"P\": 16,\n",
            "      \"N\": 17,\n",
            "      \"Q\": 18,\n",
            "      \"F\": 19,\n",
            "      \"Y\": 20,\n",
            "      \"M\": 21,\n",
            "      \"H\": 22,\n",
            "      \"C\": 23,\n",
            "      \"W\": 24,\n",
            "      \"X\": 25,\n",
            "      \"U\": 26,\n",
            "      \"B\": 27,\n",
            "      \"Z\": 28,\n",
            "      \"O\": 29\n",
            "    }\n",
            "  }\n",
            "}\n",
            "Thank you very much! If you now how to create a character based tokenizer let me know please.\n",
            "Topic Timestamp: Dec 8, 2022 10:51 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Continuation token in pertained tokenizer bert-base-chinese\n",
            "Link: https://discuss.huggingface.co/t/continuation-token-in-pertained-tokenizer-bert-base-chinese/218\n",
            "Replies: 0\n",
            "Views: 510\n",
            "Activity: Created: Jul 11, 2020 10:21 am\n",
            "Topic Owner: jiayu\n",
            "Topic Content: I am a bit confused on the different tokens (with and without ## i.e. continuation prefix) in bert-base-chinese.\n",
            "In [1]: from transformers import AutoTokenizer\n",
            "\n",
            "In [2]: tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n",
            "\n",
            "In [3]: tokenizer.encode('恋爱ing', add_special_tokens=False)\n",
            "Out[3]: [2605, 4263, 10139]\n",
            "\n",
            "In [4]: tokenizer.save_pretrained('tokenizer')\n",
            "\n",
            "Out[4]:\n",
            "('tokenizer/vocab.txt',\n",
            " 'tokenizer/special_tokens_map.json',\n",
            " 'tokenizer/added_tokens.json')\n",
            "\n",
            "In [5]: !grep -n ing tokenizer/vocab.txt\n",
            "8222:##ing\n",
            "9108:##ting\n",
            "9310:booking\n",
            "9383:king\n",
            "9427:##ling\n",
            "9536:##ning\n",
            "9663:shopping\n",
            "9741:##king\n",
            "9756:##ding\n",
            "10062:ling\n",
            "10070:ｗedding\n",
            "10140:ing\n",
            "...\n",
            "\n",
            "In [6]: !grep -n 爱 tokenizer/vocab.txt\n",
            "4264:爱\n",
            "17321:##爱\n",
            "Here it shows that 爱 is tokenized into 4263 instead of 17320, indicating that for this Chinese pre-trained model the two character word is split separately; but then why do we still need the ##爱 token in the vocab?\n",
            "Topic Timestamp: Jul 11, 2020 10:21 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: T5v1.1 tokenizer legacy=False\n",
            "Link: https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250\n",
            "Replies: 0\n",
            "Views: 508\n",
            "Activity: Created: Feb 22, 2024 3:36 am\n",
            "Topic Owner: Nevermetyou\n",
            "Topic Content: Hello, I am using t5v1.1 right now and have a question about the arg legacy. The only thing I know is it involves fixing some bugs.\n",
            "legacy (bool, optional) — Whether or not the legacy behaviour of the tokenizer should be used. Legacy is before the merge of #24622 and #25224 which includes fixes to properly handle tokens that appear after special tokens. A simple example:\n",
            "legacy=True:\n",
            "But should I set it to false to use new behavior??\n",
            "Topic Timestamp: Feb 22, 2024 3:36 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False\n",
            "Link: https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930\n",
            "Replies: 0\n",
            "Views: 502\n",
            "Activity: Created: Oct 9, 2023 4:05 pm\n",
            "Topic Owner: TurboPascal\n",
            "Topic Content: How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False\n",
            "Topic Timestamp: Oct 9, 2023 4:05 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Using Tokenizer for integer data\n",
            "Link: https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835\n",
            "Replies: 0\n",
            "Views: 501\n",
            "Activity: Created: Jan 3, 2023 10:42 am\n",
            "Topic Owner: HasnainKhan\n",
            "Topic Content: Hey guys, I am working on a problem where I am having tabular data having more than 90 features and one target and all the features are in integers (continuous). I want to use pre-trained BERT, GPT2 but when it comes to the tokenizer the tokenizer is expecting the input in the text format. I can change the integer data in the text format like this;\n",
            "original_data = [1,2,3,4,5,…,94]\n",
            "transformed_data = [“1 2 3 4 5 …94”]\n",
            "Now if I pass the transformed_data to the tokenizer then surely it will work but I wanna know if someone tried to use transformers for this purpose and if yes, then what was the outcome, and how did the results look like?\n",
            "Topic Timestamp: Jan 3, 2023 10:42 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: TemplateProcessing for encoder-decoder\n",
            "Link: https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135\n",
            "Replies: 0\n",
            "Views: 500\n",
            "Activity: Created: Nov 16, 2022 12:30 am\n",
            "Topic Owner: astariul\n",
            "Topic Content: I’ve seen the docs on how to define the encoding format with TemplateProcessing. For example for BERT it would be :\n",
            "from tokenizers.processors import TemplateProcessing\n",
            "\n",
            "tokenizer.post_processor = TemplateProcessing(\n",
            "    single=\"[CLS] $A [SEP]\",\n",
            "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
            "    special_tokens=[(\"[CLS]\", 1), (\"[SEP]\", 0)],\n",
            ")\n",
            "But what about encoder-decoder ? What I want to achieve is :\n",
            "The encoder get a sentence with CLS and SEP : <cls> sen1 <sep>\n",
            "The decoder get a sentence with BOS and EOS : <s> sen2 </s>\n",
            "I call my tokenizer like : x = tokenizer([sen1], text_target=[sen2])\n",
            "So my question is : how can I define TemplateProcessing to achieve this kind of format ?\n",
            "Topic Timestamp: Nov 16, 2022 12:30 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Token Offsets in Rust vs. Python\n",
            "Link: https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949\n",
            "Replies: 1\n",
            "Views: 354\n",
            "Activity: Created: Apr 27, 2023 7:21 pm\n",
            "Latest: Apr 27, 2023 8:17 pm\n",
            "Topic Owner: sbrunk\n",
            "Topic Content: Hi,\n",
            "I’m working on Scala bindings for the (Rust) Tokenizers library. While writing tests, I realized that the offsets for certain strings seem to differ when using the Python bindings and when calling the underlying Rust implementation directly.\n",
            "This can also be seen directly in the quicktour docs when looking at the offsets of a “”:\n",
            "print(output.offsets[9])\n",
            "# (26, 27)\n",
            "println!(\"{:?}\", output.get_offsets()[9]);\n",
            "// (26, 30)\n",
            "I suspect the difference is due to how strings are represented as bytes vs. characters, i.e. calling len() on a rust str returns the number of bytes:\n",
            "let input = \"😁\";\n",
            "println!(\"{}\", input.len());\n",
            "// 4\n",
            "There’s also OffsetType which might be related but I haven’t been able to figure out yet where this is handled on the Python side.\n",
            "Any hints appreciated. Thanks!\n",
            "Topic Timestamp: Apr 27, 2023 7:21 pm\n",
            "Replies:\n",
            "  - Username: sbrunk\n",
            "    Answer: I just realized that there’s also encode_char_offsets and the the encode method in Python calls it.\n",
            "    Timestamp: Apr 27, 2023 8:17 pm\n",
            "----------------------------------------\n",
            "Title: How do we customize the number of entites for NER pretrained model?\n",
            "Link: https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085\n",
            "Replies: 1\n",
            "Views: 348\n",
            "Activity: Created: Oct 6, 2022 5:50 pm\n",
            "Latest: Oct 6, 2022 6:59 pm\n",
            "Topic Owner: dineshmane\n",
            "Topic Content: Hi,\n",
            "I am trying to re-train the model with custom dataset with X entites. I found the documentation link: AutoModels — transformers 3.0.2 documentation\n",
            "AutoModelForTokenClassification.from_pretrained(“bert-base-multilingual-cased”)\n",
            "Anyone knows what is the configuration to set the pre-trained huggingface model to X entities?\n",
            "Thank you,\n",
            "Dinesh\n",
            "Topic Timestamp: Oct 6, 2022 5:50 pm\n",
            "Replies:\n",
            "  - Username: nbroad\n",
            "    Answer: AutoModelForTokenClassification.from_pretrained(“bert-base-multilingual-cased”, num_labels=X)\n",
            "    Timestamp: Oct 6, 2022 6:59 pm\n",
            "----------------------------------------\n",
            "Title: Running train_new_from_iterator to train a tokenizer is very slow\n",
            "Link: https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333\n",
            "Replies: 1\n",
            "Views: 345\n",
            "Activity: Created: Mar 29, 2024 5:02 pm\n",
            "Latest: Apr 13, 2024 10:04 am\n",
            "Topic Owner: tehranixyz\n",
            "Topic Content: Hi everyone, I’m running train_new_from_iterator on top of microsoft/unixcoder-base tokenizer, which itself is a RobertaTokenizer to train the tokenizer (it’s a BPE tokenizer).\n",
            "The problem is that the training is very slow.\n",
            "The tokenizer training process takes ages to finish the Count pair part.\n",
            "At first, it shows it is able to count around 1M pairs out of 14M pairs in about 1 hour, but as it continues, it slows down. It’s now running for two days and only counted 9M pairs, and still, 5M pairs are left.\n",
            "Is there any way to speed up the tokenizer training process?\n",
            "Here is the code that I’m using for training the tokenizer:\n",
            "from dataset import concatenate_datasets\n",
            "\n",
            "def get_training_corpus(train_set):\n",
            "    for start_idx in range(0, len(train_set), 1000):\n",
            "        samples = train_set[start_idx: start_idx + 1000]\n",
            "        yield samples['text']\n",
            "        \n",
            "def train_tokenizer(tokenizer, datasets):\n",
            "    trainset = concatenate_datasets([dataset['train'] for dataset in datasets]).shuffle()\n",
            "    # remove non text columns\n",
            "    trainset = trainset.remove_columns([                                                             \n",
            "        col for col in trainset.column_names if col != \"text\"                                        \n",
            "    ])  \n",
            "    training_corpus = get_training_corpus(train_set=trainset)\n",
            "    tokenizer = tokenizer.train_new_from_iterator(training_corpus, len(tokenizer))\n",
            "    return tokenizer\n",
            "It seems like not all processors are being used for training the tokenizer. But I’m not sure what I can do to utilize all processors.\n",
            "Any help is appreciated.\n",
            "Topic Timestamp: Mar 29, 2024 5:02 pm\n",
            "Replies:\n",
            "  - Username: beachbuggyracing\n",
            "    Answer: (Beach Buggy Racing Mod APK) amps up the excitement of beach racing with unlocked features. Similar to a speeding train, it accelerates your gaming adventure. Yet, like the slow process of training a tokenizer, unlocking levels and customizing your buggy may take time. But once you’re set, the thrill of racing on sandy tracks or coding swiftly is unmatched.\n",
            "    Timestamp: Apr 13, 2024 10:04 am\n",
            "----------------------------------------\n",
            "Title: Huggingface inference API issue\n",
            "Link: https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307\n",
            "Replies: 0\n",
            "Views: 486\n",
            "Activity: Created: Jan 10, 2023 4:01 pm\n",
            "Topic Owner: lukasweber\n",
            "Topic Content: Error:\n",
            "Can’t load tokenizer using from_pretrained, please update its configuration: No such file or directory (os error 2)\n",
            "These are the files that were uploaded in the huggingface models. I also checked the files of other models (like BERT-base-NER) and they also have similar files.\n",
            "I am not sure why this error raises…\n",
            "Hoping for help! Thanks!\n",
            "Topic Timestamp: Jan 10, 2023 4:01 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers\n",
            "Link: https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872\n",
            "Replies: 0\n",
            "Views: 485\n",
            "Activity: Created: Jun 12, 2023 12:27 am\n",
            "Topic Owner: abhinavp\n",
            "Topic Content: Reposting this here from the transformers forum because I got no answer there:\n",
            "Hi,\n",
            "I trained a simple WhitespaceSplit/WordLevel tokenizer using the tokenizers library. I added padding by calling enable_padding(pad_token=\"<pad>\") on the Tokenizer instance. Then I saved it to a JSON file and then loaded it into transformers using the instructions here:\n",
            "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")\n",
            "When using the tokenizers.Tokenizer object directly, encode correctly adds the padding tokens. However, if I try padding when tokenizing using the PreTrainedTokenizerFast instance, I get the exception:\n",
            "ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
            "Sure enough, if I follow the instructions and add the pad token as a special token, it works. But I want the tokenizer to work out of the box, exactly as the equivalent tokenizer.Tokenizer instance does, including in terms of padding behavior.\n",
            "Why is this not the case? Why do I have to enable padding for the tokenizer.Tokenizer instance, and then again for the PreTrainedTokenizerFast instance? Am I doing something wrong or missing something?\n",
            "To reproduce the problem, you can use the code below. Most of it is from the tokenizers Quicktour, so you’ll need to download the data files as per the instructions there (or modify files if using your own files). The rest is from the official transformers docs on how to load a tokenizer from tokenizers into transformers:\n",
            "from tokenizers import BpeTrainer, Tokenizer\n",
            "from tokenizers.models import BPE\n",
            "from tokenizers.pre_tokenizers import Whitespace\n",
            "from transformers import PreTrainedTokenizerFast\n",
            "\n",
            "files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
            "sentences = [\"Hello, y'all!\", \"How are you 😁 ?\"]\n",
            "\n",
            "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
            "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
            "tokenizer.pre_tokenizer = Whitespace()\n",
            "\n",
            "tokenizer.train(files, trainer)\n",
            "\n",
            "# Enable padding\n",
            "tokenizer.enable_padding(pad_id=3, pad_token=\"[PAD]\")\n",
            "\n",
            "# Now use this tokenizer to tokenize a couple of sentences.\n",
            "output = tokenizer.encode_batch(sentences)\n",
            "\n",
            "# The output is padded, as it should be:\n",
            "print(output[0].tokens)\n",
            "# ['Hello', ',', 'y', \"'\", 'all', '!']\n",
            "print(output[1].tokens)\n",
            "# ['How', 'are', 'you', '[UNK]', '?', '[PAD]']\n",
            "\n",
            "# But now let's say we load the tokenizer into transformers- let's try loading it directly from the tokenizer object:\n",
            "\n",
            "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
            "\n",
            "# Tokenize two strings of different token length with padding\n",
            "fast_output = fast_tokenizer(sentences, padding=True)\n",
            "This gives us the error:\n",
            "Using pad_token, but it is not set yet.\n",
            "Traceback (most recent call last):\n",
            "  File \"<stdin>\", line 1, in <module>\n",
            "  File \"/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2548, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "  File \"/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2634, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "  File \"/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2816, in batch_encode_plus\n",
            "    padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(\n",
            "  File \"/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2453, in _get_padding_truncation_strategies\n",
            "    raise ValueError(\n",
            "ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
            "We can resolve the issue by explicitly specifying the special tokens when initializing the PreTrainedTokenizerFast:\n",
            "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,  pad_token=\"[PAD]\", unk_token=\"[UNK]\")\n",
            "\n",
            "# Now padding works as expected\n",
            "fast_output = fast_tokenizer(sentences, padding=True)\n",
            "\n",
            "print(fast_output[0].tokens)\n",
            "# ['Hello', ',', 'y', \"'\", 'all', '!']\n",
            "print(fast_output[1].tokens)\n",
            "# ['How', 'are', 'you', '[UNK]', '?', '[PAD]']\n",
            "The code above uses the tokenizer_object parameter to load the fast tokenizer as a PreTrainedTokenizerFast instance, but as you can confirm for yourselves, the same behavior occurs if you first save the tokenizer to file, then load it into PreTrainedTokenizerFast using the tokenizer_file parameter instead.\n",
            "Bottom line: I don’t understand, if the padding information is already in the tokenizer (or in the saved tokenizer config file), why I should need to explicitly specify the padding token again when transferring the tokenizer. This introduces a lot of totally unnecessary friction into what should be a painless process. The tokenizer object/config should be self-contained. I should not have to re-hardcode what the padding token is in the code that loads it into transformers, if that information is already encapsulated in the tokenizer object or its saved config file, any more than I should need to specify the vocab file, or the pretokenizers to use, etc. That’s the whole point of the tokenizer object/config file: to uniquely determine the behavior the tokenizer.\n",
            "Am I doing something wrong, or is this just how this works?\n",
            "Topic Timestamp: Jun 12, 2023 12:27 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: BERT WordPiece Tokenizer: some matras missing after tokenization for Hindi Language #572\n",
            "Link: https://discuss.huggingface.co/t/bert-wordpiece-tokenizer-some-matras-missing-after-tokenization-for-hindi-language-572/2936\n",
            "Replies: 0\n",
            "Views: 482\n",
            "Activity: Created: Dec 23, 2020 12:39 pm\n",
            "Topic Owner: akshat311\n",
            "Topic Content: We trained our Bert WordPiece Tokenizer using the following dataset : https://drive.google.com/file/d/12MbWKERa7QPfI9F-xnMgIgAhjbN8EaQK/view?usp=sharing\n",
            "The words ending with certain matras (eg. ए - े ) are missing these matras in the tokens.\n",
            "For Eg : for the sentence \"अपने पोस्ट ऑफिस में 420 पदों पर भर्ती \"\n",
            "the tokens were as follows : [‘अपन’, ‘पोस्ट’ , ‘ऑफिस’ , ‘म’ , ‘420’ , ‘पदो’ , ‘पर’ , ‘भर्ती’ ]\n",
            "The first word (अपने) and third word (में ) are missing the ए ki matra after tokenization.\n",
            "Even for the pretrained huggingface tokenizers, all the uncased tokenizers have the exact same issue. Words ending with ए ki matra are missing the matra after tokenization. However, cased pretrained tokenizers are working fine. (“bert-base-multilingual-cased” is working perfectly fine, however, “bert-base-multilingual-uncased” has the same issue mentioned above.)\n",
            "Tokenization result for “bert-base-multilingual-cased” : [[‘अपने’, ‘प’, ‘##ो’, ‘##स्ट’, ‘ऑफ’, ‘##िस’, ‘में’, ‘420’, ‘##0’, ‘पद’, ‘##ों’, ‘पर’, ‘भर’, ‘##्ती’]\n",
            "Tokenization result for “bert-base-multilingual-uncased” : [‘अपन’, ‘प’, ‘##ो’, ‘##सट’, ‘ऑफ’, ‘##िस’, ‘म’, ‘420’, ‘##0’, ‘पद’, ‘##ो’, ‘पर’, ‘भर’, ‘##ती’]\n",
            "Why are these matras getting omitted after tokenization for our own tokenizer and the uncased bert tokenizers?\n",
            "Topic Timestamp: Dec 23, 2020 12:39 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: <unk> token in the output instead curly braces\n",
            "Link: https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653\n",
            "Replies: 0\n",
            "Views: 475\n",
            "Activity: Created: Mar 25, 2023 6:49 pm\n",
            "Topic Owner: MihoZaki\n",
            "Topic Content: Hello everyone! hope you all are well. i need some help because to be honest im super beginner and it got a bit confusing.\n",
            "I’m fine-tuning T5 for the task of generating mongodb queries as an ouput. i finished the training and when i loaded the model to test it, the output was missing Curly braces.\n",
            "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
            "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
            "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=False)[0]\n",
            "print(decoded_output)\n",
            "predicted_Query = nltk.sent_tokenize(decoded_output.strip())[0]\n",
            "\n",
            "print(predicted_Query)\n",
            "the code above would output this:\n",
            "<pad> db.movies.find(<unk>\"title\": \"The Poor Little Rich Girl\"<unk>, <unk>\"writers\": 1<unk>)</s>\n",
            "<pad> db.movies.find(<unk>\"title\": \"The Poor Little Rich Girl\"<unk>, <unk>\"writers\": 1<unk>)</s>\n",
            "The special token is supposed to be “{” or “}” . is there anyway to fix that ? also does that mean that during training it also outputted <unk> tokens?\n",
            "Thanks for you time!\n",
            "Topic Timestamp: Mar 25, 2023 6:49 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Why Bert-chinese use do_lower_case=False?\n",
            "Link: https://discuss.huggingface.co/t/why-bert-chinese-use-do-lower-case-false/2952\n",
            "Replies: 0\n",
            "Views: 474\n",
            "Activity: Created: Dec 24, 2020 1:38 pm\n",
            "Topic Owner: Fei-Wang\n",
            "Topic Content: Some Chinese Text has some English words, for example: “Apples是苹果的复数形式。”. I have questions about how to tokenize the text:\n",
            "why Chinese Bert Case sensitive, but I can’t find even ‘A’ in vocab.txt\n",
            "Because English words in Chinese vocab.txt is few, should I use wordpiece tokenizer as default, like \"[‘apple’, ‘##s’, ‘是’, ‘苹’, …]\"or split to char to tokenize, like “[‘a’, ‘p’, ‘p’, ‘l’, ‘e’, ‘s’, ‘是’, ‘苹’, …]”?\n",
            "Topic Timestamp: Dec 24, 2020 1:38 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Need clarity on “padding” parameter in Bert Tokenizer\n",
            "Link: https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444\n",
            "Replies: 0\n",
            "Views: 469\n",
            "Activity: Created: Dec 8, 2022 7:37 am\n",
            "Topic Owner: shivammavihs\n",
            "Topic Content: Hi All,\n",
            "I have been fine tuning a BERT model for sentence classification. In training, while tokenization I had passed these parameters padding=\"max_length\", truncation=True, max_length=150 but while inferencing it is still predicting even if padding=\"max_length\" parameter is not being passed.\n",
            "Surprisingly, predictions are same in both the cases when padding=\"max_length\" is passed or not but if padding=\"max_length\" is not being passed, inferencing is much faster.\n",
            "So, I need some clarity on the parameter “padding” in Bert Tokenizer. Can someone help me to understand how bert is able to predict even without the padding since the length of the sentences will differ and does it have any negative consequences If padding=\"max_length\" is not passed while inferencing? Any help would be highly appreciated.\n",
            "Thanks\n",
            "Topic Timestamp: Dec 8, 2022 7:37 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Scala/JVM Bindings for Tokenizers\n",
            "Link: https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393\n",
            "Replies: 0\n",
            "Views: 468\n",
            "Activity: Created: May 10, 2023 10:57 am\n",
            "Topic Owner: sbrunk\n",
            "Topic Content: I’ve just published Scala bindings for the Rust Tokenizers library to be able to use them on the JVM. Currently, it can only load and run pre-trained tokenizers. Training is not yet possible.\n",
            "While it is currently focused on Scala it should be straightforward to add pure Java support as well if there is interest in that.\n",
            "Here’s the project: GitHub - sbrunk/tokenizers-scala: Scala bindings for Hugging Face Tokenizers\n",
            "Topic Timestamp: May 10, 2023 10:57 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: EM training on unigram tokenizer taking way longer than predicted\n",
            "Link: https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502\n",
            "Replies: 0\n",
            "Views: 467\n",
            "Activity: Created: Jun 23, 2022 11:39 am\n",
            "Latest: Jun 23, 2022 11:39 am\n",
            "Topic Owner: ddeerreekk\n",
            "Topic Content: I am trying to train my own Sentencepiece unigram tokenizer, but the EM training step is taking nearly ~2x predicted iterations to prune the vocabulary. I was wondering if this is normal and whether I should retry with larger shrink factor/vocabulary. Any hints? Thanks!\n",
            "[02:22:46] Pre-processing sequences                 █████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 0        /        0\n",
            "[00:11:52] Suffix array seeds                       █████████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 43102066 / 43102066\n",
            "[1d 00:50:16] EM training                              ██████████████████████████████████████████████████████████████████████████████████████████████████████████████████ 72       /       34\n",
            "Topic Timestamp: Jun 23, 2022 11:39 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: MBart50Tokenizer vs XLMRobertaTokenizer\n",
            "Link: https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498\n",
            "Replies: 0\n",
            "Views: 466\n",
            "Activity: Created: Jul 19, 2021 4:15 pm\n",
            "Topic Owner: yoshi\n",
            "Topic Content: Hello,\n",
            "I am currently working on the MBART50 many-to-one model for translation. I noticed that the tokenizer took a long time to encode the input when the length of the sequence became relatively important (cf attached picture). So, instead of using the MBart50TokenizerFast I used to use the XLMRobertaTokenizerFast.\n",
            "Actually, both are based on sentencepiece.bpe.model and have almost the same vocabulary. Using XLMRobertaTokenizerFast, I got practically the same translation as if I had used MBart50TokenizerFast BUT with a much shorter execution time. Does anyone have any idea why there is so much difference in execution time between these two tokenizers? (One can find the evolution of the execution time - encoding time - depending on the input-string length of the two tokenizers in attached picture)\n",
            "Thank you !\n",
            "Code used :\n",
            "mbart_tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
            "xlm_tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
            "mbart_token = mbart_tokenizer(input, return_tensors='pt')\n",
            "xlm_token = xlm_tokenizer(input, return_tensors='pt')\n",
            "Topic Timestamp: Jul 19, 2021 4:15 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to create a HF tokenizer’s vocab file from a BPE model’s merges.txt file?\n",
            "Link: https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737\n",
            "Replies: 0\n",
            "Views: 464\n",
            "Activity: Created: May 13, 2023 9:37 am\n",
            "Topic Owner: junliu44\n",
            "Topic Content: I have a BPE merges file that has been trained by another trainer. How can I convert it to the vocab format of the hf tokenizer? Because I don’t want to spend a lot of time retraining the BPE model.\n",
            "Topic Timestamp: May 13, 2023 9:37 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Converting JSON/dict to flatten string with indicator tokens\n",
            "Link: https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387\n",
            "Replies: 1\n",
            "Views: 325\n",
            "Activity: Created: Apr 21, 2023 7:41 pm\n",
            "Latest: Apr 21, 2023 7:42 pm\n",
            "Topic Owner: alvations\n",
            "Topic Content: Given an input like:\n",
            "{'example_id': 0,\n",
            " 'query': ' revent 80 cfm',\n",
            " 'query_id': 0,\n",
            " 'product_id': 'B000MOO21W',\n",
            " 'product_locale': 'us',\n",
            " 'esci_label': 'I',\n",
            " 'small_version': 0,\n",
            " 'large_version': 1,\n",
            " 'split': 'train',\n",
            " 'product_title': 'Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan',\n",
            " 'product_description': None,\n",
            " 'product_bullet_point': 'WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air\\nDesigned to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace\\nDetachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation\\nThis Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan\\n0.35 amp',\n",
            " 'product_brand': 'Panasonic',\n",
            " 'product_color': 'White'}\n",
            "The goal is to output something that looks like:\n",
            "Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan [TITLE] Panasonic [BRAND] White [COLOR] WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air [SEP] Designed to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace [SEP] Detachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation [SEP] This Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan [SEP] 0.35 amp [BULLETPOINT]\n",
            "There’s a few operations going on to generate the desired output following the rules:\n",
            "If the values in the dictionary is None, don’t add the content to the output string\n",
            "If the values contains newline \\n substitute them with [SEP] tokens\n",
            "Concatenate the strings with in order that user specified, e.g. above follows the order [\"product_title\", \"product_brand\", \"product_color\", \"product_bullet_point\", \"product_description\"]\n",
            "I’ve tried this that kinda works but the function I’ve written looks a little to hardcoded to look through the wanted keys and concatenate and manipulate the strings.\n",
            "item1 = {'example_id': 0,\n",
            " 'query': ' revent 80 cfm',\n",
            " 'query_id': 0,\n",
            " 'product_id': 'B000MOO21W',\n",
            " 'product_locale': 'us',\n",
            " 'esci_label': 'I',\n",
            " 'small_version': 0,\n",
            " 'large_version': 1,\n",
            " 'split': 'train',\n",
            " 'product_title': 'Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan',\n",
            " 'product_description': None,\n",
            " 'product_bullet_point': 'WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air\\nDesigned to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace\\nDetachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation\\nThis Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan\\n0.35 amp',\n",
            " 'product_brand': 'Panasonic',\n",
            " 'product_color': 'White'}\n",
            "\n",
            "item2 = {'example_id': 198,\n",
            " 'query': '# 2 pencils not sharpened',\n",
            " 'query_id': 6,\n",
            " 'product_id': 'B08KXRY4DG',\n",
            " 'product_locale': 'us',\n",
            " 'esci_label': 'S',\n",
            " 'small_version': 1,\n",
            " 'large_version': 1,\n",
            " 'split': 'train',\n",
            " 'product_title': 'AHXML#2 HB Wood Cased Graphite Pencils, Pre-Sharpened with Free Erasers, Smooth write for Exams, School, Office, Drawing and Sketching, Pack of 48',\n",
            " 'product_description': \"<b>AHXML#2 HB Wood Cased Graphite Pencils, Pack of 48</b><br><br>Perfect for Beginners experienced graphic designers and professionals, kids Ideal for art supplies, drawing supplies, sketchbook, sketch pad, shading pencil, artist pencil, school supplies. <br><br><b>Package Includes</b><br>- 48 x Sketching Pencil<br> - 1 x Paper Boxed packaging<br><br>Our high quality, hexagonal shape is super lightweight and textured, producing smooth marks that erase well, and do not break off when you're drawing.<br><br><b>If you have any question or suggestion during using, please feel free to contact us.</b>\",\n",
            " 'product_bullet_point': '#2 HB yellow, wood-cased pencils:Box of 48 count. Made from high quality real poplar wood and 100% genuine graphite pencil core. These No 2 pencils come with 100% Non-Toxic latex free pink top erasers.\\nPRE-SHARPENED & EASY SHARPENING: All the 48 count pencils are pre-sharpened, ready to use when get it, saving your time of preparing.\\nThese writing instruments are hexagonal in shape to ensure a comfortable grip when writing, scribbling, or doodling.\\nThey are widely used in daily writhing, sketching, examination, marking, and more, especially for kids and teen writing in classroom and home.#2 HB wood-cased yellow pencils in bulk are ideal choice for school, office and home to maintain daily pencil consumption.\\nCustomer service:If you are not satisfied with our product or have any questions, please feel free to contact us.',\n",
            " 'product_brand': 'AHXML',\n",
            " 'product_color': None}\n",
            "\n",
            "\n",
            "def product2str(row, keys):\n",
            "    key2token = {'product_title': '[TITLE]', \n",
            "     'product_brand': '[BRAND]', \n",
            "     'product_color': '[COLOR]',\n",
            "     'product_bullet_point': '[BULLETPOINT]', \n",
            "     'product_description': '[DESCRIPTION]'}\n",
            "    \n",
            "    output = \"\"\n",
            "    for k in keys:\n",
            "        content = row[k]\n",
            "        if content:\n",
            "            output += content.replace('\\n', ' [SEP] ') + f\" {key2token[k]} \"\n",
            "\n",
            "    return output.strip()\n",
            "\n",
            "product2str(item2, keys=['product_title', 'product_brand', 'product_color',\n",
            "                        'product_bullet_point', 'product_description'])\n",
            "Q: Is there some sort of native CPython JSON to str flatten functions/recipes that can achieve similar results to the product2str function?\n",
            "Q: Or is there already some function/pipeline in tokenizers library tokenizers · PyPI that can flatten a JSON/dict into tokens?\n",
            "Topic Timestamp: Apr 21, 2023 7:41 pm\n",
            "Replies:\n",
            "  - Username: alvations\n",
            "    Answer: Also asked on python - Converting JSON/dict to flatten string with indicator tokens - Stack Overflow\n",
            "    Timestamp: Apr 21, 2023 7:42 pm\n",
            "----------------------------------------\n",
            "Title: Enhaced word_ids() API for Chinese or CJK languages?\n",
            "Link: https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662\n",
            "Replies: 0\n",
            "Views: 454\n",
            "Activity: Created: Jun 2, 2022 5:52 pm\n",
            "Topic Owner: cramraj8\n",
            "Topic Content: Is there an API like tokenizer.word_ids() to map/align sub-word to whole-word in CJK languages ? The word_ids() is useful for white-space tokenizable languages like Farsi and Russian. But I have difficulty in mapping Chinese to get the whole-word vocabulary embeddings.\n",
            "Topic Timestamp: Jun 2, 2022 5:52 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Encode_plus Pretokenized input seuqence must be Union\n",
            "Link: https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449\n",
            "Replies: 0\n",
            "Views: 453\n",
            "Activity: Created: Nov 21, 2022 2:19 pm\n",
            "Topic Owner: Adelija\n",
            "Topic Content: Hello,\n",
            "I have a problem with arguments types for the encode_plus method.\n",
            "The code works when we put, for example, df[‘labels’][0] or any other index (so when it is list of strings), but not with the whole column (df[‘labels’].to_list(), so when it is list of lists of strings).\n",
            "We need to tokenize the whole data frame, so our question is - is there another function that can do that, or we need to iterate through the data frame and call encode_plus() on each separate cell value?\n",
            "Thanks\n",
            "Topic Timestamp: Nov 21, 2022 2:19 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Is there a way to save a pre-compiled AutoTokenizer?\n",
            "Link: https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581\n",
            "Replies: 1\n",
            "Views: 318\n",
            "Activity: Created: Jan 25, 2024 12:25 pm\n",
            "Latest: Jan 25, 2024 12:26 pm\n",
            "Topic Owner: alvations\n",
            "Topic Content: Sometimes, we’ll have to do something like this to extend a pre-trained tokenizer:\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "from datasets import load_dataset\n",
            "\n",
            "\n",
            "ds_de = load_dataset(\"mc4\", 'de')\n",
            "ds_fr = load_dataset(\"mc4\", 'fr')\n",
            "\n",
            "de_tokenizer = tokenizer.train_new_from_iterator(\n",
            "    ds_de['text'],vocab_size=50_000\n",
            ")\n",
            "\n",
            "fr_tokenizer = tokenizer.train_new_from_iterator(\n",
            "    ds_fr['text'],vocab_size=50_000\n",
            ")\n",
            "\n",
            "new_tokens_de = set(de_tokenizer.vocab).difference(tokenizer.vocab)\n",
            "new_tokens_fr = set(fr_tokenizer.vocab).difference(tokenizer.vocab)\n",
            "new_tokens = set(new_tokens_de).union(new_tokens_fr)\n",
            "\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\n",
            "    'moussaKam/frugalscore_tiny_bert-base_bert-score'\n",
            ")\n",
            "\n",
            "tokenizer.add_tokens(list(new_tokens))\n",
            "\n",
            "tokenizer.save_pretrained('frugalscore_tiny_bert-de-fr')\n",
            "And then when loading the tokenizer,\n",
            "tokenizer = AutoTokenizer.from_pretrained(\n",
            "  'frugalscore_tiny_bert-de-fr', local_files_only=True\n",
            ")\n",
            "It takes pretty long to load from %%time in a Jupyter cell:\n",
            "CPU times: user 34min 20s\n",
            "Wall time: 34min 22s\n",
            "I guess this is due to regex compilation for the added tokens which was also raised in Loading of Tokenizer is really slow when there are lots of additional tokens · Issue #914 · huggingface/tokenizers · GitHub\n",
            "I think it’s okay since it’ll load once and the work can be done without redoing the regex compiles.\n",
            "But, is there a way to just save the tokenizer in binary form and avoid the whole regex compilation the next time?\n",
            "Topic Timestamp: Jan 25, 2024 12:25 pm\n",
            "Replies:\n",
            "  - Username: alvations\n",
            "    Answer: Also asked on nlp - Is there a way to save a pre-compiled AutoTokenizer? - Stack Overflow\n",
            "    Timestamp: Jan 25, 2024 12:26 pm\n",
            "----------------------------------------\n",
            "Title: Issues with BPE tokenizer\n",
            "Link: https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381\n",
            "Replies: 2\n",
            "Views: 260\n",
            "Activity: Created: Jan 24, 2024 8:49 am\n",
            "Latest: Jan 24, 2024 4:09 pm\n",
            "Topic Owner: Imran1\n",
            "Topic Content: I attempted to train the BPE Hugging Face tokenizer for Pashto from scratch, but it is not decoding words correctly.\n",
            "Topic Timestamp: Jan 24, 2024 8:49 am\n",
            "Replies:\n",
            "  - Username: malikarumi\n",
            "    Answer: No one can help you with such limited information.\n",
            "I am unfamiliar with Pashto. Does the official documentation say anything about BPE?\n",
            "Is it possible that Pashto has already been pre trained with a different tokenizer?\n",
            "Post your code so we can understand what the software is trying to tell you.\n",
            "    Timestamp: Jan 24, 2024 3:56 pm\n",
            "  - Username: Imran1\n",
            "    Answer: I solve the issues.\n",
            "    Timestamp: Jan 24, 2024 4:09 pm\n",
            "----------------------------------------\n",
            "Title: How to concatenate an answer to multiple choices after padded tokenization\n",
            "Link: https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110\n",
            "Replies: 0\n",
            "Views: 441\n",
            "Activity: Created: Nov 15, 2022 2:52 pm\n",
            "Topic Owner: drt\n",
            "Topic Content: I am evaluating a LM model on multiple choice questions. I want to dynamically concatenate each choice to the question after tokenization.\n",
            "An tokenization example:\n",
            "Q: How many hours are there in a day? —tokenize—> 23 67 52 13 78 [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "C1: 12 hours. —tokenize–> 312 [PAD] [PAD]\n",
            "C2: 24 hours. —tokenize—> 89 [PAD] [PAD]\n",
            "I’d like to make two question choice pairs:\n",
            "23 67 52 13 78 312 [PAD] [PAD] [PAD] [PAD]\n",
            "23 67 52 13 78 89 [PAD] [PAD] [PAD] [PAD]\n",
            "I currently do this at tokenization time – I concatenate the texts and feed to tokenizers.\n",
            "But this is very inconvenient when, i.e. I also want question and answers separately. Also, I think this is clumsy and will cause extra GPU memory consumption and extra space to save tokenized dataset on disk.\n",
            "Therefore, is there an elegant way to do this concatenation after tokenization?\n",
            "Topic Timestamp: Nov 15, 2022 2:52 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Why does tokenization take so long?\n",
            "Link: https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098\n",
            "Replies: 0\n",
            "Views: 79\n",
            "Activity: Created: Aug 13, 2024 4:13 am\n",
            "Topic Owner: Jason-Hwang1\n",
            "Topic Content: I found that tokenization steps before training takes longer time than training itself.\n",
            "Yes, Training involves GPU, but I thought tokenization is not that compute-intensive (just splitting the sentence into words and mapping words to ID and other substeps…) so I thought it should be bounded to the IO time for loading the raw dataset. But it takes much time to tokenize subset of Wikipedia for more than 2 hours.\n",
            "Can someone give me the reason why tokenization steps takes so long?\n",
            "The code that I used for tokenization is as below. I also tried multiprocessing but it doesn’t make meaningful difference.\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
            "\n",
            "    def encode_example(example):\n",
            "        return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)  # Reduce max_length to save memory\n",
            "\n",
            "    # Tokenize dataset\n",
            "    dataset = dataset.map(encode_example, batched=True)\n",
            "Topic Timestamp: Aug 13, 2024 4:13 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Application of TFBertTokenizer\n",
            "Link: https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437\n",
            "Replies: 0\n",
            "Views: 435\n",
            "Activity: Created: Nov 21, 2022 10:31 am\n",
            "Topic Owner: realonbebeto\n",
            "Topic Content: I wish to use the TFBertTokenizer as a layer in my custom model where there is:\n",
            "Input Layer - Receiving String\n",
            "Tokenizer to process string tensor\n",
            "Bert model\n",
            "Other layers of my custom layer\n",
            "The model is for classification problem and the intentions are to have a model that has both preprocessing as part of the model. I’m thinking this is possible since TFBertTokenizer is an in-graph tokenizer.\n",
            "Please help.\n",
            "Topic Timestamp: Nov 21, 2022 10:31 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Change bpe-dropout value on the fly?\n",
            "Link: https://discuss.huggingface.co/t/change-bpe-dropout-value-on-the-fly/1721\n",
            "Replies: 0\n",
            "Views: 427\n",
            "Activity: Created: Oct 24, 2020 11:39 am\n",
            "Topic Owner: esceptico\n",
            "Topic Content: Hello!\n",
            "Is there’s any way to change bpe-dropout value of tokenizer without reinitialization tokenizer from files?\n",
            "Many thanks!\n",
            "Topic Timestamp: Oct 24, 2020 11:39 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Overflowing Tokens in MarkupLM\n",
            "Link: https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217\n",
            "Replies: 0\n",
            "Views: 425\n",
            "Activity: Created: Mar 31, 2023 8:31 pm\n",
            "Latest: Mar 31, 2023 8:31 pm\n",
            "Topic Owner: jl2010\n",
            "Topic Content: When using the MarkupLMTokenizerFast there is an argument to return_overflowing_tokens. This makes sense since the model can only handle 512 tokens at a time. For example, if my data has 1024 tokens the tokenizer would return a tensor of size(2,512).\n",
            "My question is, does the model consider the overflowing tokens when it’s training? If so can you point me to where it’s actually training on that data? It’s not clear to me that this is happening and want to make sure either way.\n",
            "Thank you!\n",
            "Topic Timestamp: Mar 31, 2023 8:31 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unigram vocab_size doesn’t fit\n",
            "Link: https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856\n",
            "Replies: 0\n",
            "Views: 419\n",
            "Activity: Created: Nov 28, 2022 11:06 am\n",
            "Latest: Nov 28, 2022 11:33 am\n",
            "Topic Owner: Axllm\n",
            "Topic Content: Hi there, thanks for your amazing work !\n",
            "I encounter an issue when I’m using Unigram and UnigramTrainer.\n",
            "In fact I want to build a vocabulary of size 10 (based on vocabulary built with BPE)\n",
            "But I don’t understand why, using both train_from_iterator (with list and with iterator) and with train (from file name containing same information as in list) I can’t retrieve a vocab of wished size.\n",
            "Here is some code to work with:\n",
            "corpus = [\"table\", \"bleu\", \"cable\"]\n",
            "# tokenizers.__version__ == 0.13.2\n",
            "from tokenizers import Tokenizer\n",
            "from tokenizers.models import Unigram\n",
            "\n",
            "vocab = [\"a\", \"b\", \"c\", \"e\", \"l\", \"t\", \"u\", \"bl\", \"ble\", \"able\", \"cable\"]\n",
            "tokenizer = Tokenizer(Unigram())\n",
            "tokenizer.add_tokens(vocab)\n",
            "print(\"With added tokens\")\n",
            "print(tokenizer.get_vocab(with_added_tokens=True))\n",
            "print(tokenizer.get_vocab_size(with_added_tokens=True))\n",
            "print(\"Without added tokens\")\n",
            "print(tokenizer.get_vocab(with_added_tokens=False))\n",
            "print(tokenizer.get_vocab_size(with_added_tokens=False))\n",
            "\n",
            "from tokenizers.trainers import UnigramTrainer\n",
            "trainer = UnigramTrainer(vocab_size=10)#, initial_alphabet=[\"a\", \"b\", \"c\", \"e\", \"l\", \"t\", \"u\"])\n",
            "tokenizer.train_from_iterator(corpus, trainer=trainer)\n",
            "#tokenizer.train([\"corpus.txt\"], trainer=trainer)\n",
            "#tokenizer.train_from_iterator(iterator(), trainer=trainer)\n",
            "\n",
            "print(\"With added tokens\")\n",
            "print(dict(sorted(tokenizer.get_vocab(with_added_tokens=True).items(), key=lambda item: item[1])))\n",
            "print(tokenizer.get_vocab_size(with_added_tokens=True) )\n",
            "\n",
            "print(\"Without added tokens\")\n",
            "print(dict(sorted(tokenizer.get_vocab(with_added_tokens=False).items(), key=lambda item: item[1])))\n",
            "print(tokenizer.get_vocab_size(with_added_tokens=False) )\n",
            "And results associated :\n",
            "Topic Timestamp: Nov 28, 2022 11:06 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: I/O error calling ToenizersLibrary.createTokenizer in container\n",
            "Link: https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204\n",
            "Replies: 1\n",
            "Views: 296\n",
            "Activity: Created: Feb 14, 2024 6:35 pm\n",
            "Latest: Feb 23, 2024 4:33 pm\n",
            "Topic Owner: wnmills3\n",
            "Topic Content: I’m using code like:\n",
            "    static String DJL_MODEL = \"intfloat/multilingual-e5-base\";\n",
            "    static String DJL_PATH = \"djl://ai.djl.huggingface.pytorch/\" + DJL_MODEL;\n",
            "    static private HuggingFaceTokenizer huggingFaceTokenizer;\n",
            "...\n",
            "    static private HuggingFaceTokenizer getHuggingFaceTokenizer() {\n",
            "        if (huggingFaceTokenizer == null) {\n",
            "            huggingFaceTokenizer = HuggingFaceTokenizer.newInstance(DJL_MODEL,\n",
            "                getDJLConfig());\n",
            "        }\n",
            "        return huggingFaceTokenizer;\n",
            "    }\n",
            "and it works fine running from the command line. However, when executing the same code in a Docker container I am getting an error:\n",
            "I/O error Permission denied (os error 13)\n",
            "RuntimeException: I/O error Permission denied (os error 13)\n",
            "ai.djl.huggingface.tokenizers.jni.TokenizersLibrary.createTokenizer(Native Method)\n",
            "ai.djl.huggingface.tokenizers.HuggingFaceTokenizer.newInstance(HuggingFaceTokenizer.java: 109)\n",
            "I’m sure the issue has to do with permissions in the container, but I have no idea what happens when loading the instance so I don’t know which directory is needed to have its attributes changed. Unfortunately, there method resolves to a native library so I can’t figure out what is causing this error.\n",
            "Any help is greatly appreciated.\n",
            "Topic Timestamp: Feb 14, 2024 6:35 pm\n",
            "Replies:\n",
            "  - Username: wnmills3\n",
            "    Answer: Set the HF_HOME environment variable went starting the container to a directory that can be written to by the default user in the container.\n",
            "    Timestamp: Feb 23, 2024 4:33 pm\n",
            "----------------------------------------\n",
            "Title: Get intermediate tokens and merges used in tokenization\n",
            "Link: https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256\n",
            "Replies: 0\n",
            "Views: 415\n",
            "Activity: Created: Dec 1, 2023 1:40 pm\n",
            "Topic Owner: cwallenwein\n",
            "Topic Content: Hi Friends\n",
            "Is there a way to get intermediate tokens and merges used during BPE tokenization?\n",
            "Example:\n",
            "vocab: “a”, “b”, “c”, “ab”, “”, “abc”\n",
            "merges: “a b”, “b c”, “ab c”\n",
            "What I want: tokenize(“abc”): {“intermediate_tokens”: [“a”, “b”, “ab”, “c”], “intermediate_merges”: [“a b”]}\n",
            "I currently solve this by manually implementing BPE in Python, but my implementation is too slow\n",
            "Topic Timestamp: Dec 1, 2023 1:40 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Bug in Offset generation for Rupee symbol\n",
            "Link: https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655\n",
            "Replies: 0\n",
            "Views: 412\n",
            "Activity: Created: Jun 27, 2022 7:32 am\n",
            "Latest: Jun 27, 2022 7:34 am\n",
            "Topic Owner: Giriteja\n",
            "Topic Content: Hi, When I am using the rupee symbol in a sentence Offset is dividing that symbol into 3 different symbols but instead of having (0,1)(1,2)(2,3), it is giving (0,1)(0,1)(0,1) which is causing issues in a mismatch between actual words and generated labels.For example\n",
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(‘distilroberta-base’, add_prefix_space=True)\n",
            "sent=“total amount that need to be paid is ₹ 500”\n",
            "words=sent.split()\n",
            "output=tokenizer(words, is_split_into_words=True,return_offsets_mapping=True)\n",
            "tokens=output.tokens()\n",
            "offset=output[‘offset_mapping’]\n",
            "for token,offset in zip(tokens,offset):\n",
            "print(token,“----->”,offset)\n",
            "I am getting the following output\n",
            "-----> (0, 0)\n",
            "Ġtotal -----> (0, 5)\n",
            "Ġamount -----> (0, 6)\n",
            "Ġthat -----> (0, 4)\n",
            "Ġneed -----> (0, 4)\n",
            "Ġto -----> (0, 2)\n",
            "Ġbe -----> (0, 2)\n",
            "Ġpaid -----> (0, 4)\n",
            "Ġis -----> (0, 2)\n",
            "Ġâ -----> (0, 1) #problem\n",
            "Ĥ -----> (0, 1)#problem\n",
            "¹ -----> (0, 1)#poblem\n",
            "Ġ500 -----> (0, 3)\n",
            "-----> (0, 0)\n",
            "As you can see above rupee symbol got divided in to 3 different labels but offset is still (0,1) for all three symbols\n",
            "Topic Timestamp: Jun 27, 2022 7:32 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Issue with XLM-RoBERTa tokenizer\n",
            "Link: https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311\n",
            "Replies: 1\n",
            "Views: 290\n",
            "Activity: Created: May 1, 2023 2:57 pm\n",
            "Latest: Aug 15, 2024 7:11 pm\n",
            "Topic Owner: ozanarmagan\n",
            "Topic Content: Hi;\n",
            "I have a problem with encoding with XLM-RoBERTa sentencepiece tokenizer. Why is the hugging face encoding 1 greater compared to the google sentencepiece encoding?\n",
            "Example\n",
            "## Hugging Face:\n",
            "tokenizer_xlmroberta.encode(\"I don't understand why\",add_special_tokens=False)\n",
            "Output: [87, 2301, 25, 18, 28219, 15400]\n",
            "## Sentencepiece:\n",
            "tokenizer_xlmroberta_.encode_as_ids(\"I don't understand why\")\n",
            "Output: [86, 2300, 24, 17, 28218, 15399]\n",
            "Topic Timestamp: May 1, 2023 2:57 pm\n",
            "Replies:\n",
            "  - Username: stryptsTester99\n",
            "    Answer: Hey\n",
            "I am having the same issue, have you found any solution?\n",
            "Kind regards\n",
            "    Timestamp: Aug 15, 2024 7:11 pm\n",
            "----------------------------------------\n",
            "Title: Create a simple tokenizer\n",
            "Link: https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677\n",
            "Replies: 0\n",
            "Views: 404\n",
            "Activity: Created: Feb 14, 2023 6:57 am\n",
            "Topic Owner: beannn\n",
            "Topic Content: I have a preprocessed dataset. The tokens are split by whitespace. So I need a very simple tokenizer to load this. Is there any advice about how to create this?\n",
            "Topic Timestamp: Feb 14, 2023 6:57 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: ByteLevelBPETokenizer inconsistent behavior\n",
            "Link: https://discuss.huggingface.co/t/bytelevelbpetokenizer-inconsistent-behavior/442\n",
            "Replies: 0\n",
            "Views: 402\n",
            "Activity: Created: Jul 23, 2020 3:47 pm\n",
            "Latest: Jul 23, 2020 3:50 pm\n",
            "Topic Owner: mstekel\n",
            "Topic Content: Hi, I encountered a weird behavior of the ByteLevelBPETokenizer: this publicly available notebook is parameterized to run on two almost identical text files. The first one is a transliteration of the Hebrew Bible text while the second one is the same transliteration with 2 modifications - the ‘ and ’ characters are replaced by the Hebrew letters ע and א accordingly (the transliteration used these 2 types of apostrophes for denoting these Hebrew consonants). After training on the first file(tanach_translit_orig) the tokenizer is given a test sentence for encoding and results with 19 tokens but when running the same process on the second file and the same test sentence modified accordingly (by replacing the apostrophes by the Hebrew letters) the tokenizer results with 9 tokens. I assumed the ByteLevelBPETokenizer to be agnostic to the character meanings and I cannot understand why the results vary between the experiments. Can anyone shed some light please?\n",
            "P.S. for toggling between the files all you need is to un-comment the appropriate line in the second step of the notebook:\n",
            "Thank you in advance,\n",
            "Moshe.\n",
            "Topic Timestamp: Jul 23, 2020 3:47 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: A question about the DataCollator for LM\n",
            "Link: https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273\n",
            "Replies: 2\n",
            "Views: 228\n",
            "Activity: Created: May 5, 2024 3:34 pm\n",
            "Latest: May 6, 2024 9:06 am\n",
            "Topic Owner: dydavide\n",
            "Topic Content: I have a question about DataCollatorForLanguageModeling, when training a LM.\n",
            "I saw this video, which explains very well, how the training process works.\n",
            "Starting from minute 5:12 it says “the datacollator shifts the input, such that the label is the next token in the sequence for every single token in the input”\n",
            "It make sense to me and it is a nice explanation about what is happening behind the scene.\n",
            "But then, looking into the documentation for understanding the mlm parameter I found the following:\n",
            "mlm (bool, optional, defaults to True) — Whether or not to use masked language modeling. If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token.\n",
            "So, now I’m totally confused. Is DataCollator shifting the tokens to the left? Or it is controlling only the behavior of the padding tokens?\n",
            "Thanks\n",
            "Topic Timestamp: May 5, 2024 3:34 pm\n",
            "Replies:\n",
            "  - Username: nielsr\n",
            "    Answer: Hi,\n",
            "No the data collator itself is not shifting any tokens. Typically the labels are just a copy of the input_ids, with padding tokens replaced by -100 (which is the ignore index of the cross-entropy loss in PyTorch).\n",
            "The shifting of the tokens one position happens inside the model (hence the user doesn’t need to take care of that). This can be seen here for Llama for instance.\n",
            "    Timestamp: May 6, 2024 9:06 am\n",
            "----------------------------------------\n",
            "Title: 1 line code for NER data set preparation using tokenizer library!\n",
            "Link: https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816\n",
            "Replies: 0\n",
            "Views: 391\n",
            "Activity: Created: Sep 9, 2022 3:40 am\n",
            "Latest: Sep 9, 2022 3:40 am\n",
            "Topic Owner: Imran1\n",
            "Topic Content: you can see this code is hard to read and understand. Is there any easy way in which we can simplify 15 line of code to 1 line???\n",
            "def tokenize_and_align_labels(examples):\n",
            "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
            "\n",
            "    labels = []\n",
            "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
            "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
            "        previous_word_idx = None\n",
            "        label_ids = []\n",
            "        for word_idx in word_ids:\n",
            "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
            "            # ignored in the loss function.\n",
            "            if word_idx is None:\n",
            "                label_ids.append(-100)\n",
            "            # We set the label for the first token of each word.\n",
            "            elif word_idx != previous_word_idx:\n",
            "                label_ids.append(label[word_idx])\n",
            "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
            "            # the label_all_tokens flag.\n",
            "            else:\n",
            "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
            "            previous_word_idx = word_idx\n",
            "\n",
            "        labels.append(label_ids)\n",
            "\n",
            "    tokenized_inputs[\"labels\"] = labels\n",
            "    return tokenized_inputs\n",
            "Topic Timestamp: Sep 9, 2022 3:40 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Configure RobertaTokenizer\n",
            "Link: https://discuss.huggingface.co/t/configure-robertatokenizer/23969\n",
            "Replies: 0\n",
            "Views: 388\n",
            "Activity: Created: Oct 4, 2022 4:26 am\n",
            "Topic Owner: srishti-hf1110\n",
            "Topic Content: Hi, I am willing to configure RobertaTokenizer such that it outputs token_type_ids that it doesn’t by default. Is there a way to do that?\n",
            "I have changed the model configuration and updated its type_vocab_size to 2, like so:\n",
            "model = RobertaModel.from_pretrained('roberta-base')\n",
            "\n",
            "# Update config to finetune token type embeddings\n",
            "model.config.type_vocab_size = 2 \n",
            "\n",
            "# Create a new Embeddings layer, with 2 possible segments IDs instead of 1\n",
            "model.embeddings.token_type_embeddings = nn.Embedding(2, model.config.hidden_size)\n",
            "                \n",
            "# Initialize it\n",
            "model.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
            "I want to input token_type_ids to the model instance like so:\n",
            "model(token_ids, attn_masks, token_type_ids)\n",
            "Topic Timestamp: Oct 4, 2022 4:26 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: BertTokenizer’s encode_plus returns 2d tensor when printing ‘input_ids’/ ‘attention_mask’\n",
            "Link: https://discuss.huggingface.co/t/berttokenizers-encode-plus-returns-2d-tensor-when-printing-input-ids-attention-mask/3530\n",
            "Replies: 0\n",
            "Views: 384\n",
            "Activity: Created: Feb 7, 2021 11:40 pm\n",
            "Topic Owner: AnkitaDas\n",
            "Topic Content: When using BertTokenizer’s encode_plus, why does tokenized[‘input_ids’] return a 2d tensor?\n",
            "I understand when batch_encode_plus does the same(an extra dimension for containing the batch size).\n",
            "But what’s the purpose of returning a 2d tensor in encode_plus? The returned tensor size is of the form [1,N].\n",
            "Topic Timestamp: Feb 7, 2021 11:40 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF\n",
            "Link: https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602\n",
            "Replies: 0\n",
            "Views: 383\n",
            "Activity: Created: Jun 26, 2023 12:19 pm\n",
            "Latest: Jun 26, 2023 12:25 pm\n",
            "Topic Owner: mdrpanwar\n",
            "Topic Content: Hi, I am writing the code for training an LM from scratch on a custom dataset following the run_clm_no_trainer.py file. Please help me with the following questions. I could not find clear answers in the docs. Please point me to references in case I may have missed them.\n",
            "An example in the batch should look like this:\n",
            "BOS example_1 EOS BOS example_2 EOS … BOS example_n EOS PAD … PAD\n",
            "Q1. Is there an efficient method for grouping and padding data examples like above? My use cases: (1) pad s.t. the maximum number of data examples fit in context, (2) pad s.t. k data examples fit in the context (where k is a fixed natural number). run_clm_no_trainer.py has a custom method group_texts that does grouping (no padding) that can be modified. However, I think my use cases are pretty standard and some built-in method should exist.\n",
            "Q2. [Design Choice] Is using a SEP token better than using the BOS and EOS? What are the considerations here?\n",
            "Q3. What tokens do I need to update in the tokenizer and model? I have BOS, EOS, PAD and UNK (or SEP in place of BOS and EOS depending on the answer to Q2). UNK goes into the tokenizer while instantiating the tokenizer’s model (WordLevel in my case). BOS and EOS token IDs go in the model config (I am using GPT-2). What about PAD and SEP? Also, what changes are needed from my end if I want to use EOS as PAD?\n",
            "Q4. HF won’t compute the loss for the predictions at special tokens, right? I read somewhere that tokenizer sets labels as -100 for special tokens which are ignored while computing the loss. Please confirm if that is correct.\n",
            "Q5. What should the token_type_ids be? Since I am using GPT-2, should I care about token_type_ids? If yes, what does GPT-2 expect – same token_type_ids for all tokens or different token_type_ids for each segment (i.e. “BOS example_k EOS” gets k-th token_type_id) in a batch sentence like in BERT?\n",
            "If the questions are more appropriate for the Transformers forum, please let me know. Thanks.\n",
            "Topic Timestamp: Jun 26, 2023 12:19 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Customization of Wav2Vec2CTCTokenizer with rules\n",
            "Link: https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912\n",
            "Replies: 0\n",
            "Views: 382\n",
            "Activity: Created: Aug 22, 2022 10:52 am\n",
            "Latest: Aug 22, 2022 10:52 am\n",
            "Topic Owner: spicci\n",
            "Topic Content: Hi, my goal is to fine-tune an ASR model, WavLM, that relies on the pretrained tokenizer Wav2Vec2CTCTokenizer.\n",
            "I want to fine-tune this ASR model with another language and to perform the tokenization according to phonological rules, such as syllable segmentation.\n",
            "Providing a vocabulary with all the possible syllables (aka my tokens), is it possible to customize the Wav2Vec2CTCTokenizer segmentation so that it will respect syllable segmentation rules?\n",
            "Example:\n",
            "Original sentence:\n",
            "Il tentativo era cosi bello\n",
            "Segmentation made by Wav2Vec2CTCTokenizer (not respecting syllabification rules):\n",
            "[‘il’, ‘ten’, ‘tat’, ‘iv’, ‘o’, ‘Er’, ‘a’, ‘kos’, ‘i’, ‘bEl’, ‘lo’]\n",
            "Expected segmentation according to syllabification rules:\n",
            "[‘il’, ‘ten’, ‘ta’, ‘ti’, ‘vo’, ‘E’, ‘ra’, 'ko, ‘si’, ‘bEl’, ‘lo’]\n",
            "Basically, I need to state and include some rules in the tokenizer, for example to give priority to tokens with a consonant in the onset position instead of in the coda of the syllable.\n",
            "Is it possible to insert this kind of rules in the tokenizer?\n",
            "If so, where can I modify these parameters?\n",
            "If not, if I train a new tokenizer, will it be ok to implement it in the pre-trained WavLm model that I need to fine-tune?\n",
            "Thanks in advance!\n",
            "Topic Timestamp: Aug 22, 2022 10:52 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: GPT2Tokenizer not working in Kaggle Notebook\n",
            "Link: https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508\n",
            "Replies: 0\n",
            "Views: 373\n",
            "Activity: Created: May 30, 2023 2:58 pm\n",
            "Topic Owner: admarcosai\n",
            "Topic Content: Hello I have been trying to tokenize the WMT14 en-de. I first started in a free google colab and the following code worked there; I then switched to Kaggle notebook since it is a better environment but it doesn’t work there for some reason. The error it throws is below:\n",
            "from transformers import GPT2Tokenizer\n",
            "bpe_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
            "\n",
            "def tokenization(examples):\n",
            "    source, target = [], []\n",
            "    for example in examples:\n",
            "        trgt = bpe_tokenizer(example['de'])\n",
            "        src = bpe_tokenizer(example['en'])\n",
            "        target.append(trgt)\n",
            "        source.append(src)\n",
            "        \n",
            "    return {'de': target,\n",
            "            'en': source}\n",
            "\n",
            "train_dataset = dataset_de_en['train'].map(lambda examples: tokenization(examples['translation']), batched=True)\n",
            "test_dataset = dataset_de_en['test'].map(lambda examples: tokenization(examples['translation']), batched=True)\n",
            "val_dataset = dataset_de_en['val'].map(lambda examples: tokenization(examples['translation']), batched=True)\n",
            "ArrowInvalid: Could not convert {'input_ids': [54, 798, 263, 559, 22184, 993, 1326, 4587, 311, 4224, 2150, 82, 525, 72, 1098], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} with type BatchEncoding: did not recognize Python value type when inferring an Arrow data type\n",
            "Is there an explanation to this?\n",
            "Topic Timestamp: May 30, 2023 2:58 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Skip-gram tokens\n",
            "Link: https://discuss.huggingface.co/t/skip-gram-tokens/5294\n",
            "Replies: 0\n",
            "Views: 368\n",
            "Activity: Created: Apr 4, 2021 6:02 am\n",
            "Topic Owner: vitali\n",
            "Topic Content: Is there any way to produce skip-gram tokens with tokenizers library?\n",
            "Topic Timestamp: Apr 4, 2021 6:02 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Training tokenizers with padding in between tokens\n",
            "Link: https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173\n",
            "Replies: 0\n",
            "Views: 367\n",
            "Activity: Created: Oct 19, 2023 6:43 pm\n",
            "Topic Owner: akanakia\n",
            "Topic Content: Hi folks,\n",
            "I am trying to train a set of tokenizers (BPE, WordPiece, and Unigram) on a dataset containing antibody sequences. The issue is that antibody sequences are usually pre-aligned using some aligment scheme like cabat or IMGT. This results in padding tokens being introduced in the middle of the sequence which never happens when tokenizing text. E.g., An aligned antibody sequence can look something like this “QVQT–TYHHH ASTR-MTPY Q-----QY”, with “-” being the pad token introduced during sequence alignment.\n",
            "I would like the tokenizers to essentially treat the pad token as an “unk” token during pretraining but there is already an unk token in the initial vocab, usually denoted by “X” representing an unknown amino acid in a sequence. Is there some way to enforce learned tokens do no contain any padding characters in them using huggingface.tokenizers? Any help would be really appreciated. Thanks.\n",
            "Topic Timestamp: Oct 19, 2023 6:43 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: BpeTrainer implementation in Python\n",
            "Link: https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625\n",
            "Replies: 0\n",
            "Views: 368\n",
            "Activity: Created: Jul 23, 2021 12:58 pm\n",
            "Topic Owner: amankhandelia\n",
            "Topic Content: Hi there,\n",
            "So I was perusing through the tokenizer library in order pick some knowledge about the nitty gritty of the BpeTrainer, although I was only able to find the rust implementation (github.com), after bit more of digging I came across the fact that the tokenizer library only offers bindings for other languages not any native implementations, so my question is, is there any good implementation of the BpeTrainer or similar functionality in python, which one can refer\n",
            "Thanks in advance.\n",
            "Topic Timestamp: Jul 23, 2021 12:58 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Map tokenization and posterior to smaller substrings\n",
            "Link: https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792\n",
            "Replies: 0\n",
            "Views: 365\n",
            "Activity: Created: Sep 29, 2022 5:38 pm\n",
            "Topic Owner: mdelas\n",
            "Topic Content: Dear community,\n",
            "My objective is to have sequences of 1024 tokens ready to GPT-2. As input data, I have DNA sequences of 2E5 length, and I have built a BPE tokenizer using this data.\n",
            "I am trying to build a function which concatenates:\n",
            "Tokenize the full sequences (2E5 max_length)\n",
            "Chunk data into 1024 sequence length and enlarge the number of rows of my Huggingface Dataset\n",
            "For now I am trying to use this function, and map it posterior to the Dataset:\n",
            "def tokenize_and_chunk(examples):\n",
            "    chunks = []\n",
            "    tokenized_inputs = fast_tokenizer(\n",
            "        examples['text'],\n",
            "        max_length=200000,\n",
            "        truncation=True\n",
            "    )\n",
            "\n",
            "    for sentence in examples['text']:\n",
            "        chunks += [sentence[i:i + 1024] for i in range(0, len(sentence), 1024)]\n",
            "\n",
            "    return {\"chunks\": chunks} \n",
            "\n",
            "chunked_data = dna_data.map(tokenize_and_chunk, remove_columns=dna_data.column_names)\n",
            "The problem is that I receive a list of lists just like this:\n",
            "[[‘C’,\n",
            "‘C’,\n",
            "‘A’,\n",
            "‘G’,\n",
            "…,\n",
            "],…,]\n",
            "where this list is a list where for each example I have a inner-list with length of 91170 chars.\n",
            "I can’t figure out how should I correctly map this function. Does someone have an idea of which should be the best practice for this?\n",
            "Topic Timestamp: Sep 29, 2022 5:38 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Are special_tokens the only tokens guaranteed to be atomic?\n",
            "Link: https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124\n",
            "Replies: 0\n",
            "Views: 365\n",
            "Activity: Created: Mar 3, 2021 4:53 pm\n",
            "Topic Owner: jncasey\n",
            "Topic Content: I’m building a BPE tokenizer from scratch, and I’d like to add some tokens to its vocabulary that are never broken apart and never included in merges.\n",
            "The second part of that (no tokens in the merges) is easy – the tokens don’t appear in the training text I give the tokenizer. But for the first part (never breaking my special token, e.g. [Y], into its [,Y, and ] components), I’m less sure of the correct process.\n",
            "I had though I could just add the tokens to the tokenizer using add_tokens() before training, and when testing using the Tokenizers library version directly, everything seems fine.\n",
            "But when I saved out that tokenizer json and load it as the tokenizer_file param in a Transformers RobertTokenizerFast and used it to train a BART model, I was getting unexpected results – the added tokens were sometimes getting broken apart into their components.\n",
            "The problem stopped and tokenization went as expected when I shifted to adding my tokens using add_special_tokens() instead. But that’s not ideal, because I want my tokens to be part of my output, and not stripped away by the handy skip_special_tokens=True param in the tokenizer’s decode method.\n",
            "It’s entirely possible that in my first attempt, I wasn’t loading the saved tokenizer correctly into RobertaTokenizerFast – it’s a little confusing what can be loaded from the json and what needs to be explicitly passed as a parameter. But after re-reading the docs more carefully, it seems that maybe only special tokens are guaranteed to be atomic? Is there any way to define a token to be atomic but also not in the same class as control tokens like eos and pad?\n",
            "Topic Timestamp: Mar 3, 2021 4:53 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unable to convert output to interpretable format\n",
            "Link: https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882\n",
            "Replies: 0\n",
            "Views: 363\n",
            "Activity: Created: Jul 31, 2021 7:39 pm\n",
            "Topic Owner: markrogersjr\n",
            "Topic Content: I’m not able to convert model outputs to a human-interpretable format. I went ahead and created an issue along with a PR that adds the feature.\n",
            "Topic Timestamp: Jul 31, 2021 7:39 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unable to upload custom Pytorch model in huggingface\n",
            "Link: https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545\n",
            "Replies: 0\n",
            "Views: 362\n",
            "Activity: Created: Apr 4, 2023 4:46 pm\n",
            "Topic Owner: shivam21mishra08\n",
            "Topic Content: I created a custom generative pytorch model . but while uploading to hugging face i cannot access hosted API … help me fix it!!\n",
            "Topic Timestamp: Apr 4, 2023 4:46 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Incorporate SARI score into run_summarization.py example script\n",
            "Link: https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511\n",
            "Replies: 0\n",
            "Views: 350\n",
            "Activity: Created: Jan 13, 2023 6:02 pm\n",
            "Topic Owner: brooklynsheppard\n",
            "Topic Content: I am working with the example summarization script provided by Huggingface here. My task is not quite summarization, but a different seq2seq task where I would like to use the SARI metric to evaluate my models. The compute_metrics function in this script takes only the predictions and labels as input, making it easy to calculate ROUGE or BLEU sores, but does not include the source text, which is required for computing the SARI score. The compute_metrics function is then given to the Seq2SeqTrainer as an argument, and I’m not sure how it is used from there. Does anyone have any guidance on how I can edit this function to be able to take the source text as an argument as well? Thanks!\n",
            "Topic Timestamp: Jan 13, 2023 6:02 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType\n",
            "Link: https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581\n",
            "Replies: 0\n",
            "Views: 349\n",
            "Activity: Created: Feb 24, 2024 6:18 am\n",
            "Topic Owner: deshwalmahesh\n",
            "Topic Content: Issue 1: tokenizer.vocab_size has a size of 50257 when printed but the Phi-2 model gives me an output shape (5, 1256, 51200) during evaluation and the error below while decoding\n",
            "I’m working on callback like:\n",
            "def decode_predictions(tokenizer, predictions):\n",
            "    print(type(predictions), predictions.predictions.shape, predictions.label_ids.shape) # (5, 1256, 51200)\n",
            "    labels = tokenizer.batch_decode(predictions.label_ids) \n",
            "    prediction_text = tokenizer.batch_decode(predictions.predictions.argmax(axis=-1)) # HERE COMES THE ERROR\n",
            "    return {\"labels\": labels, \"predictions\": prediction_text}\n",
            "\n",
            " def on_evaluate(self, args, state, control,  **kwargs):\n",
            "        super().on_evaluate(args, state, control, **kwargs)\n",
            "\n",
            "        predictions = self.trainer.predict(self.sample_dataset)# generate predictions\n",
            "        predictions = decode_predictions(self.tokenizer, predictions) # decode predictions and labels\n",
            "        predictions_df = pd.DataFrame(predictions) # add predictions to a wandb.Table\n",
            "        predictions_df[\"epoch\"] = state.epoch\n",
            "        records_table = self._wandb.Table(dataframe=predictions_df)\n",
            "        self._wandb.log({\"sample_predictions\": records_table}) # log the table to wandb\n",
            "Issue-2: When I create a random example, the tokenizer works till 50300\n",
            "and after that, I’m getting:\n",
            "---------------------------------------------------------------------------\n",
            "TypeError                                 Traceback (most recent call last)\n",
            "Cell In[19], line 1\n",
            "----> 1 tokenizer.batch_decode(predictions.argmax(axis=-1), skip_special_tokens = True)\n",
            "\n",
            "File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3742, in PreTrainedTokenizerBase.batch_decode(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\n",
            "   3718 def batch_decode(\n",
            "   3719     self,\n",
            "   3720     sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
            "   (...)\n",
            "   3723     **kwargs,\n",
            "   3724 ) -> List[str]:\n",
            "   3725     \"\"\"\n",
            "   3726     Convert a list of lists of token ids into a list of strings by calling decode.\n",
            "   3727 \n",
            "   (...)\n",
            "   3740         `List[str]`: The list of decoded sentences.\n",
            "   3741     \"\"\"\n",
            "-> 3742     return [\n",
            "   3743         self.decode(\n",
            "   3744             seq,\n",
            "   3745             skip_special_tokens=skip_special_tokens,\n",
            "   3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
            "   3747             **kwargs,\n",
            "   3748         )\n",
            "   3749         for seq in sequences\n",
            "   3750     ]\n",
            "\n",
            "File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3743, in <listcomp>(.0)\n",
            "   3718 def batch_decode(\n",
            "   3719     self,\n",
            "   3720     sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n",
            "   (...)\n",
            "   3723     **kwargs,\n",
            "   3724 ) -> List[str]:\n",
            "   3725     \"\"\"\n",
            "   3726     Convert a list of lists of token ids into a list of strings by calling decode.\n",
            "   3727 \n",
            "   (...)\n",
            "   3740         `List[str]`: The list of decoded sentences.\n",
            "   3741     \"\"\"\n",
            "   3742     return [\n",
            "-> 3743         self.decode(\n",
            "   3744             seq,\n",
            "   3745             skip_special_tokens=skip_special_tokens,\n",
            "   3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
            "   3747             **kwargs,\n",
            "   3748         )\n",
            "   3749         for seq in sequences\n",
            "   3750     ]\n",
            "\n",
            "File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:358, in CodeGenTokenizer.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, truncate_before_pattern, **kwargs)\n",
            "    331 \"\"\"\n",
            "    332 Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
            "    333 tokens and clean up tokenization spaces.\n",
            "   (...)\n",
            "    353     `str`: The decoded sentence.\n",
            "    354 \"\"\"\n",
            "    356 token_ids = to_py_obj(token_ids)\n",
            "--> 358 decoded_text = super()._decode(\n",
            "    359     token_ids=token_ids,\n",
            "    360     skip_special_tokens=skip_special_tokens,\n",
            "    361     clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n",
            "    362     **kwargs,\n",
            "    363 )\n",
            "    365 if truncate_before_pattern is not None and len(truncate_before_pattern) > 0:\n",
            "    366     decoded_text = self.truncate(decoded_text, truncate_before_pattern)\n",
            "\n",
            "File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils.py:1024, in PreTrainedTokenizer._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\n",
            "   1022         current_sub_text.append(token)\n",
            "   1023 if current_sub_text:\n",
            "-> 1024     sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n",
            "   1026 if spaces_between_special_tokens:\n",
            "   1027     text = \" \".join(sub_texts)\n",
            "\n",
            "File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:284, in CodeGenTokenizer.convert_tokens_to_string(self, tokens)\n",
            "    282 def convert_tokens_to_string(self, tokens):\n",
            "    283     \"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\n",
            "--> 284     text = \"\".join(tokens)\n",
            "    285     text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
            "    286     return text\n",
            "\n",
            "TypeError: sequence item 31: expected str instance, NoneType found\n",
            "This is the code I’m using:\n",
            "import numpy as np\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", use_fast = False)\n",
            "\n",
            "print(tokenizer.vocab_size)\n",
            "\n",
            "tokenizer.add_tokens([\"<|im_start|>\", \"<PAD>\"])\n",
            "tokenizer.pad_token = \"<PAD>\"\n",
            "tokenizer.add_special_tokens(dict(eos_token=\"<|im_end|>\"))\n",
            "\n",
            "print(tokenizer.vocab_size)\n",
            "\n",
            "predictions = np.random.uniform(size = (5, 1256, 50300)) # [No of samples, sequence_length, Vocab]\n",
            "preds = predictions.argmax(axis=-1)\n",
            "\n",
            "tokenizer.batch_decode(preds) # Works till 50300\n",
            "Topic Timestamp: Feb 24, 2024 6:18 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: `add_tokens` with argument `special_tokens=True` vs `add_special_tokens`\n",
            "Link: https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648\n",
            "Replies: 0\n",
            "Views: 345\n",
            "Activity: Created: Apr 5, 2023 12:54 pm\n",
            "Topic Owner: Buxian\n",
            "Topic Content: There are many tutorial using add_tokens(special_tokens=True), but I read the source code, and find that add_special_tokens will do more thing than add_tokens, Which is prefer?\n",
            "Topic Timestamp: Apr 5, 2023 12:54 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Is it okay to split ids sequence when it is encoded using Byte-level BPE\n",
            "Link: https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129\n",
            "Replies: 0\n",
            "Views: 339\n",
            "Activity: Created: Jul 7, 2021 8:59 am\n",
            "Topic Owner: jisng-prk\n",
            "Topic Content: TLDR;\n",
            "So, When I use byte level BPE tokenizer,\n",
            "Is it possible to split the encoded sequences into sub list without loss of information.\n",
            "I think when the sublist is decoded into string, It should be generate error at the tail of the sequence because the last ids has only part of the bytes of a string token\n",
            "I’m using BART BPE , which is same with that of RoBERTa and thus, it is byte-level BPE\n",
            "When using the tokenizer, it split the sequence based on the byte sequence.\n",
            "But, when I should split the sequence into limited sequence length such as 128,\n",
            "maybe, some byte sequence could be split into different documents, thus it should makes error when decode the byte into string because the part of the byte is not provided in the tail of ids sequence\n",
            "For example, when I encode the following sequence\n",
            "“This string will be encoded as byte level” => [0, … , 2]\n",
            "And I split the ids into two lists,\n",
            "[0, … , 2] => [0, …], […, 2]\n",
            "the boundary ids between two lists can miss some information about the rest byte of the token string\n",
            "So, When I use byte level BPE,\n",
            "Is it possible to split the encoded sequences into sub list without loss of information?\n",
            "Topic Timestamp: Jul 7, 2021 8:59 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Can’t get to the source code of `tokenizer.convert_tokens_to_string`\n",
            "Link: https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714\n",
            "Replies: 0\n",
            "Views: 338\n",
            "Activity: Created: Feb 28, 2023 3:07 pm\n",
            "Topic Owner: Shaike04\n",
            "Topic Content: I have a list of tokens and I want to understand the logic behind tokenizer.convert_tokens_to_string but I can’t seem to find the source code for this function or debug my way to it.\n",
            "Can you help?\n",
            "Topic Timestamp: Feb 28, 2023 3:07 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to use a trained tokenizer for semantic search?\n",
            "Link: https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091\n",
            "Replies: 0\n",
            "Views: 337\n",
            "Activity: Created: Nov 5, 2023 3:17 am\n",
            "Topic Owner: panigrah\n",
            "Topic Content: I trained a new tokenizer with vocabulary from my domain. I started with sentence-transformers/multi-qa-mpnet-base-dot-v1.\n",
            "I don’t understand what model to use in order to use the new tokenizer with for semantic search.\n",
            "Do I need to train the model on my vocabulary also before I can use it to create embeddings?\n",
            "I read through the tutorial on FAISS here Semantic search with FAISS - Hugging Face NLP Course. I get reasonably good matches on my dataset using the base tokenizer. But if I swap the base tokenizer with the trained one - I don’t get matches.\n",
            "So does the model also need to be trained on the new dataset and if so would I be looking to train from scratch or is there a way to just fine tune from this checkpoint?\n",
            "thanks\n",
            "Topic Timestamp: Nov 5, 2023 3:17 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: BertTokenizer.decode not understanding new vocabulary\n",
            "Link: https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254\n",
            "Replies: 0\n",
            "Views: 335\n",
            "Activity: Created: Dec 1, 2023 1:34 pm\n",
            "Latest: Dec 1, 2023 1:38 pm\n",
            "Topic Owner: rberga\n",
            "Topic Content: I added 3 new items to the BertTokenizer vocabulary (2 emojis and a made-up word), and saved the new vocabulary. Then I instantiated a new BertTokenizer using the new vocabulary file and checked that the tokenizer understood the new words. That worked fine.\n",
            "Then I ran ‘encode’ to see the tokens encodings and verified that the new encodings were used.\n",
            "Then I ran ‘decode’ on the encoded tokens and did NOT get the original words back. The new items added to the vocabulary were NOT decoded and were left as [UNK] even though the ‘encode’ generated the correct encodings.\n",
            "The code below illustrates the problem:\n",
            "# Importing transformers tokenizer\n",
            "from transformers import BertTokenizer\n",
            "\n",
            "# Adding 3 new words or symbols to tokenizer vocabulary: thumbs-up and down emojis and a made-up word\n",
            "newvocab = [ '👍', '👎', 'babalu' ]\n",
            "print(newvocab)\n",
            "\n",
            "# Get basic Bert Tokenizer from pretrained\n",
            "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
            "\n",
            "# quick check to make sure new vocabulary is not yet present in existing tokenizer\n",
            "print(tokenizer.tokenize('babalu'))    # ['baba', '##lu']\n",
            "print(tokenizer.tokenize('👍'))          # ['[UNK]']\n",
            "print(tokenizer.tokenize('👎'))          # ['[UNK]']\n",
            "\n",
            "# Get base vocabulary (to add newvocab to)\n",
            "bert_vocab = tokenizer.get_vocab()\n",
            "\n",
            "# add newvocab to bert_vocab\n",
            "print(f'bert_vocab before = {len(bert_vocab)}')\n",
            "for i, k in enumerate(newvocab, len(bert_vocab)):\n",
            "    print(f'new item {k} : {i}')\n",
            "    bert_vocab[k] = i\n",
            "print(f'bert_vocab after = {len(bert_vocab)}')\n",
            "\n",
            "# The above lines print:\n",
            "# bert_vocab before = 30522\n",
            "# new item 👍 : 30522\n",
            "# new item 👎 : 30523\n",
            "# new item babalu : 30524\n",
            "# bert_vocab after = 30525\n",
            "\n",
            "# save new vocab file\n",
            "with open('/tmp/newvocab.tmp', 'w', encoding = 'utf-8') as tmp_vocab_file:\n",
            "    tmp_vocab_file.write('\\n'.join(bert_vocab))\n",
            "\n",
            "# Get new tokenizer using the new vocabulary file                        \n",
            "new_bert = BertTokenizer.from_pretrained('bert-base-uncased',vocab_file = '/tmp/newvocab.tmp' )\n",
            "\n",
            "# Does the new tokenizer understand the new items added to the vocabulary?\n",
            "new_bert.tokenize('thumbs-up 👍, thumbs-down 👎,  new word babalu.')\n",
            "\n",
            "# This produces:\n",
            "# ['thumbs',  '-',  'up',  '👍',  ',',  'thumbs',  '-',  'down',  '👎',  ',',  'new',  'word',  'babalu',  '.']\n",
            "# which shows that the new tokenizer does understand the new items added to the vocabulary\n",
            "\n",
            "# Checking the encoding and decoding\n",
            "# It seems that the ENCODING is using all the new vocabulary entries (i.e., mapping emojis to their encodings)\n",
            "# But the DECODING is not mapping them back to their original representation.\n",
            "\n",
            "tokens = new_bert.encode('thumbs-up 👍, thumbs-down 👎,  new word babalu.',\n",
            "                         add_special_tokens=True,\n",
            "                         max_length=32,\n",
            "                         truncation=True\n",
            "                        )\n",
            "print(f'tokens after encode:\\n{tokens}')\n",
            "tokens_decoded = tokenizer.decode(tokens)\n",
            "print(f'tokens after decoding it back:\\n{tokens_decoded}')\n",
            "\n",
            "# this prints:\n",
            "# tokens after encode:\n",
            "# [101, 16784, 1011, 2039, 30522, 1010, 16784, 1011, 2091, 30523, 1010, 2047, 2773, 30524, 1012, 102]\n",
            "# tokens after decoding it back:\n",
            "# [CLS] thumbs - up [UNK], thumbs - down [UNK], new word [UNK]. [SEP]\n",
            "\n",
            "# It seems that the new items are being mapped to their correct encodings (30522, 30523, 30524),\n",
            "# but are not being decoded back to their original representation.\n",
            "Am I doing anything wrong here?\n",
            "Thanks!\n",
            "Topic Timestamp: Dec 1, 2023 1:34 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Issue in loading the saved tokenizer\n",
            "Link: https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978\n",
            "Replies: 1\n",
            "Views: 234\n",
            "Activity: Created: Jan 4, 2024 1:54 pm\n",
            "Latest: Jan 4, 2024 1:54 pm\n",
            "Topic Owner: hadnoor\n",
            "Topic Content: \n",
            "Topic Timestamp: Jan 4, 2024 1:54 pm\n",
            "Replies:\n",
            "  - Username: hadnoor\n",
            "    Answer: i was using the following code :\n",
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(“Salesforce/xgen-7b-8k-base”,trust_remote_code=True)\n",
            "tokenizer.pad_token = tokenizer.eos_token\n",
            "tokenizer.save_pretrained(“toksav”)\n",
            "loaded_tokenizer = AutoTokenizer.from_pretrained(“toksav”)\n",
            "    Timestamp: Jan 4, 2024 1:54 pm\n",
            "----------------------------------------\n",
            "Title: Can’t load tokenizer for ‘sshleifer/student_blarge_12_3’\n",
            "Link: https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027\n",
            "Replies: 0\n",
            "Views: 330\n",
            "Activity: Created: May 6, 2021 11:24 am\n",
            "Topic Owner: jyotsna2893\n",
            "Topic Content: \n",
            "Topic Timestamp: May 6, 2021 11:24 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Why I’m getting same result with or without using Wav2Vec2Processor?\n",
            "Link: https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482\n",
            "Replies: 0\n",
            "Views: 320\n",
            "Activity: Created: Feb 25, 2023 3:56 am\n",
            "Topic Owner: laro1\n",
            "Topic Content: I’m running simple wav2vec2 code on short without noise voice:\n",
            "#processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
            "model     = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")     \n",
            "\n",
            "FILE_NAME        = \"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
            "SPEECH_FILE      = download_asset(FILE_NAME)\n",
            "\n",
            "speech, sr       = librosa.load(SPEECH_FILE, sr=16000)\n",
            "\n",
            "speech           = torch.tensor(speech)\n",
            "speech           = speech.reshape(1, -1)\n",
            "\n",
            "logits           = model(speech).logits\n",
            "\n",
            "predicted_ids    = torch.argmax(logits, dim=-1)\n",
            "transcription    = processor.decode(predicted_ids[0])\n",
            "transcription\n",
            "results: 'I HAD THAT CURIOSITY BESIDE ME AT THIS MOMENT'\n",
            "As you see, I didn’t use processor.\n",
            "The examples on the net always used processor\n",
            "So:\n",
            "What is the benefit of using processor ?\n",
            "When do we need to use it ?\n",
            "Topic Timestamp: Feb 25, 2023 3:56 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Many ambiguous unicode characters for trained tokenizer\n",
            "Link: https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553\n",
            "Replies: 0\n",
            "Views: 317\n",
            "Activity: Created: Dec 31, 2023 1:06 pm\n",
            "Topic Owner: KellRa\n",
            "Topic Content: Hey,\n",
            "I trained a tokenizer on english wikipedia texts. Everything works fine, however, looking into the vocabulary of the saved tokenizer, I find a multitude of unicode characters which I would not expect to be included in the corpus, e.g., many chinese symbols.\n",
            "You can take a look at the vocabulary here: finroberta/dicts_and_tokenizers/wikipedia_tokenizer.json at main · RalfKellner/finroberta · GitHub\n",
            "Odd symbols start at id 68 and go until approximately 10,000 (out of 40,000 tokens).\n",
            "I use unicode normalization when training the tokenizer which I thought would prevent this behavior. More concrete, I use NFD normalization, however, I also tried other forms of normalization.\n",
            "This also happens when using a domain specific corpus from the financial markets area, so I was wondering if I am doing something wrong here?\n",
            "The training script can be found here: finroberta/00_train_wikipedia_tokenizer.py at main · RalfKellner/finroberta · GitHub\n",
            "I would appreciate any help or clarification! Many thanks in advance!\n",
            "Cheers,\n",
            "Ralf\n",
            "Topic Timestamp: Dec 31, 2023 1:06 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: T5Tokenizer add a whitespace token after added special tokens\n",
            "Link: https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101\n",
            "Replies: 0\n",
            "Views: 317\n",
            "Activity: Created: Nov 22, 2023 6:44 am\n",
            "Topic Owner: xianf\n",
            "Topic Content: I am using the mt5 Tokenizer and I want to add “\\n” to the origin tokenizer. I add \"additional_special_tokens\": [\"\\n\"] to the tokenizer_config.json. But the output is not what I want.\n",
            "The script is like:\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
            "print(tokenizer.tokenize(\"你好\\n啊\", add_special_tokens=False))\n",
            "and the output is\n",
            "['▁', '你', '好', '\\n', '▁', '啊']\n",
            "I want to remove this '▁' token after '\\n'.\n",
            "Of course, I can do it with some more codes with Python. But can I do this with the tokenizer itself?\n",
            "Topic Timestamp: Nov 22, 2023 6:44 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer not found\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-not-found/757\n",
            "Replies: 0\n",
            "Views: 314\n",
            "Activity: Created: Aug 18, 2020 8:40 pm\n",
            "Topic Owner: huggingfans\n",
            "Topic Content: when I try to use patrickvonplaten/longformer-random-tiny model and download the tokenizer, I get this error:\n",
            "OSError: Model name ‘patrickvonplaten/longformer-random-tiny’ was not found in tokenizers model name list\n",
            "There is the same problem on patrickvonplaten/reformer-tiny-random\n",
            "But the model can be downloaded, and only the tokenizer was not found.\n",
            "Topic Timestamp: Aug 18, 2020 8:40 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Can’t load tokenizer using from_pretrained, Interface API\n",
            "Link: https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639\n",
            "Replies: 0\n",
            "Views: 310\n",
            "Activity: Created: May 21, 2024 11:06 am\n",
            "Topic Owner: yewo\n",
            "Topic Content: Can’t load tokenizer using from_pretrained, please update its configuration: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 2102 column 3\n",
            "This is my project file link project File\n",
            "hugginceface as a model, it doesn’t seem to work anymore. Could anyone please help me with that!\n",
            "Topic Timestamp: May 21, 2024 11:06 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: What is required to create a fast tokenizer? For example for a Marian model\n",
            "Link: https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941\n",
            "Replies: 0\n",
            "Views: 310\n",
            "Activity: Created: Mar 16, 2023 12:49 pm\n",
            "Topic Owner: pejrich\n",
            "Topic Content: I notice that there are no fast tokenizers available for the Opus Marian translation models. The library I’m using to run my models currently only supports fast tokenizers from HF, but they’re not available for Marian, is this due to a limitation unique to Marian, or would it be possible for me to create one? What would be involved in the process. I couldn’t find any information specific to creating a fast tokenizer from an existing slow tokenizer. Thanks!\n",
            "Topic Timestamp: Mar 16, 2023 12:49 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Word tokenizers for text generators\n",
            "Link: https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464\n",
            "Replies: 0\n",
            "Views: 307\n",
            "Activity: Created: Sep 21, 2022 11:02 pm\n",
            "Latest: Sep 22, 2022 9:49 pm\n",
            "Topic Owner: PeterB123A\n",
            "Topic Content: Word based text generators make use of tokenizations, they scan a text, build a word-vector table.\n",
            "I wonder how this is done. ea does it learn each verb, walk walking walked walks, etc as different words. Or is there an indexer, stored with a base verb, with only a lookup table for irregular verbs?\n",
            "In essence, To Walk Conjugation - All English Verb Forms\n",
            "does already contains short base sentences, that are part of everyday chat. \" I’ve been walking.\"\n",
            "( maybe just stored as some number- that could become walk 24 walk or Sandra 27 walk.\n",
            "I was thinking could such a scheme optimize (reduce footprint) a smaller amount of total word vectors.\n",
            "by making the tokenizer / de-tokenizer smart (supply it with most language rules, firt word capitalize add . at the end etc)\n",
            "Topic Timestamp: Sep 21, 2022 11:02 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to see contents of a normalizer\n",
            "Link: https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045\n",
            "Replies: 0\n",
            "Views: 300\n",
            "Activity: Created: May 7, 2021 2:11 am\n",
            "Topic Owner: sumsaw\n",
            "Topic Content: Hello all,\n",
            "I have just started exploring the tokenizer library and I want to know what know what are the basic normalizer steps which are performed in BPE tokenizer . Is there any way I can list out the contents of\n",
            "tokenizer.normarlizer\n",
            "Regards\n",
            "Sumeet\n",
            "Topic Timestamp: May 7, 2021 2:11 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Encoding and then decodeing text is not equal\n",
            "Link: https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851\n",
            "Replies: 2\n",
            "Views: 98\n",
            "Activity: Created: Aug 11, 2024 9:08 am\n",
            "Latest: Aug 12, 2024 7:59 am\n",
            "Topic Owner: ron5569\n",
            "Topic Content: I wonder why in some cases, encdoing the text and then decoding it, is not the same the original text\n",
            "For example very simple code\n",
            "from transformers import AutoTokenizer\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\")\n",
            "\n",
            "texts = ['  A LoyaltyImport is a ...', '  You can ...', '    .withVariants(\"true\")']\n",
            "for row in texts:\n",
            "    encoded = tokenizer(row,\n",
            "                        add_special_tokens=False,\n",
            "                        truncation=True,\n",
            "                        padding=False,\n",
            "                        max_length=2000,\n",
            "                        return_overflowing_tokens=False,\n",
            "\n",
            "                        return_length=False, )\n",
            "    decoded = tokenizer.decode(encoded[\"input_ids\"])\n",
            "    if decoded != row:\n",
            "        print(\"Different\")\n",
            "        print(row)\n",
            "        print(decoded)\n",
            "Then I see the output\n",
            "Different\n",
            "  A LoyaltyImport is a ...\n",
            "  A LoyaltyImport is a...\n",
            "Different\n",
            "  You can ...\n",
            "  You can...\n",
            "Different\n",
            "    .withVariants(\"true\")\n",
            "   .withVariants(\"true\")\n",
            "This is happen especially when using of dots and spaces.\n",
            "Why does it happen and how can I fix that\n",
            "Thanks!\n",
            "Topic Timestamp: Aug 11, 2024 9:08 am\n",
            "Replies:\n",
            "  - Username: mapama247\n",
            "    Answer: Just add this line after loading the tokenizer:\n",
            "tokenizer.clean_up_tokenization_spaces = False\n",
            "Llama3 has it to true by default, as you can see on its tokenizer_config.json file.\n",
            "    Timestamp: Aug 12, 2024 7:59 am\n",
            "----------------------------------------\n",
            "Title: Initialize Vocabulary for Unigram Tokenizer\n",
            "Link: https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371\n",
            "Replies: 0\n",
            "Views: 296\n",
            "Activity: Created: Jul 11, 2023 6:28 am\n",
            "Topic Owner: dotan1111\n",
            "Topic Content: I have reviewed the documentation and code, but I was unable to find the specific method used for initializing the set of tokens in the Unigram tokenizer. Typically, it is initialized by running a BPE tokenizer, but I would like to confirm this information. If anyone could provide a link to the code or documentation or offer an answer regarding this matter, it would be greatly appreciated.\n",
            "Topic Timestamp: Jul 11, 2023 6:28 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer producing token index greater than size of the dictionary\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989\n",
            "Replies: 0\n",
            "Views: 295\n",
            "Activity: Created: May 15, 2023 7:58 pm\n",
            "Topic Owner: surya-narayanan\n",
            "Topic Content: I am using a tokenizer with a vocab size of 30522, but the tokenized dataset has a token with id 50,000 and above. Is that possible? What might I be doing wrong?\n",
            "Topic Timestamp: May 15, 2023 7:58 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Issue with pushing tokenizer to hub\n",
            "Link: https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120\n",
            "Replies: 0\n",
            "Views: 294\n",
            "Activity: Created: Oct 7, 2022 8:14 pm\n",
            "Topic Owner: hadiqa123\n",
            "Topic Content: Can anyone help me with how to resolve this issue?\n",
            "Topic Timestamp: Oct 7, 2022 8:14 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Exploring the Majestic Temples in Karnataka\n",
            "Link: https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952\n",
            "Replies: 0\n",
            "Views: 292\n",
            "Activity: Created: May 25, 2023 12:02 pm\n",
            "Topic Owner: justinpaul\n",
            "Topic Content: Karnataka, a southwestern state in India, is renowned for its rich cultural heritage and magnificent architecture. Among its many treasures, the state boasts an impressive collection of temples that showcase the artistic and spiritual prowess of ancient dynasties. From intricately carved stone structures to grandiose monuments, Karnataka’s temples offer a mesmerizing glimpse into the region’s history and devotion.\n",
            "One of the most iconic temples in Karnataka is the Virupaksha Temple in Hampi. Built in the 7th century, this UNESCO World Heritage Site is dedicated to Lord Shiva and stands as a marvel of Dravidian architecture. Its towering gopuram (gateway tower), pillared halls, and beautifully adorned sanctum make it a must-visit for history enthusiasts and spiritual seekers alike.\n",
            "Topic Timestamp: May 25, 2023 12:02 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Loading BPE modeled Tokenizer results in empty tokenizer\n",
            "Link: https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849\n",
            "Replies: 0\n",
            "Views: 291\n",
            "Activity: Created: Apr 15, 2024 12:18 pm\n",
            "Topic Owner: timdadum\n",
            "Topic Content: I am creating a GPT-like model from scratch to improve my understanding of tokenization, encoding and the architecture in itself. As such, I tried to create my own, fairly naïve BPE encoding using for-loops, but as you could guess, it was terribly slow for large vocabularies with many merges. I decided to use the Huggingface Tokenizer class from the tokenizers library with the following function to create a tokenizer:\n",
            "def create_tokenizer(corpus_path, tokenizer_path, vocab_size):\n",
            "    # Initialize a tokenizer with BPE model\n",
            "    tokenizer = Tokenizer(BPE(unk_token='<UNK>'))\n",
            "    tokenizer.pre_tokenizer = Whitespace()\n",
            "\n",
            "    # Initialize the trainer\n",
            "    trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=['<UNK>'])\n",
            "\n",
            "    # List of files to train on\n",
            "    files = [corpus_path]\n",
            "\n",
            "    # Train the tokenizer\n",
            "    tokenizer.train(files, trainer)\n",
            "\n",
            "    # Saving the tokenizer\n",
            "    tokenizer.save(tokenizer_path)\n",
            "    print(f\"Succesfully saved tokenizer at {tokenizer_path}\")\n",
            "    return tokenizer\n",
            "Which actually works well to create a tokenizer. The resulting .json file also looks good:\n",
            "{\n",
            "  \"version\": \"1.0\",\n",
            "  \"truncation\": null,\n",
            "  \"padding\": null,\n",
            "  \"added_tokens\": [\n",
            "    {\n",
            "      \"id\": 0,\n",
            "(......)\n",
            "    \"vocab\": {\n",
            "      \"<UNK>\": 0,\n",
            "      \"'\": 1,\n",
            "      \",\": 2,\n",
            "      \"-\": 3,\n",
            "      \".\": 4,\n",
            "      \"_\": 5,\n",
            "      \"a\": 6,\n",
            "      \"b\": 7,\n",
            "      \"c\": 8,\n",
            "      \"d\": 9,\n",
            "      \"e\": 10,\n",
            "      \"f\": 11,\n",
            "      \"g\": 12,\n",
            "      \"h\": 13,\n",
            "      \"i\": 14,\n",
            "Then, when I try to load it using the following method:\n",
            "def load_model(model_class, config):\n",
            "    model = model_class(**config['Hyperparameters'])\n",
            "    tokenizer = Tokenizer(BPE(unk_token='<UNK>'))\n",
            "    tokenizer.from_file(config['Files']['tokenizer'])\n",
            "    \n",
            "    # DEBUG\n",
            "    print(\"Loaded tokenizer from file:\", config['Files']['tokenizer'])\n",
            "    vocab = tokenizer.get_vocab()\n",
            "    print(\"Vocabulary size:\", len(vocab))\n",
            "    print(\"Sample entries from vocabulary:\", dict(list(vocab.items())[:10]))\n",
            "    # GUBED\n",
            "\n",
            "    model.set_tokenizer(tokenizer, config)\n",
            "    model.load_state_dict(torch.load(config['Files']['model'], map_location=config['Hyperparameters']['device']))\n",
            "    model.eval()\n",
            "    return model\n",
            "\n",
            "trained_model = load_model(gpt.GPT, config)\n",
            "Loaded tokenizer from file: tiny-llm/tokenizers/nature.json\n",
            "Vocabulary size: 0\n",
            "Sample entries from vocabulary: {}\n",
            "Tokenizer succesfully set\n",
            "Yields an empty vocabulary, as-if the tokenizer is not instantiated correctly. Yet, I do not receive any errors. I checked The Huggingface quicktour for tokenizers, but to no avail. I can’t spot any issues. Does anyone know what may cause my vocabulary to be empty?\n",
            "Topic Timestamp: Apr 15, 2024 12:18 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Authorization header is correct, but the token seems invalid\n",
            "Link: https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177\n",
            "Replies: 3\n",
            "Views: 84\n",
            "Activity: Created: Oct 10, 2024 12:02 pm\n",
            "Latest: Oct 10, 2024 2:02 pm\n",
            "Topic Owner: DiegoHexagon\n",
            "Topic Content: Hello,\n",
            "I read more solutions for this problem, but no one is valid for me.\n",
            "I use huggingface with nodejs.\n",
            "Also with a simple code I have the error.\n",
            "const { HfInference } = require(‘@huggingface/inference’);\n",
            "const hf = new HfInference({ token: ‘hf_…’ }); //\n",
            "await hf.featureExtraction({\n",
            "model: “sentence-transformers/distilbert-base-nli-mean-tokens”,\n",
            "inputs: “That is a happy person”,\n",
            "});\n",
            "I checked\n",
            "Write access to contents/settings of all repos under your personal namespace\n",
            "Make calls to the serverless Inference API\n",
            "in ther option of the AccessToken\n",
            "Maybe someone can help me.\n",
            "Topic Timestamp: Oct 10, 2024 12:02 pm\n",
            "Replies:\n",
            "  - Username: John6666\n",
            "    Answer: I’m not familiar with JavaScript, but let’s follow the sample anyway. Sometimes a model is bad, so you can try a different one.\n",
            "huggingface.co\n",
            "Hugging Face JS libraries\n",
            "We’re on a journey to advance and democratize artificial intelligence through open source and open science.\n",
            "huggingface.co\n",
            "Accessing Private/Gated Models\n",
            "We’re on a journey to advance and democratize artificial intelligence through open source and open science.\n",
            "    Timestamp: Oct 10, 2024 12:38 pm\n",
            "  - Username: DiegoHexagon\n",
            "    Answer: Thanks for reply.\n",
            "I have to use the first model for my solution\n",
            "    Timestamp: Oct 10, 2024 1:25 pm\n",
            "  - Username: DiegoHexagon\n",
            "    Answer: Solved.\n",
            "The problem was that I used \"const hf \", and this is noa accepted.\n",
            "I changed in “const inference” and now it works.\n",
            "    Timestamp: Oct 10, 2024 2:02 pm\n",
            "----------------------------------------\n",
            "Title: What file type should my training data be?\n",
            "Link: https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075\n",
            "Replies: 0\n",
            "Views: 289\n",
            "Activity: Created: Feb 20, 2023 11:07 am\n",
            "Topic Owner: TheaMT\n",
            "Topic Content: Hi,\n",
            "I want to fine tune the opus nt ar-en model using my own dataset, but I’m not sure what type of files my training data should be in? In the huggingface Marian tutorial (MarianMT) they just pass in lists of sentences, but I also read somewhere that I’m supposed to preprocess the data with Sentencepiece first, since the Marian tokenizer takes spm files. Or is sentencepiece “built in” into the Marian tokenizer? As for now, my data is a csv file. I’m a beginner, so all help is much appreciated.\n",
            "Topic Timestamp: Feb 20, 2023 11:07 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer\n",
            "Link: https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843\n",
            "Replies: 0\n",
            "Views: 287\n",
            "Activity: Created: May 14, 2023 11:49 pm\n",
            "Topic Owner: mehran\n",
            "Topic Content: I have trained a SentencePiece tokenizer locally and I would like to instantiate an XLMRobertaTokenizer object based on it. Can someone please help me understand how this is done?\n",
            "What I have at the moment are two files: my_sp.model and my_sp.vocab\n",
            "Topic Timestamp: May 14, 2023 11:49 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Easy way to compare tokenizers\n",
            "Link: https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347\n",
            "Replies: 0\n",
            "Views: 285\n",
            "Activity: Created: May 1, 2023 10:08 pm\n",
            "Topic Owner: surya-narayanan\n",
            "Topic Content: Is there any easy way to compare tokenizers? I want to see if tokenizer x is compatible with model y\n",
            "Topic Timestamp: May 1, 2023 10:08 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Pretokenise on punctuation except hyphens\n",
            "Link: https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691\n",
            "Replies: 0\n",
            "Views: 286\n",
            "Activity: Created: Apr 15, 2023 2:46 pm\n",
            "Topic Owner: Bauwens\n",
            "Topic Content: I need to preprocess some sentences so that an existing module can split them into words based on single spaces. Currently, I have:\n",
            "import tokenizers.normalizers as tn\n",
            "import tokenizers.pre_tokenizers as tp\n",
            "\n",
            "normalizer   = tn.Sequence([tn.NFD(), tn.StripAccents()])\n",
            "pretokeniser = tp.Whitespace()  # Combines WhitespaceSplit and Punctuation\n",
            "\n",
            "def preprocess(line: str):\n",
            "    pretokens = pretokeniser.pre_tokenize_str(normalizer.normalize_str(line))\n",
            "    return \" \".join([w for w,_ in pretokens])\n",
            "The problem is that my language (Dutch) has hyphenation within words for some compounds, and these are split into separate words by this process. As an example:\n",
            "Energie-efficiëntie, i.e. zuinig omgaan met stroomverbruik, wordt steeds belangrijker bij het trainen van transformer-architecturen – zoveel is zeker!\n",
            "now becomes\n",
            "Energie - efficientie , i . e . zuinig omgaan met stroomverbruik , wordt steeds belangrijker bij het trainen van transformer - architecturen – zoveel is zeker !\n",
            "Is there a way to exclude hyphens as punctuation mark?\n",
            "Topic Timestamp: Apr 15, 2023 2:46 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Why is BertNormalizer not exposed on the tokenizers library?\n",
            "Link: https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340\n",
            "Replies: 0\n",
            "Views: 279\n",
            "Activity: Created: Sep 19, 2022 12:03 pm\n",
            "Topic Owner: ndamulelonemakh\n",
            "Topic Content: Hi all\n",
            "I am trying to use the BertNormalizer() to train a ByteLevelBPETokenizer(). But it seems this normaliser is not exposed looking at the source(see attached image). I am using tokenizers==0.12.1\n",
            "Was this intentional or is it a feature that will be added going forward?\n",
            "Topic Timestamp: Sep 19, 2022 12:03 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Custom training - tokenization via collate fn or __getitem__?\n",
            "Link: https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711\n",
            "Replies: 0\n",
            "Views: 278\n",
            "Activity: Created: Apr 14, 2024 6:55 pm\n",
            "Topic Owner: malba96\n",
            "Topic Content: hello,\n",
            "I am finetuning clip with 1.1TB of image and text pairs using pytorch lighting fabric and mosaic-ml streaming datasets to load the data from multiple shards. When loading I have the option to apply the tokenizer and processor for text and images respectively in __getitem__ or using them via colllate_fn to do it batch-wise staking them afterwards.\n",
            "My question is, what is more recommended, apply transformations/tokenizers in collate_fn or __getitem__? I have seen very few examples online of people applying tokenizers/processors via collate_fn (theoretically it should be faster than in __getitem__)\n",
            "I tried to look the ViT example and it seems the transformations are done on the fly and then collate fn to stack, should I follow the same to gain training speed and reduce memory footprint?\n",
            "thank you in advance\n",
            "best,\n",
            "Topic Timestamp: Apr 14, 2024 6:55 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Use sentence-transformers/all-MiniLM-L6-v2 fully local\n",
            "Link: https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685\n",
            "Replies: 1\n",
            "Views: 196\n",
            "Activity: Created: Jun 6, 2024 1:37 pm\n",
            "Latest: Jun 6, 2024 4:24 pm\n",
            "Topic Owner: Riadrfm\n",
            "Topic Content: Hello,\n",
            "i want to use the sentence-transformers/all-MiniLM-L6-v2 totaly localy, need your support\n",
            "Br.\n",
            "Topic Timestamp: Jun 6, 2024 1:37 pm\n",
            "Replies:\n",
            "  - Username: MattiLinnanvuori\n",
            "    Answer: https://www.sbert.net SentenceTransformers Documentation has local use instructions.\n",
            "    Timestamp: Jun 6, 2024 4:24 pm\n",
            "----------------------------------------\n",
            "Title: Getting Wholeword corresponding to a subword in a text?\n",
            "Link: https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152\n",
            "Replies: 0\n",
            "Views: 275\n",
            "Activity: Created: Oct 8, 2022 8:32 pm\n",
            "Topic Owner: cramraj8\n",
            "Topic Content: I am using AutoTokenizer to tokenize a text, and then for a certain subword mention index, I am trying to find which whole word it belongs to. I was using word_ids() to find the whole word index and white-space split the text to find the corresponding wholeword. However, if the text has puncutations, the whole word count after white-space splitting is getting mismatched. How can I handle it ?\n",
            "whole_word_ids = encoded_passage.word_ids(0)\n",
            "max_wholeword_id = np.nanmax(whole_word_ids[1:-1])            \n",
            "whitespace_tokens = passage_text.split(' ')\n",
            "assert max_wholeword_id+1 == len(whitespace_tokens), \"Mismatched length\"\n",
            "Topic Timestamp: Oct 8, 2022 8:32 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Help defining tokenizer\n",
            "Link: https://discuss.huggingface.co/t/help-defining-tokenizer/38091\n",
            "Replies: 0\n",
            "Views: 275\n",
            "Activity: Created: Apr 28, 2023 11:33 pm\n",
            "Topic Owner: surya-narayanan\n",
            "Topic Content: Im just curious- i want to train multiple models on the same dataloader, kinda similar to vision- is there any way to train a new tokenizer that’s not specific to a model, such that I can run the following workflow?\n",
            "dataset = load_dataset('wiki')\n",
            "model1 = AutoModelForMaskedLM.from_pretrained(\"model1name\")\n",
            "model2 = AutoModelForMaskedLM.from_pretrained(\"model2name\")\n",
            "\n",
            "### help me with code to tokenize the dataset here\n",
            "tokenizer = (...) # I would have done a from_pretained here, but am not sure what to do, since model1 and model2 might have different tokenizers\n",
            "\n",
            "def tokenize_function(examples):\n",
            "    return tokenizer.encode(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
            "\n",
            "dataset = dataset.map(tokenize_function, batched=True)\n",
            "\n",
            "dataloader = Dataloader(dataset)\n",
            "####\n",
            "\n",
            "for x in dataloader: \n",
            "  y1 = model1(x) \n",
            "  y2 = model2(x)\n",
            "Topic Timestamp: Apr 28, 2023 11:33 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Train tokenizer for seq2seq model\n",
            "Link: https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524\n",
            "Replies: 0\n",
            "Views: 267\n",
            "Activity: Created: Apr 19, 2024 1:14 am\n",
            "Topic Owner: Nevermetyou\n",
            "Topic Content: Hi all,\n",
            "I am trying to train the seq2seq model using t5v1.1\n",
            "My task is simple; I want to map text from format A to format B\n",
            "for example, 19 July 2020 → 19/07/2020. This is not my real data, just an example of what I am doing.\n",
            "I want to train the tokenizer for this data, but for the seq2seq model, the tokenizer needs to tokenize both input data and label, right?\n",
            "So, I am a bit confused about arranging the data to train the tokenizer.\n",
            "Should I concatenate input data and label together and then pass it old_tokenizer.train_new_from_iterator\n",
            "I have read the doc Training a new tokenizer from an old one - Hugging Face NLP Course\n",
            "But I am still confused about seq2seq setting\n",
            "Thanks\n",
            "Topic Timestamp: Apr 19, 2024 1:14 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Customize FlauBERT tokenizer to split line breaks\n",
            "Link: https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011\n",
            "Replies: 0\n",
            "Views: 265\n",
            "Activity: Created: Mar 4, 2023 10:45 am\n",
            "Topic Owner: rapminerz\n",
            "Topic Content: Hello,\n",
            "I want to train FlauBERT model on french music lyrics and I want to adapt the tokenizer to my usecase : for example I’ve seen the tokenizer is actually ignoring line breaks\n",
            "How can I make it tokenize them ? I’ve also seen FlauBERT Tokenizer is a “slow” tokenizer so it cannot be trained with the .train_from_iterator() method\n",
            "Should I preprocess data myself ? Can I train another tokenizer ? I’m pretty blocked\n",
            "Thanks !\n",
            "Topic Timestamp: Mar 4, 2023 10:45 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab!\n",
            "Link: https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940\n",
            "Replies: 0\n",
            "Views: 263\n",
            "Activity: Created: Nov 21, 2023 2:36 am\n",
            "Latest: Nov 21, 2023 6:55 am\n",
            "Topic Owner: ulinnuha\n",
            "Topic Content: I load NLLB Tokenizer with new vocab after adding some vocabs using sentencepiece.\n",
            "tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M', vocab_file=NEW_SPM_NAME)\n",
            "However, I have a problem with several doubled tokens in the lang_code.\n",
            "print(tokenizer.convert_ids_to_tokens([256001]))\n",
            "['ace_Arab']\n",
            "print(tokenizer.convert_ids_to_tokens([270130]))\n",
            "['ace_Arab']\n",
            "in addition, the length of tokenizer and vocab size are different\n",
            "print(len(tokenizer)), print(tokenizer.vocab_size)\n",
            "270130\n",
            "270333\n",
            "It looks like that tokenizer.added_tokens_encoder is still in the old config before loading the new vocab\n",
            "print(tokenizer.added_tokens_encoder)\n",
            "{'<s>': 0,\n",
            " '<pad>': 1,\n",
            " '</s>': 2,\n",
            " '<unk>': 3,\n",
            " 'ace_Arab': 256001,\n",
            " 'ace_Latn': 256002,\n",
            " 'acm_Arab': 256003,\n",
            " 'acq_Arab': 256004,\n",
            " 'aeb_Arab': 256005,\n",
            " 'afr_Latn': 256006,\n",
            " 'ajp_Arab': 256007,\n",
            " 'aka_Latn': 256008,\n",
            " 'amh_Ethi': 256009,\n",
            " 'apc_Arab': 256010,\n",
            " 'arb_Arab': 256011,\n",
            " 'ars_Arab': 256012,\n",
            " 'ary_Arab': 256013,\n",
            " 'arz_Arab': 256014,\n",
            " 'asm_Beng': 256015,\n",
            "....\n",
            "\n",
            "print(tokenizer.fairseq_tokens_to_ids)\n",
            "{'<s>': 0,\n",
            " '<pad>': 1,\n",
            " '</s>': 2,\n",
            " '<unk>': 3,\n",
            " '<mask>': 270332,\n",
            " 'ace_Arab': 270130,\n",
            " 'ace_Latn': 270131,\n",
            " 'acm_Arab': 270132,\n",
            " 'acq_Arab': 270133,\n",
            " 'aeb_Arab': 270134,\n",
            " 'afr_Latn': 270135,\n",
            " 'ajp_Arab': 270136,\n",
            " 'aka_Latn': 270137,\n",
            " 'amh_Ethi': 270138,\n",
            " 'apc_Arab': 270139,\n",
            " 'arb_Arab': 270140,\n",
            " 'ars_Arab': 270141,\n",
            "How to solve it?\n",
            "Topic Timestamp: Nov 21, 2023 2:36 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: `additional_special_tokens` are not added\n",
            "Link: https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192\n",
            "Replies: 1\n",
            "Views: 186\n",
            "Activity: Created: Jun 20, 2024 12:55 pm\n",
            "Latest: Jun 20, 2024 12:58 pm\n",
            "Topic Owner: Yao-Lirong\n",
            "Topic Content: Hi Hugging Face Community,\n",
            "I have the following questions regarding special tokens:\n",
            "Why doesn’t tokenizer.all_special_tokens include <image> token? I’m using the LLaVA model that has <image> as a special token (as defined in added_tokens_decoder of tokenizer_config.json). The tokenizer encodes and decodes it indeed as a special token. However, when I load in its tokenizer and call tokenizer.all_special_tokens or tokenizer.additional_special_tokens, <image> token is not included.\n",
            "Where is <image> token loaded? I looked into the tokenizer.from_pretrained function but there doesn’t seem to be a place to actually read in the added_tokens_decoder where in config file this special token is defined ?\n",
            "Where is tokenizer.decode function defined as a special token? I tried to break-point into it to find how it skipped <image> as a special token but I seem to get into a loop call between tokenization_utils_base.py and tokenization_utils_fast.py\n",
            "It would be really helpful if you give an answer to any of these questions. Thank you very much!\n",
            "Topic Timestamp: Jun 20, 2024 12:55 pm\n",
            "Replies:\n",
            "  - Username: Yao-Lirong\n",
            "    Answer: I’m not allowed to paste more than 2 links in a post so I will provide the codes related to question 3 where I got into a loop.\n",
            "tokenization_utils_base.py and tokenization_utils_fast.py\n",
            "    Timestamp: Jun 20, 2024 12:58 pm\n",
            "----------------------------------------\n",
            "Title: Unmasking adds an extra whitespace for BPE tokenizer\n",
            "Link: https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100\n",
            "Replies: 0\n",
            "Views: 258\n",
            "Activity: Created: Jan 14, 2024 5:03 pm\n",
            "Latest: Jan 14, 2024 6:13 pm\n",
            "Topic Owner: Eghbal\n",
            "Topic Content: I created a custom BPE tokenizer for pre-training a Roberta model, utilizing the following parameters (I tried to align it with the default parameters of BPE for RoBERTa.):\n",
            "from tokenizers.models import BPE\n",
            "from tokenizers import ByteLevelBPETokenizer\n",
            "from tokenizers.processors import RobertaProcessing\n",
            "    \n",
            "tokenizer = ByteLevelBPETokenizer()\n",
            "tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False)\n",
            "tokenizer.train_from_iterator(Data_full, vocab_size = 50264, min_frequency = 2, special_tokens = [\"<s>\", \"<pad>\", \"</s>\", \"<unk>\"])\n",
            "tokenizer.add_special_tokens([\"<mask>\"])\n",
            "tokenizer.post_processor = RobertaProcessing(sep = (\"</s>\", 2), cls = (\"<s>\", 0), trim_offsets = False, add_prefix_space = False)\n",
            "tokenizer.enable_padding(direction = 'right', pad_id = 1, pad_type_id = 1, pad_token = \"<pad>\", length = 512)\n",
            "When pre-training a Roberta model with this tokenizer, I observe unusual behavior during the unmasking process:\n",
            "from tokenizers import Tokenizer\n",
            "from transformers import pipeline\n",
            "from transformers import RobertaTokenizerFast\n",
            "tokenizer_in = Tokenizer.from_file('tokenizer_file')\n",
            "tokenizer_m = RobertaTokenizerFast(tokenizer_object=tokenizer_in, clean_up_tokenization_spaces=True) \n",
            "unmasker = pipeline('fill-mask', model=model_m, tokenizer = tokenizer_m)\n",
            "unmasker(\"Capital of France is <mask>.\")\n",
            "The output consistently appears as follows: Capital of France is(two whitespaces)Paris. I’m curious about the persistent extra space before ‘Paris’. I believe activating the clean_up_tokenization_spaces option might resolve this. Could there be an error in my code leading to this issue? This happens for all unmasking tasks. Also, when I conduct a test with a command like unmasker(\"Capital of France is<mask>.\"), the quality improves and the issue seems to be resolved.\n",
            "Topic Timestamp: Jan 14, 2024 5:04 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unable to load saved tokenizer\n",
            "Link: https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631\n",
            "Replies: 0\n",
            "Views: 244\n",
            "Activity: Created: May 14, 2024 9:05 am\n",
            "Topic Owner: willsaliba\n",
            "Topic Content: I’m able to successfully train and save my tokenizer but then i cant reload it.\n",
            "tokenizer.save(tokenizer_save_path+\"tokenizer.json\") #works\n",
            "newTokenizer = Tokenizer.from_file(tokenizer_save_path+\"tokenizer.json\") #breaks\n",
            "I always get this error:\n",
            "Exception: data did not match any variant of untagged enum ModelWrapper at line 3258 column 3\n",
            "Topic Timestamp: May 14, 2024 9:05 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Leaving unknown words untokenized like in OpenMNT\n",
            "Link: https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969\n",
            "Replies: 0\n",
            "Views: 243\n",
            "Activity: Created: Oct 18, 2023 9:32 am\n",
            "Latest: Oct 18, 2023 9:56 am\n",
            "Topic Owner: KhaiKit\n",
            "Topic Content: Hi\n",
            "Is there a way to collate the OOV tokens for future finetuning?\n",
            "I wanted to leave unknown words untokenized instead of being replaced by <unk> but couldn’t figure out how to.\n",
            "For example the text \"Hi there hello word\" is sent to the tokenizer and outputs [Hi, <unk>, hello, word]\n",
            "But I want the tokenizer to output  [Hi, there, hello, word] even if the word “there” is OOV.\n",
            "Seems like OpenNMT (https://forum.opennmt.net/t/leave-unknown-words-untranslated/2790) has it implemented and I was wondering if HF has it as I would want to stick to the HF framework.\n",
            "Topic Timestamp: Oct 18, 2023 9:32 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: RobertaTokenizer decode and tokenize do not have the same output\n",
            "Link: https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709\n",
            "Replies: 0\n",
            "Views: 239\n",
            "Activity: Created: Oct 24, 2023 2:43 pm\n",
            "Topic Owner: BettyFbr\n",
            "Topic Content: I use a RobertaTokenizer to tokenize sentences that contains french characters like é or ç.\n",
            "I need the generated tokens with the Ġ character and the french characters well formatted.\n",
            "For instance with the input\n",
            "input = \"3 allées paris, 75000\"\n",
            "[tokenizer.decode([token]) for token in tokenizer.encode(input)] outputs ['<s>', ' 3', ' all', 'ées', ' paris', ',', ' 7', '5000', '</s>'] so the Ġ are replaced by spaces.\n",
            "And tokenizer.tokenize(input) outputs ['Ġ3', 'Ġall', 'Ã©es', 'Ġparis', ',', 'Ġ7', '5000'] so the french characters are not well formatted.\n",
            "I used to do this, and it used to work:\n",
            "inputs = self.tokenizer.encode_plus(input, return_tensors=\"pt\")\n",
            "ids = inputs['input_ids'].cpu().tolist()\n",
            "clean_tokens = [self.tokenizer.decode([token]) for token in ids[0]]\n",
            "But for some reasons I cannot understand, it does not output the tokens with the Ġ characters anymore and I cannot figure out what was the breaking change.\n",
            "Do you have any idea ?\n",
            "Topic Timestamp: Oct 24, 2023 2:43 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Custom Tokenizing?\n",
            "Link: https://discuss.huggingface.co/t/custom-tokenizing/78027\n",
            "Replies: 0\n",
            "Views: 237\n",
            "Activity: Created: Mar 19, 2024 4:15 am\n",
            "Topic Owner: thedarkknight7\n",
            "Topic Content: I have already tokenized my dataset in the desired format for the problem, so I don’t want to tokenize it again. I’m working with nucleotides so I want single, paired and triple sequences. However, I do want to pass this into a BERT model and would like to preprocess the data. I’ve looked at this link here: Preprocess (particularly the video). I currently have my tokens stored as a list. Is there anyway I can use that for the remaining steps mentioned in the video (i.e. converting to ids and preparing for model)?\n",
            "Thanks!\n",
            "Topic Timestamp: Mar 19, 2024 4:15 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Incorporating my tokenizer into huggingface\n",
            "Link: https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331\n",
            "Replies: 0\n",
            "Views: 223\n",
            "Activity: Created: Feb 15, 2024 2:46 pm\n",
            "Topic Owner: yuvalpinter\n",
            "Topic Content: Hi there,\n",
            "About a year ago my lab released SaGe, a tokenizer that incorporates contextual signals from corpora and thus learns tokens which are more aligned with LM objectives. The paper is here:\n",
            "ACL Anthology\n",
            "Incorporating Context into Subword Vocabularies\n",
            "Shaked Yehezkel, Yuval Pinter. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023.\n",
            "Recently, we released a version that’s much faster than the original, better streamlining the corpus for training the vocab. The (python) implementation is here:\n",
            "GitHub\n",
            "GitHub - MeLeLBGU/SaGe: Code for SaGe subword tokenizer (EACL 2023)\n",
            "Code for SaGe subword tokenizer (EACL 2023). Contribute to MeLeLBGU/SaGe development by creating an account on GitHub.\n",
            "We were wondering if and how we can get support for porting SaGe into hf tokenizers and making it a first-class member of the codebase? Would any of your engineers be able to help? What would you need from us?\n",
            "Thanks,\n",
            "Yuval\n",
            "uvp@cs.bgu.ac.il\n",
            "Topic Timestamp: Feb 15, 2024 2:46 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Train Retry Tokenizer\n",
            "Link: https://discuss.huggingface.co/t/train-retry-tokenizer/37031\n",
            "Replies: 0\n",
            "Views: 222\n",
            "Activity: Created: Apr 18, 2023 10:28 pm\n",
            "Topic Owner: Q-bert\n",
            "Topic Content: Hello, can I enlarge the tokenizer that I trained on a corpus before, by training it on another corpus?\n",
            "Thanks\n",
            "Topic Timestamp: Apr 18, 2023 10:28 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Creating tokenizer from counts file?\n",
            "Link: https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399\n",
            "Replies: 0\n",
            "Views: 216\n",
            "Activity: Created: Feb 9, 2023 7:37 pm\n",
            "Topic Owner: sabharadwaj\n",
            "Topic Content: I would like to train a wordpiece tokenizer from scratch from a counts file with tokens and counts.\n",
            "Topic Timestamp: Feb 9, 2023 7:37 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: [NER][Japanese] labeled segment shorter than token\n",
            "Link: https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330\n",
            "Replies: 0\n",
            "Views: 214\n",
            "Activity: Created: Nov 23, 2023 7:38 pm\n",
            "Latest: Nov 24, 2023 12:52 am\n",
            "Topic Owner: ewfian\n",
            "Topic Content: When dealing with Japanese NER tasks, I have a problem.\n",
            "the labeled segment “日本”[29,31] is shorter than the token ‘日本の’, how to deal with it?\n",
            "here are more cases:\n",
            "token labeled segment\n",
            "WRN END 町の 大手町\n",
            "WRN START の世界 世界チェス選手権\n",
            "WRN END ルの アップル\n",
            "WRN END 駅から ガイルドルフ西駅\n",
            "WRN END 日本の 日本\n",
            "WRN END いで 鳥羽・伏見の戦い\n",
            "WRN END 国の アメリカ合衆国\n",
            "WRN END !」 ソー・ファー、ソー・グッド…ソー・ホワット!\n",
            "WRN END 米国 米\n",
            "WRN END スの スノープス\n",
            "WRN END 内で 知内\n",
            "WRN END 市の 札幌市\n",
            "WRN END スの ゼウス\n",
            "WRN END 市の 福島県いわき市\n",
            "xlmr_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
            "text= \"株式会社ジェイアール東日本企画は、東京都渋谷区に本社を置く日本の総合広告代理店。\"\n",
            "embed = xlmr_tokenizer(text)\n",
            "print(xlmr_tokenizer.convert_ids_to_tokens(embed.input_ids))\n",
            "\n",
            "## ['<s>', '▁', '株式会社', 'ジェ', 'イ', 'ア', 'ール', '東', '日本', '企画', 'は', '、', '東京都', '渋谷', '区', 'に', '本', '社', 'を', '置', 'く', '日本の', '総合', '広告', '代理', '店', '。', '</s>']\n",
            "\"entities\": [\n",
            "            {\n",
            "                \"name\": \"株式会社ジェイアール東日本企画\",\n",
            "                \"span\": [\n",
            "                    0,\n",
            "                    15\n",
            "                ],\n",
            "                \"type\": \"法人名\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"東京都渋谷区\",\n",
            "                \"span\": [\n",
            "                    17,\n",
            "                    23\n",
            "                ],\n",
            "                \"type\": \"地名\"\n",
            "            },\n",
            "            {\n",
            "                \"name\": \"日本\",\n",
            "                \"span\": [\n",
            "                    29,\n",
            "                    31\n",
            "                ],\n",
            "                \"type\": \"地名\"\n",
            "            }\n",
            "        ]\n",
            "Topic Timestamp: Nov 23, 2023 7:38 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Caching tokenization\n",
            "Link: https://discuss.huggingface.co/t/caching-tokenization/69072\n",
            "Replies: 0\n",
            "Views: 213\n",
            "Activity: Created: Jan 14, 2024 12:16 pm\n",
            "Topic Owner: Kason123\n",
            "Topic Content: Hi I tokenize my data as follows but every time I try to run it, the code does the mapping scratch although there is a cached one in the respective folder. Can anyone help to avoid this redundant process?\n",
            "tokenizer=AutoTokenizer.from_pretrained(script_args.model_name, cache_dir=\"hf_cache_dir\", local_files_only=True)\n",
            "def tokenize_function(example):\n",
            "    return tokenizer(example[\"text\"], truncation=True)\n",
            "\n",
            "\n",
            "tokenized_datasets = dataset.map(tokenize_function, batched=True, load_from_cache_file=True)\n",
            "Topic Timestamp: Jan 14, 2024 12:16 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem\n",
            "Link: https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207\n",
            "Replies: 0\n",
            "Views: 209\n",
            "Activity: Created: Dec 18, 2023 1:30 pm\n",
            "Topic Owner: namnnumbr\n",
            "Topic Content: I’d like to use portions of the tokenizer pipeline (Normalizer, Pre-tokenizer) separately for some initial preprocessing/cleaning, do some external functions for additional preprocessing, then hand back to (a new?) tokenizer pipeline for\n",
            "normalizer\n",
            "pre-tokenizer\n",
            "–\n",
            "custom (non-tokenizer pipeline) functions\n",
            "–\n",
            "tokenizer.normalizer\n",
            "tokenizer.pre-tokenizer\n",
            "tokenizer.tokenize\n",
            "Is there a way to create a Tokenizer pipeline object that doesn’t tokenize? Or should I just do something like\n",
            "nzr = normalizers.Sequence(...)\n",
            "ptok = pre_tokenizer(...)\n",
            "\n",
            "def custom_fn(text: str):\n",
            "    # custom preprocessing\n",
            "    ...\n",
            "    return txt\n",
            "\n",
            "cleaned = custom_fn(\n",
            "    ptok.pre_tokenize_str(\n",
            "       nzr.normalize_str(text)\n",
            "    )\n",
            ")\n",
            "Further, if I hope to apply these to a Huggingface Dataset, should I just map the function to the dataset?\n",
            "my_ds = load_dataset(...)\n",
            "nzr = normalizers.Sequence(...)\n",
            "ptok = pre_tokenizer(...)\n",
            "\n",
            "my_ds = my_ds.map(nzr.normalize_str)\n",
            "my_ds = my_ds.map(ptok.pre_tokenize_str)\n",
            "Topic Timestamp: Dec 18, 2023 1:30 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Doubts about the tokenization strategy and the explanation of models through SHAP\n",
            "Link: https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803\n",
            "Replies: 0\n",
            "Views: 204\n",
            "Activity: Created: May 22, 2024 8:04 am\n",
            "Latest: May 23, 2024 6:20 am\n",
            "Topic Owner: MattBlue92\n",
            "Topic Content: Hi there!\n",
            "I have a doubt about the tokenizers of the transformers models, and to simplify I will restrict the case to BERT.\n",
            "BERT uses the wordpiece tokenizer therefore its features are word pieces and some whole words, the advantages of this approach are: a small dictionary, robustness to new words and computational efficiency.\n",
            "I’m doing a project in text-classification and an important module of this project is about XAI, I was required to use SHAP to explain BERT’s classification.\n",
            "We discovered without much surprise that SHAP highlights word pieces to explain BERT’s classification, but for my boss this isn’t good for him.\n",
            "He wanted that SHAP highlights words and not piece of words but with wordpiece tokenizer it’s impossible because that algorithm works for split words!\n",
            "Now he has asked me to train a new tokenizer that processes word-level features for BERT, but I am not very convinced about this strategy because\n",
            "hugginface does not provide a word-level tokenizer to make from scracht;\n",
            "it would result in a huge dictionary that would have to be built from the domain documents, which are very few (about a hundred documents), plus a huge generic data corpus like the one used by BERT and roBERTa to try to have as many words as possible and increase the resilience to new words;\n",
            "it is a computationally expensive operation in terms of time, effort and resources, because BERT would then have to be retrained because its knowledge was based on the features extracted from the word piece tokeniser, and to get things right it would then have to redo a hyperparametisation, which would be very challenging as I do everything.\n",
            "I don’t found papers (IEEE, elsever journals) or on web no one train word-level tokenizer to make from scracht for BERT and co.\n",
            "I am convinced that the best way forward is to try a post-processing strategy where it is possible to reconstruct the information by retrieving the word embeddings of the split words and obtain the word embeddings of the reconstructed word and then understand how this can be used by SHAP.\n",
            "Did anyone train from scratch a word-level tokenizer?\n",
            "Did anyone use a post processing tecnique to retrive the correct embedding and rappresentation of a split word for SHAP?\n",
            "Topic Timestamp: May 22, 2024 8:04 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Does AutoTokenizer uploads data to HuggingFace\n",
            "Link: https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829\n",
            "Replies: 0\n",
            "Views: 198\n",
            "Activity: Created: Oct 25, 2023 8:51 am\n",
            "Topic Owner: javigallego4\n",
            "Topic Content: I want to use AutoTokenizer in a project, but I’m a bit concerned about whether it’ll upload to HuggingFace any kind of data from my texts. Can anyone confirm if it does so ?\n",
            "Topic Timestamp: Oct 25, 2023 8:51 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: FastTokenizer add 10 more tokens in Avg\n",
            "Link: https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854\n",
            "Replies: 0\n",
            "Views: 196\n",
            "Activity: Created: Jan 20, 2024 5:48 am\n",
            "Topic Owner: Hennara\n",
            "Topic Content: Hi,\n",
            "I want to ask a question about the LlamaTokenizerFast, or TokenizerFast in general.\n",
            "I am working on a project to create LLM in support of the Arabic language, we’ve decided to extend the LlamaTokenizer with Arabic tokens following the Chinese-LLama approach.\n",
            "We add the tokens and get a new tokenizer. When converting it to TokenizerFast just by reading by “AutoTokenizer” \" the Tokenizer fast generates on average 8 more extra tokens for Arabic text.\n",
            "Can any please explain what is going on?\n",
            "Topic Timestamp: Jan 20, 2024 5:48 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Problem with AutoTokenizer\n",
            "Link: https://discuss.huggingface.co/t/problem-with-autotokenizer/93626\n",
            "Replies: 1\n",
            "Views: 139\n",
            "Activity: Created: Jun 23, 2024 3:03 pm\n",
            "Latest: Jun 24, 2024 7:37 am\n",
            "Topic Owner: agnavale\n",
            "Topic Content: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
            "\n",
            "model_path = \"./raphael\"\n",
            "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
            "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
            "\n",
            "# Example prompts\n",
            "prompts = [\n",
            "    \"What is your name?\",\n",
            "    \"Who are you?\",\n",
            "    \"Do you know Raphael\",\n",
            "]\n",
            "\n",
            "# Tokenize and generate responses\n",
            "for prompt in prompts:\n",
            "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
            "    input_ids = inputs[\"input_ids\"]\n",
            "    outputs = model.generate(input_ids, max_length=100)\n",
            "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
            "    print(f\"Prompt: {prompt}\")\n",
            "    print(f\"Response: {response}\\n\")\n",
            "This my code when using AutoTokenizer. and It gives error. Exception: Error while initializing BPE: Token _</w> out of vocabulary\n",
            "but same program works fine when I use BlenderbotSmallTokenizer in place of AutoTokenizer.\n",
            "from transformers import BlenderbotSmallForConditionalGeneration, BlenderbotSmallTokenizer\n",
            "\n",
            "model_path = \"./raphael\"\n",
            "model = BlenderbotSmallForConditionalGeneration.from_pretrained(model_path)\n",
            "tokenizer = BlenderbotSmallTokenizer.from_pretrained(model_path)\n",
            "What is exactly the problem?\n",
            "Topic Timestamp: Jun 23, 2024 3:03 pm\n",
            "Replies:\n",
            "  - Username: agnavale\n",
            "    Answer: Is it even possible to write errorless code for this library. I’m always getting some or the other errors. tired of all this shit.\n",
            "    Timestamp: Jun 24, 2024 7:37 am\n",
            "----------------------------------------\n",
            "Title: T5 tokenizer vs t51.1 tokenizer\n",
            "Link: https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421\n",
            "Replies: 0\n",
            "Views: 195\n",
            "Activity: Created: Mar 1, 2024 2:35 am\n",
            "Topic Owner: Nevermetyou\n",
            "Topic Content: Hello, there\n",
            "I wonder if the tokenizer of t51.1 is the same as that of normal t5.\n",
            "Or is the pre-tokenization step not the same?\n",
            "Topic Timestamp: Mar 1, 2024 2:35 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to deal SQL query in tabular dataset?\n",
            "Link: https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570\n",
            "Replies: 0\n",
            "Views: 192\n",
            "Activity: Created: Feb 17, 2024 12:11 pm\n",
            "Topic Owner: Iamexperimenting\n",
            "Topic Content: Hi, I have a dataset which contains numerical columns, categorical columns and SQL Query as a separate column. Can someone recommend me how to deal with it?\n",
            "I would like to get suggestion on how to deal with SQL Query column? what sort of feature engineering should I apply for SQL Query column?\n",
            "Is there any specific model to deal with this?\n",
            "Datasets Beginners Models Intermediate Tokenizers Research Community Calls\n",
            "Topic Timestamp: Feb 17, 2024 12:11 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Modifying normalizer for pretrained tokenizers don’t consistently work\n",
            "Link: https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729\n",
            "Replies: 2\n",
            "Views: 111\n",
            "Activity: Created: Jun 12, 2024 11:07 am\n",
            "Latest: Jun 12, 2024 11:10 am\n",
            "Topic Owner: alvations\n",
            "Topic Content: I’m not sure if it’s a bug/feature but sometimes modifying the normalizer of a pretrained tokenizer works but sometimes it doesn’t.\n",
            "For example, it works for \"mistralai/Mistral-7B-v0.1\" but not \"mistralai/Mistral-7B-v0.3\":\n",
            "from transformers import AutoTokenizer\n",
            "from tokenizers.normalizers import Sequence, Replace, Prepend\n",
            "\n",
            "tokenizer_name = \"mistralai/Mistral-7B-v0.1\"\n",
            "old_tok = AutoTokenizer.from_pretrained(tokenizer_name)\n",
            "\n",
            "assert old_tok.backend_tokenizer.normalizer != None\n",
            "\n",
            "new_normalizer = Sequence(\n",
            "    [Prepend('▁'), Replace('▁', ' '), Replace(\"foo\", \"bar\"), Replace('<br>', '\\n')]\n",
            ")\n",
            "\n",
            "old_tok.backend_tokenizer.normalizer = new_normalizer\n",
            "new_tokenizdr_name = f\"new_tokenizer-{tokenizer_name}\"\n",
            "old_tok.save_pretrained(new_tokenizdr_name)\n",
            "\n",
            "\n",
            "old_tok = AutoTokenizer.from_pretrained(tokenizer_name)\n",
            "new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name)\n",
            "[out]:\n",
            ">>> print(' '.join(old_tok.batch_decode(old_tok(\"I foo you<br>hello world\")['input_ids'])))\n",
            "<s> I foo you < br > hello world\n",
            "\n",
            ">>> print(' '.join(new_tok.batch_decode(new_tok(\"I foo you<br>hello world\")['input_ids'])))\n",
            "<s>  I  bar  you \n",
            " hello  world\n",
            "The same process above won’t work for \"mistralai/Mistral-7B-v0.3\".\n",
            "Topic Timestamp: Jun 12, 2024 11:07 am\n",
            "Replies:\n",
            "  - Username: alvations\n",
            "    Answer: Also raised issue on Modifying normalizer for pretrained tokenizers don't consistently work · Issue #1552 · huggingface/tokenizers · GitHub\n",
            "And asked on python - How do we add/modify the normalizer in a pretrained Huggingface tokenizer? - Stack Overflow\n",
            "    Timestamp: Jun 12, 2024 11:08 am\n",
            "  - Username: alvations\n",
            "    Answer: A few more follow-up questions:\n",
            "How do we add/modify the normalizer in a pretrained Huggingface tokenizer?\n",
            "Can any normalizer from a pretrained tokenizer be modified or just specific ones?\n",
            "If the latter, why and how do we know if a pretrained tokenizer’s normalizer can be extended or modified?\n",
            "Can we add a normalizer when the pretrained one doesn’t already have one?\n",
            "    Timestamp: Jun 12, 2024 11:10 am\n",
            "----------------------------------------\n",
            "Title: Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct\n",
            "Link: https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459\n",
            "Replies: 0\n",
            "Views: 188\n",
            "Activity: Created: Feb 16, 2024 12:03 pm\n",
            "Topic Owner: Michael22\n",
            "Topic Content: Hi,\n",
            "I have an issue with German umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct tokenizer.\n",
            "If I run the following script I got � für every German umlaut.\n",
            "from transformers import AutoTokenizer\n",
            "import torch\n",
            "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
            "print(\"Target Device: \", DEVICE)\n",
            "MODEL = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
            "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
            "print (tokenizer)\n",
            "inputs = tokenizer(\"A\", return_tensors=\"pt\").to(\"cuda\")\n",
            "inputs.input_ids[0][1] = 32009\n",
            "print (inputs)\n",
            "decoded = tokenizer.decode(inputs.input_ids[0])\n",
            "print (decoded)\n",
            "How to fix this?\n",
            "Topic Timestamp: Feb 16, 2024 12:03 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Construct a Marian tokenizer. Based on huggingface tokenizers\n",
            "Link: https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679\n",
            "Replies: 0\n",
            "Views: 185\n",
            "Activity: Created: May 7, 2024 5:27 pm\n",
            "Topic Owner: RaphaelKalandadze\n",
            "Topic Content: I have a Hugging Face tokenizer with three files: tokenizer.json, tokenizer_config.json, and vocab.txt. However, according to the documentation, the Marian tokenizer requires files in the SentencePiece format (.model and .vocab files). Is there a way to construct a Marian tokenizer without these specific file formats?\n",
            "I have already tried converting the tokenizer into a .model file and then constructing the Marian tokenizer, but it raised new issues. Are there any alternative approaches or workarounds to use the existing tokenizer files with the Marian tokenizer?\n",
            "Topic Timestamp: May 7, 2024 5:27 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Where the introduction of tokenizers.implementations?\n",
            "Link: https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959\n",
            "Replies: 0\n",
            "Views: 183\n",
            "Activity: Created: May 7, 2023 8:51 am\n",
            "Topic Owner: Love2DL\n",
            "Topic Content: I look at the doc in Tokenizers, i don’t find the introduction of tokenizers.implementations.Can give me a website, thank you!\n",
            "Topic Timestamp: May 7, 2023 8:51 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unable to register my own tokenizer\n",
            "Link: https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024\n",
            "Replies: 0\n",
            "Views: 179\n",
            "Activity: Created: Nov 21, 2023 2:37 pm\n",
            "Topic Owner: RANITBAG\n",
            "Topic Content: I have tried to register my own tokenization model based on sentencepiece using CustomAITokenizer.register_for_auto_class(“AutoTokenizer”) . But I am falied to do so.\n",
            "RANITBAG/CustomAItokenizer at main . This the repo link. Can anyone help me in this?\n",
            "Topic Timestamp: Nov 21, 2023 2:37 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: How to handle translations one source language to many target sentences for the same language\n",
            "Link: https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669\n",
            "Replies: 0\n",
            "Views: 178\n",
            "Activity: Created: Nov 9, 2023 3:29 pm\n",
            "Topic Owner: sancelot\n",
            "Topic Content: Hi,\n",
            "I am training a m2m100 model with more technical corpus data using IATE database (https://iate.europa.eu/)\n",
            "I don’t know how to handle a case , when a source text may have different significations for the same sentence.\n",
            "That means for one source language sentence , I will assign 3 targets language definitions\n",
            "What would happen ???\n",
            "by example , I have these data for the same term, I have 3 associated definitions in english language :\n",
            "line[61163] = ['1448173', 'mechanical engineering', 'en', 'to jam', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z']\n",
            "line[61164] = ['1448173', 'mechanical engineering', 'en', 'to seize', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z']\n",
            "line[61165] = ['1448173', 'mechanical engineering', 'en', 'to get stuck', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z']\n",
            "line[61166] = ['1448173', 'mechanical engineering', 'de', 'festlaufen', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z']\n",
            "Topic Timestamp: Nov 9, 2023 3:29 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Is it safe to assume tokenizer does not change after initialization?\n",
            "Link: https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379\n",
            "Replies: 0\n",
            "Views: 171\n",
            "Activity: Created: Mar 30, 2024 3:46 am\n",
            "Topic Owner: youkaichao\n",
            "Topic Content: Hi, I find many attributes in tokenizer are very expensive to compute, e.g.\n",
            "    def __len__(self):\n",
            "        \"\"\"\n",
            "        Size of the full vocabulary with the added tokens. Counts the `keys` and not the `values` because otherwise if\n",
            "        there is a hole in the vocab, we will add tokenizers at a wrong index.\n",
            "        \"\"\"\n",
            "        return len(set(self.get_vocab().keys()))\n",
            "Therefore, I want to cache the results of some attributes on tokenizer. I do see some functions that can change a tokenizer, but I find they are only used during initialization, and these methods are marked as private by an _ prefix, e.g. _update_trie/_add_tokens .\n",
            "Is it safe to assume a tokenizer does not change after initialization? If not, what are some typical use cases to change a tokenizer after initialization?\n",
            "Topic Timestamp: Mar 30, 2024 3:46 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Special_tokens_mask\n",
            "Link: https://discuss.huggingface.co/t/special-tokens-mask/69161\n",
            "Replies: 0\n",
            "Views: 169\n",
            "Activity: Created: Jan 15, 2024 4:53 am\n",
            "Topic Owner: suhg-203\n",
            "Topic Content: Hi there, when training tokenizer from scratch, after encoding text, I caught a weird special_tokens_mask. Not as I expected, the special_tokens_mask is not [1, 0, 0, 0, 0, 0, 0, 0, 0, 1] . Bellow is my script. Can anyone help me out? Thank you very much!\n",
            "Topic Timestamp: Jan 15, 2024 4:53 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Preprocessing of dataset\n",
            "Link: https://discuss.huggingface.co/t/preprocessing-of-dataset/81086\n",
            "Replies: 0\n",
            "Views: 166\n",
            "Activity: Created: Apr 10, 2024 1:54 pm\n",
            "Latest: Apr 10, 2024 2:01 pm\n",
            "Topic Owner: boringblobking\n",
            "Topic Content: I’m going through this notebook and below is a gist of some of it:\n",
            "datasets = load_dataset(\"squad_v2\")\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
            "example = datasets[\"train\"][i]\n",
            "\n",
            "tokenized_example = tokenizer(\n",
            "    example[\"question\"],\n",
            "    example[\"context\"],\n",
            "    max_length=max_length,\n",
            "    truncation=\"only_second\",\n",
            "    return_overflowing_tokens=True,\n",
            "    return_offsets_mapping=True,\n",
            "    stride=doc_stride\n",
            ")\n",
            "\n",
            "sequence_ids = tokenized_example.sequence_ids()\n",
            "\n",
            "answers = example[\"answers\"]\n",
            "start_char = answers[\"answer_start\"][0]\n",
            "end_char = start_char + len(answers[\"text\"][0])\n",
            "# Start token index of the current span in the text.\n",
            "token_start_index = 0\n",
            "while sequence_ids[token_start_index] != 1:\n",
            "    token_start_index += 1\n",
            "\n",
            "# End token index of the current span in the text.\n",
            "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
            "while sequence_ids[token_end_index] != 1:\n",
            "    token_end_index -= 1\n",
            "So as you can see, return_overflowing_tokens is True so the tokenized_examples input_ids are a list of lists so tokenized_example[“input_ids”][0] would contain just the first segment of the context for that sample.\n",
            "So then to get token_end_index, the code starts at len(tokenized_example[“input_ids”][0]) - 1 and then moves backwards until it finds a 1. But surely len(tokenized_example[“input_ids”][0]) - 1 isn’t going to be the end of the context because len(tokenized_example[“input_ids”][0]) is only going to give you the length of the first segment of the context?\n",
            "Topic Timestamp: Apr 10, 2024 1:54 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Treat Hawaiian Glottal stop as consonant, not punctuation\n",
            "Link: https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531\n",
            "Replies: 0\n",
            "Views: 165\n",
            "Activity: Created: Apr 19, 2024 2:33 am\n",
            "Topic Owner: HURIMOZ\n",
            "Topic Content: Iʻm struggling to get the ʻokina, the Hawaiian glottal stop character (U+02BB), treated as a letter and not as punctuation in SentencePiece subword tokenization, whether BPE or Unigram. Can someone tell me how I can achieve that please?\n",
            "This is what I attempted:\n",
            "spm_train --input=tgt-train.txt --model_prefix=data/tgt_spm --vocab_size=32000 --model_type=bpe --character_coverage=1.0 --output_format=piece --input_sentence_size=1000000 --user_defined_symbols=ʻa,ʻe,ʻi,ʻo,ʻu,ʻā,ʻē,ʻī,ʻō,ʻū\n",
            "However, only these specific tokens above are listed in the created vocab file, when I want all tokens coming from a word that contains the glottal stop to carry the glottal stop.\n",
            "Topic Timestamp: Apr 19, 2024 2:33 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: The process for tokenizing concatenated dataset is slow st the end of tokenizing\n",
            "Link: https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412\n",
            "Replies: 0\n",
            "Views: 163\n",
            "Activity: Created: Oct 30, 2023 10:35 am\n",
            "Topic Owner: pkr7098\n",
            "Topic Content: I am tokenizing wikipedia English and bookcorpus dataset, which is concatenated in one dataset for training GPT2. Tokenizing each of dataset is fast(i.e. not concatenated) but after concatenation, the tokenizing process is extremly slow at the end of tokenizing. I am using fast tokenizer option\n",
            "Topic Timestamp: Oct 30, 2023 10:35 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error\n",
            "Link: https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874\n",
            "Replies: 6\n",
            "Views: 36\n",
            "Activity: Created: Nov 5, 2024 7:46 pm\n",
            "Latest: Nov 6, 2024 8:17 am\n",
            "Topic Owner: antoshka1608\n",
            "Topic Content: I’m encountering an issue when trying to load my custom tokenizer from a model repository on the Hugging Face Hub. Despite following the documentation for custom tokenizers.\n",
            "huggingface.co\n",
            "antoshka1608/wordpiece-tokenizer-v1 at main\n",
            "We’re on a journey to advance and democratize artificial intelligence through open source and open science.\n",
            "To load the tokenizer, I’m using:\n",
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(“antoshka1608/wordpiece-tokenizer-v1”, use_fast=False, trust_remote_code=True)\n",
            "When loading the tokenizer, it downloads tokenizer_config.json and vocab.json but then fails with the Tokenizer class BaseTokenizer does not exist or is not currently imported error.\n",
            "Has anyone else encountered this issue or have suggestions on what might be going wrong? Any guidance on troubleshooting this would be greatly appreciated!\n",
            "Thank you for your help!\n",
            "Topic Timestamp: Nov 5, 2024 7:46 pm\n",
            "Replies:\n",
            "  - Username: John6666\n",
            "    Answer: That error usually occurs when the transoformers library is out of date, but it’s hard to imagine that you’re using a version so old that BaseTokeniker isn’t defined.\n",
            "pip install -U transformers\n",
            "There could be some other cause, such as the manual now being out of date.\n",
            "    Timestamp: Nov 6, 2024 4:14 am\n",
            "  - Username: antoshka1608\n",
            "    Answer: Sorry, for, probably, misleading you, its not transformer’s tokenizer, but my custom ( i did it, inherited from PreTrainedTokenizer)\n",
            "And yes, i reinstalled transformers mant times, doesnt work.\n",
            "After your reply, i thought that this is might be issue with name duplication, but no. With new name of my tokenizer the same error(but of course with new tokenizer name ) .\n",
            "    Timestamp: Nov 6, 2024 7:21 am\n",
            "  - Username: John6666\n",
            "    Answer: huggingface.co\n",
            "CohereForAI/c4ai-command-r-plus · ValueError: Tokenizer class CohereTokenizer...\n",
            "Running this example script gives an error\n",
            "Oh, I see. The name is the same as the existing one, and the tokenizer is also a custom one. Have you encountered this bug?\n",
            "Also, in the case of HF in general, there are cases where information that was true at the time is now false in the manual, so I think the easiest thing to do is to refer to the py or json of someone else’s model that is working. It’s tough if there is no similar model…\n",
            "The next best thing is to refer to the code of the library itself.\n",
            "    Timestamp: Nov 6, 2024 7:53 am\n",
            "  - Username: antoshka1608\n",
            "    Answer: Yeah, thank you for you respond, anyway!!\n",
            "Actually, if i import my tokenizer directly from\n",
            "directory of my project.\n",
            "tokenizer = CustomBaseTokenizer.from_pretrained(hugging_face_name)\n",
            "It works good, but probably it takes just vocab file and doesnt use another files, so we cant check anything here.\n",
            "Ill try your idea about other’s works later.\n",
            "    Timestamp: Nov 6, 2024 8:03 am\n",
            "  - Username: John6666\n",
            "    Answer: Many of the HF libraries use hard-coded file names, so sometimes they work and sometimes they don’t. If it works locally but not online, the problem is often with the file names, their placement, or the YAML part of README.md (the de facto repository configuration file).\n",
            "Also, in your case, since you’re using a gated model, there’s a chance that the error is occurring because you’re failing to pass the token.\n",
            "Even when calling the tokenizer, you need a token to read the repo.\n",
            "If it seems like a bug in the library, you’ll need to find a way to make the error more visible and identify the bug itself. Or you could find the part that’s causing the problem and bypass it.\n",
            "In this case, I’ll go read the github…\n",
            "    Timestamp: Nov 6, 2024 8:10 am\n",
            "  - Username: antoshka1608\n",
            "    Answer: Yep, going to do a deep dive later.\n",
            "    Timestamp: Nov 6, 2024 8:17 am\n",
            "----------------------------------------\n",
            "Title: Tokenizer: what function removes spaces between ‘<’ and ‘>’?\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257\n",
            "Replies: 0\n",
            "Views: 30\n",
            "Activity: Created: Dec 9, 2024 7:46 pm\n",
            "Topic Owner: idruker\n",
            "Topic Content: Here is an detokenized sequence:\n",
            "print(tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=False, clean_up_tokenization_space=False))\n",
            "<tool_call>\n",
            "{“arguments”: {“symbol”: “AAPL”}, “name”: “get_stock_fundamentals”}\n",
            "</tool_call><|im_end|>\n",
            "As you see, </tool_call> is a construct without spaces. The original sequence of token ids is\n",
            "700 6462 28730 2845 28767 which is\n",
            "“</” “tool” “_” “call” “>”\n",
            "What Transformers function implements the logic of removal of the spaces? How does it know that ‘tool’, ‘_’ and ‘call’ are part of one keyword?\n",
            "Would appreciate your guidance.\n",
            "Topic Timestamp: Dec 9, 2024 7:46 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Paligemma model Forward Method Not Returning Loss in Trainer #31045\n",
            "Link: https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591\n",
            "Replies: 0\n",
            "Views: 155\n",
            "Activity: Created: May 26, 2024 2:55 pm\n",
            "Topic Owner: damerajee\n",
            "Topic Content: I was trying to finetune goggle new vision lanaguge model and I keep getting this error\n",
            "I was running the code on kaggle notebook free T4 and also tried with P100\n",
            "Google Colab\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "---------------------------------------------------------------------------\n",
            "ValueError                                Traceback (most recent call last)\n",
            "Cell In[14], line 1\n",
            "----> 1 trainer.train()\n",
            "\n",
            "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1876, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n",
            "   1873 try:\n",
            "   1874     # Disable progress bars when uploading models during checkpoints to avoid polluting stdout\n",
            "   1875     hf_hub_utils.disable_progress_bars()\n",
            "-> 1876     return inner_training_loop(\n",
            "   1877         args=args,\n",
            "   1878         resume_from_checkpoint=resume_from_checkpoint,\n",
            "   1879         trial=trial,\n",
            "   1880         ignore_keys_for_eval=ignore_keys_for_eval,\n",
            "   1881     )\n",
            "   1882 finally:\n",
            "   1883     hf_hub_utils.enable_progress_bars()\n",
            "\n",
            "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n",
            "   2213     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n",
            "   2215 with self.accelerator.accumulate(model):\n",
            "-> 2216     tr_loss_step = self.training_step(model, inputs)\n",
            "   2218 if (\n",
            "   2219     args.logging_nan_inf_filter\n",
            "   2220     and not is_torch_xla_available()\n",
            "   2221     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n",
            "   2222 ):\n",
            "   2223     # if loss is nan or inf simply add the average of previous logged losses\n",
            "   2224     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n",
            "\n",
            "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3238, in Trainer.training_step(self, model, inputs)\n",
            "   3235     return loss_mb.reduce_mean().detach().to(self.args.device)\n",
            "   3237 with self.compute_loss_context_manager():\n",
            "-> 3238     loss = self.compute_loss(model, inputs)\n",
            "   3240 del inputs\n",
            "   3241 torch.cuda.empty_cache()\n",
            "\n",
            "File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3282, in Trainer.compute_loss(self, model, inputs, return_outputs)\n",
            "   3280 else:\n",
            "   3281     if isinstance(outputs, dict) and \"loss\" not in outputs:\n",
            "-> 3282         raise ValueError(\n",
            "   3283             \"The model did not return a loss from the inputs, only the following keys: \"\n",
            "   3284             f\"{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.\"\n",
            "   3285         )\n",
            "   3286     # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
            "   3287     loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
            "\n",
            "ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask,pixel_values,labels.\n",
            "Topic Timestamp: May 26, 2024 2:55 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Which file stores token frequency in SentencePieceBPETokenizer?\n",
            "Link: https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954\n",
            "Replies: 0\n",
            "Views: 155\n",
            "Activity: Created: May 3, 2024 6:52 am\n",
            "Topic Owner: raptorkwok\n",
            "Topic Content: After training a Tokenizer, there are several files generated:\n",
            "merges.txt\n",
            "special_tokens_map.json\n",
            "tokenizer.json\n",
            "tokenizer_config.json\n",
            "vocab.json\n",
            "However, none of these store the frequency of the tokens found in the training dataset. Does it mean the training process does not store such data?\n",
            "The reason for this question is that I found that some words that frequently appeared in the dataset are not included in the token list, while some words with fewer occurrences are included in the token list.\n",
            "How does a SentencePieceBPETokenizer choose tokens from a dataset?\n",
            "Topic Timestamp: May 3, 2024 6:52 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Translate from one tokenizer to another\n",
            "Link: https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811\n",
            "Replies: 0\n",
            "Views: 154\n",
            "Activity: Created: Apr 15, 2024 9:03 am\n",
            "Latest: Apr 15, 2024 9:03 am\n",
            "Topic Owner: broadwayj\n",
            "Topic Content: I got tokens encoded with one tokenizer which I want to yield to the LM with another tokenizer. problem seems natural so may be there exist some convenient and unified solution to do so? Here’s what I use rn\n",
            "def translate_to_other_tokenizer(ids, tokenizer_from, tokenizer_to):\n",
            "    text = tokenizer_from.batch_decode(\n",
            "        ids,\n",
            "        skip_special_tokens=True,\n",
            "        clean_up_tokenization_spaces=True\n",
            "    )\n",
            "\n",
            "    output = tokenizer_to(\n",
            "        text,\n",
            "        truncation=False,\n",
            "        padding=True,\n",
            "        return_attention_mask=True,\n",
            "        return_special_tokens_mask=True,\n",
            "        return_tensors='pt',\n",
            "    )\n",
            "\n",
            "    return text, output\n",
            "Topic Timestamp: Apr 15, 2024 9:03 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: OPT special tokens\n",
            "Link: https://discuss.huggingface.co/t/opt-special-tokens/78667\n",
            "Replies: 0\n",
            "Views: 145\n",
            "Activity: Created: Mar 25, 2024 8:56 am\n",
            "Topic Owner: aljazpotocnik\n",
            "Topic Content: Hello,\n",
            "I can’t understand something about OPT tokenizer and its special tokens. I came across <unk> token in opt vocabulary. But when encoding it with opt_tokenizer it doesn’t find it as a single token but three tokens: <, unk, >. How does this make sense?\n",
            "Below is some code to reproduce my findings:\n",
            "opt_tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
            "\n",
            "print('<unk>' in opt_tokenizer.get_vocab())\n",
            "\n",
            "ids = opt_tokenizer.encode(\"<unk>\", add_special_tokens=False)\n",
            "\n",
            "print(opt_tokenizer.convert_ids_to_tokens(ids))\n",
            "Another funny thing is that <unk> token isn’t present in opt_tokenizer.special_tokens_map:\n",
            "print(opt_tokenizer.special_tokens_map)\n",
            "{‘bos_token’: ‘</s>’,\n",
            "‘eos_token’: ‘</s>’,\n",
            "‘unk_token’: ‘</s>’,\n",
            "‘pad_token’: ‘<pad>’}\n",
            "Best,\n",
            "AP\n",
            "Topic Timestamp: Mar 25, 2024 8:56 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: BUGs on offset-mapping\n",
            "Link: https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313\n",
            "Replies: 0\n",
            "Views: 143\n",
            "Activity: Created: May 24, 2024 4:39 pm\n",
            "Topic Owner: Ruiton\n",
            "Topic Content: Hi guys, I recently notice that the offset mapping returned by tokenizer seems to be problematic. I am working with the llama3-8b-Instruct model with dtype=fp16.\n",
            "text='''Suppose A represents a certain relation. Infer the relation based on certain examples.'''\n",
            "token_ranges = tokenizer(text,return_offsets_mapping=True)['offset_mapping'] \n",
            "and what it returns is:\n",
            "[(0, 0), (0, 3), (3, 7), (7, 7), (9, 9), (20, 20), (22, 22), (30, 30), (39, 39), (40, 40), (46, 46), (50, 50), (59, 59), (65, 65), (68, 68), (76, 76), (85, 85)]\n",
            "I assume that what it should return is the starting and end position of the token. Is there a bug or something that I misunderstand?\n",
            "Topic Timestamp: May 24, 2024 4:39 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Encode token without spaced between them\n",
            "Link: https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955\n",
            "Replies: 0\n",
            "Views: 141\n",
            "Activity: Created: May 9, 2024 11:03 am\n",
            "Latest: May 9, 2024 11:07 am\n",
            "Topic Owner: ron5569\n",
            "Topic Content: I’m working with an LLM that generates files in unified diff format. However, in some cases, the LLM generates invalid output due to spaces between tokens.\n",
            "For example\n",
            "--- EventsLiteTestKit.scala 2024-05-31 07:00:00\n",
            "+++ EventsLiteTestKit.scala 2024-05-31 07:00:01\n",
            "@@ -39,10 +39,10 @@\n",
            "\n",
            " import java.util.UUID\n",
            " import scala.collection.concurrent.TrieMap\n",
            "- import scala.concurrent.Future\n",
            "+ import scala.concurrent.{Future, _}\n",
            " import scala.concurrent.Future.{failed, successful}\n",
            "\n",
            " class EventsLiteTestKit {\n",
            "This is not a valid patch file because of the space between the ‘-’ character and the word ‘import’.\n",
            "Any ideas on how to force the model to encode such that there are no spaces after the ‘-’ and ‘+’ symbols?\n",
            "Topic Timestamp: May 9, 2024 11:03 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: AutoTokenizer.encode with multiThread and mutliProcess\n",
            "Link: https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822\n",
            "Replies: 2\n",
            "Views: 43\n",
            "Activity: Created: Oct 9, 2024 3:48 am\n",
            "Latest: Oct 9, 2024 9:20 am\n",
            "Topic Owner: DaleMeng\n",
            "Topic Content: Hello, every one, I want to convert the all texts to index tokens and save them in files.\n",
            "So I use the encode Function to do it.\n",
            "I tried it with the multiProcess way and multiThread way,\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"./llama2-tokenizer\", trust_remote_code=True)\n",
            "\n",
            "#MultiThread way\n",
            "with ThreadPoolExecutor(max_workers=8) as pool:\n",
            "        result = list(pool.map(tokenizer,data))  #data is a list of str\n",
            "\n",
            "#MultiProcess way\n",
            "with ProcessPoolExecutor(max_workers=8) as pool:\n",
            "        result = list(pool.map(tokenizer,data))\n",
            "result shows the MultiProcess way is much slower than the MultiThread way. With total 1M tokens, MultiProcess way takes about 34s while MultiThread way takes about 2s.\n",
            "Isn’t the encode function computationally intensive? Since the low-level implementation of the encode is in the Tokenizers package with Rust. I can not figure out how the above result was caused. Can anybody offers some explanation ? Thanks!\n",
            "Topic Timestamp: Oct 9, 2024 3:48 am\n",
            "Replies:\n",
            "  - Username: samchain\n",
            "    Answer: Hey\n",
            "Best option would be to use the hugging face datasets class and use the “.map()” method with an argument “num_proc” that enables parallel tokenization : Main classes\n",
            "    Timestamp: Oct 9, 2024 7:59 am\n",
            "  - Username: DaleMeng\n",
            "    Answer: Thanks for your answer!\n",
            "I know that it is one of a good way to use huggingface datasets.map function. But I also wonder how to do it in a more custom way without such module. And I find that by adding the chunksize parameter, the total time with MultiProcess way decrease a lot, about that same as MultiThread way.\n",
            "    Timestamp: Oct 9, 2024 9:20 am\n",
            "----------------------------------------\n",
            "Title: Escape symbol appearance\n",
            "Link: https://discuss.huggingface.co/t/escape-symbol-appearance/82015\n",
            "Replies: 0\n",
            "Views: 125\n",
            "Activity: Created: Apr 16, 2024 9:33 am\n",
            "Topic Owner: broadwayj\n",
            "Topic Content: I have a dataset of prefixes which I would like to continue up with my LM. Further I want to build up a dataset of generated sentences and save it to csv file as raw text. Thereby I do not want delimiter to appear within generated symbols by no means. Currently I am using parameter bad_words_ids=DELIMITER_SYMBOL_ID in GenerationConfig of LM to ban delimiter symbol from being generated. However, theoretically this symbol could be a part of some other more complex token from tokenizer’s keys (e.g. if delimiter is !, then token !!! could still persist in tokenizer), which will be decoded to text with banned delimiter symbol. How do I completely erase this symbol from final text? I could only come up with filtration of resulted text to manually substitute all delimiters with some third party symbol. May be there exist some more convenient ways?\n",
            "Topic Timestamp: Apr 16, 2024 9:33 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Printing tokens array\n",
            "Link: https://discuss.huggingface.co/t/printing-tokens-array/81427\n",
            "Replies: 0\n",
            "Views: 124\n",
            "Activity: Created: Apr 12, 2024 1:06 pm\n",
            "Topic Owner: Mahmoodn\n",
            "Topic Content: Hi,\n",
            "How can I see the tokens with tokenizer()? In the example:\n",
            "raw_inputs = [\n",
            "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
            "    \"I hate this so much!\",\n",
            "]\n",
            "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
            "print(inputs)\n",
            "The inputs contains the unique numbers. I would like to see the tokens. For example, I want to know if ['I've'] is the token or ['i',''','ve'] is the token.\n",
            "Topic Timestamp: Apr 12, 2024 1:06 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Unusual input_id size for distilBERT tokenizer\n",
            "Link: https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695\n",
            "Replies: 0\n",
            "Views: 112\n",
            "Activity: Created: May 14, 2024 5:29 pm\n",
            "Topic Owner: rishuhuhu\n",
            "Topic Content: I am tokenizing a news article using DistilBERT tokenizer whose max token length supposed to be 512, but I a getting 280,\n",
            "I dont know what is the issue, I have worked with DistilBERT before and it used to give 512 as the max length…\n",
            "Topic Timestamp: May 14, 2024 5:29 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Create entirely new vocabulary for tokenizer\n",
            "Link: https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453\n",
            "Replies: 0\n",
            "Views: 112\n",
            "Activity: Created: May 30, 2024 3:48 pm\n",
            "Topic Owner: Svangorden13\n",
            "Topic Content: I am building an encoder-decoder model based off facebook-bart-base for the purpose of solving math problems. I would like to train the model so that the decoder can only output a small set of words (e.g. “multiply”, “divide”, “add”, “subtract”, etc.) and numbers. Is it possible to completely redefine the corpus used by the decoder tokenizer, rather than just adding new tokens to it?\n",
            "Topic Timestamp: May 30, 2024 3:48 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Emojis poisoning tokenizer\n",
            "Link: https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479\n",
            "Replies: 0\n",
            "Views: 103\n",
            "Activity: Created: Jun 17, 2024 2:09 am\n",
            "Topic Owner: Dts1\n",
            "Topic Content: I have a problem with emojis “poisoning” tokenizers and I wonder if there is an existing solution to deal with this problem. So far, I was not able to find it by doing different web searches. I have some ideas how to solve it by myself but I’d like to know if somebody already dealt with it.\n",
            "The problem is the following: is an emoji is not separated from a word, then the whole word is marked as UNK token. Example:\n",
            "from transformers import AutoTokenizer\n",
            "tokenizer = AutoTokenizer.from_pretrained(“sentence-transformers/distiluse-base-multilingual-cased-v1”)\n",
            "text=“sky”\n",
            "print(tokenizer(text).input_ids)\n",
            "text=“sky🙂”\n",
            "print(tokenizer(text).input_ids)\n",
            "text=“sky”\n",
            "print(tokenizer(text).input_ids)\n",
            "Output:\n",
            "[101, 62368, 102]\n",
            "[101, 100, 102]\n",
            "[101, 100, 102]\n",
            "Topic Timestamp: Jun 17, 2024 2:09 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer\n",
            "Link: https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435\n",
            "Replies: 0\n",
            "Views: 94\n",
            "Activity: Created: Jun 11, 2024 2:44 am\n",
            "Topic Owner: raptorkwok\n",
            "Topic Content: I’m using Seq2SeqTrainer to train my model with a custom tokenizer. The base model is BART Chinese (fnlp/bart-base-chinese). If the original tokenizer of BART Chinese is used, the output is normal. Yet when I swap the tokenizer with another tokenizer that I made, the output of compute_metrics, specifically the preds part of EvalPrediction is incorrect (the decoded text becomes garbage).\n",
            "The codes are as follows:\n",
            "model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
            "model.resize_token_embeddings(len(tokenizer))\n",
            "model.config.vocab_size = len(tokenizer)\n",
            "\n",
            "steps = 500 # small value for debug purpose\n",
            "batch_size = 4\n",
            "training_args = CustomSeq2SeqTrainingArguments(\n",
            "    output_dir = \"my_output_dir\",\n",
            "    evaluation_strategy = IntervalStrategy.STEPS,\n",
            "    optim = \"adamw_torch\",\n",
            "    eval_steps = steps,\n",
            "    logging_steps = steps,\n",
            "    save_steps = steps,\n",
            "    learning_rate = 2e-5,\n",
            "    per_device_train_batch_size = batch_size,\n",
            "    per_device_eval_batch_size = batch_size,\n",
            "    weight_decay = 0.01,\n",
            "    save_total_limit = 1,\n",
            "    num_train_epochs = 30,\n",
            "    predict_with_generate = True,\n",
            "    remove_unused_columns = False, \n",
            "    fp16 = True, # save memory\n",
            "    metric_for_best_model = \"bleu\",\n",
            "    load_best_model_at_end = True,\n",
            "    report_to = \"wandb\",\n",
            "    # HuggingFace Hub related\n",
            "    hub_token = hf_token,\n",
            "    push_to_hub = True,\n",
            "    save_safetensors = True,\n",
            ")\n",
            "\n",
            "trainer = Seq2SeqTrainer(\n",
            "    model = model,\n",
            "    args = training_args,\n",
            "    train_dataset = tokenized_train_dataset,\n",
            "    eval_dataset = tokenized_eval_dataset,\n",
            "    tokenizer = tokenizer,\n",
            "    data_collator = data_collator,\n",
            "    compute_metrics = compute_metrics,\n",
            "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)],\n",
            ")\n",
            "which the tokenizer is my custom tokenizer. The result is normal if my tokenizer uses the original tokenizer (tokenizer = BertTokenizer.from_pretrained(checkpoint)).\n",
            "For the compute_metrics, it is as follows:\n",
            "def postprocess_text(preds, labels):\n",
            "    preds = [pred.strip() for pred in preds]\n",
            "    labels = [[label.strip()] for label in labels]\n",
            "\n",
            "    return preds, labels\n",
            "\n",
            "def compute_metrics(eval_preds):\n",
            "    preds, labels = eval_preds\n",
            "\n",
            "    print(\"Preds and Labels:\", preds[0], labels[0])\n",
            "    \n",
            "    if isinstance(preds, tuple):\n",
            "        preds = preds[0]\n",
            "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
            "\n",
            "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
            "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
            "\n",
            "    print(\"Decoded Preds (before postprocess):\", decoded_preds[0])\n",
            "    print(\"Decoded Labels (before postprocess):\", decoded_labels[0])\n",
            "\n",
            "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
            "    print(\"Decoded Preds:\", decoded_preds[0])\n",
            "    print(\"Decoded Labels:\", decoded_labels[0])\n",
            "\n",
            "    result_bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels, tokenize='zh')\n",
            "    result_chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)\n",
            "    results = {\"bleu\": result_bleu[\"score\"], \"chrf\": result_chrf[\"score\"]}\n",
            "\n",
            "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
            "    results[\"gen_len\"] = np.mean(prediction_lens)\n",
            "    results = {k: round(v, 4) for k, v in results.items()}\n",
            "    return results\n",
            "From the debug message, the output sentence does not make sense and consists of weird characters only. I think the model does not recognize the token IDs produced by my custom tokenizer.\n",
            "How should I tackle this problem? My goal is to train the model with my custom tokenizer.\n",
            "Topic Timestamp: Jun 11, 2024 2:44 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer method inference\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-method-inference/114974\n",
            "Replies: 3\n",
            "Views: 27\n",
            "Activity: Created: Nov 1, 2024 6:31 am\n",
            "Latest: Nov 2, 2024 6:32 am\n",
            "Topic Owner: jinoooooooooo\n",
            "Topic Content: This is a beginner question but\n",
            "How does huggingface infer what type of method to use for tokenizing the sentence? Can you point me where the implementation could be?\n",
            "Thanks\n",
            "Topic Timestamp: Nov 1, 2024 6:31 am\n",
            "Replies:\n",
            "  - Username: John6666\n",
            "    Answer: Maybe simply written in a json file in the same folder.\n",
            "huggingface.co\n",
            "tokenizer_config.json · unsloth/Llama-3.2-1B-Instruct at main\n",
            "We’re on a journey to advance and democratize artificial intelligence through open source and open science.\n",
            "    Timestamp: Nov 2, 2024 1:39 am\n",
            "  - Username: jinoooooooooo\n",
            "    Answer: not really, i couldnt find it\n",
            "    Timestamp: Nov 2, 2024 6:14 am\n",
            "  - Username: John6666\n",
            "    Answer: Oh, you’re looking for code. The following is called based on the YAML set in the model repo and the json as above. in Llama’s case, this is it.\n",
            "github.com\n",
            "huggingface/transformers/blob/main/src/transformers/models/llama/tokenization_llama.py\n",
            "# coding=utf-8\n",
            "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n",
            "#\n",
            "# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n",
            "# and OPT implementations in this library. It has been modified from its\n",
            "# original forms to accommodate minor architectural differences compared\n",
            "# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "This file has been truncated. show original\n",
            "    Timestamp: Nov 2, 2024 6:32 am\n",
            "----------------------------------------\n",
            "Title: Adding special tokens to LEDTokenizer\n",
            "Link: https://discuss.huggingface.co/t/adding-special-tokens-to-ledtokenizer/132987\n",
            "Replies: 0\n",
            "Views: 17\n",
            "Activity: Created: Dec 25, 2024 9:10 am\n",
            "Latest: Dec 25, 2024 11:34 am\n",
            "Topic Owner: KeertiPrem\n",
            "Topic Content: Context\n",
            "I need to add 3 special section tokens to LED-base-16384 model vocabulary. (RoBERTa/LED uses ByteLevelBPE (Byte-Pair Encoding) and then finetune on a custom dataset.\n",
            "Current vocab size: 0 to 50264 i.e. 50265.\n",
            "Current Understanding\n",
            "Placeholder tokens exist:\n",
            "madeupword0000 (50261)\n",
            "madeupword0001 (50262)\n",
            "madeupword0002 (50263)\n",
            "Problem with mask token id in RoBERTa vocab · Issue #1091 · huggingface/transformers · GitHub\n",
            "Temporary fix for RoBERTa's mismatch of vocab size and embedding size - issue #1091 by amirsaffari · Pull Request #1096 · huggingface/transformers · GitHub\n",
            "Two Possible Approaches:\n",
            "A. Replace placeholder tokens (50261-50263)\n",
            "Maintains multiple of 8 (as mentioned in the above links)\n",
            "Uncertain about impact on model\n",
            "B. Add after (50265-50267)\n",
            "Safer but loses multiple of 8\n",
            "New size would be 50268\n",
            "Questions\n",
            "Which approach is recommended?\n",
            "What are the performance implications of:\n",
            "Replacing placeholder tokens\n",
            "Losing multiple-of-8 alignment\n",
            "Are there any documented cases of successfully replacing placeholder tokens?\n",
            "Will either approach affect the model’s learned patterns?\n",
            "Topic Timestamp: Dec 25, 2024 9:10 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer splits words with accents into separate subwords\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088\n",
            "Replies: 0\n",
            "Views: 78\n",
            "Activity: Created: Jun 20, 2024 1:26 am\n",
            "Topic Owner: leestevennz\n",
            "Topic Content: Hi there,\n",
            "My aim is to finetune an existing pretrained LLM on a new language. My new language contains vowels with the following accents: [‘Ā’, ‘ā’, ‘Ē’, ‘ē’, ‘Ī’, ‘ī’, ‘Ō’, ‘ō’, ‘Ū’, ‘ū’].\n",
            "I first train a new tokenizer on my target language. This tokenizer performs well on my target language. Common words that include accents are split into a new token.\n",
            "new_tokenizer = tokenizer.train_new_from_iterator(training_corpus,                                                            new_vocab_size)\n",
            "I then add the vocab from the the new tokenizer to the pretrained tokenizer:\n",
            "new_vocab = list(new_tokenizer.vocab.keys()) tokenizer.add_tokens(new_vocab)\n",
            "My issue is that when I use the tokenizer with the added tokens, it ignores the added tokens and always split accented vowels into a seperate token.\n",
            "Does anyone know how to solve this issue?\n",
            "Topic Timestamp: Jun 20, 2024 1:26 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Byte Level Tokenizer While Training\n",
            "Link: https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009\n",
            "Replies: 0\n",
            "Views: 16\n",
            "Activity: Created: Dec 14, 2024 1:27 am\n",
            "Latest: Dec 14, 2024 2:19 am\n",
            "Topic Owner: dinesh-bk\n",
            "Topic Content: Hi, I have trained the tokenizer using the model BPE and pre tokenizer as ByteLevel tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\")) tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel() tokenizer.decoder = decoders.ByteLevel() \n",
            "Now, my vocabulary is saved in bytes and tokenizer.tokenize give me output in bytes too which is obvious.\n",
            "tokenizer.tokenize output is\n",
            "['Ġà¤¨',\n",
            " 'à¥ĩ',\n",
            " 'à¤ª',\n",
            " 'à¤¾',\n",
            " 'à¤²',\n",
            " 'à¥Ģ',\n",
            " 'Ġà¤Ń',\n",
            " 'à¤¾',\n",
            " 'à¤·',\n",
            " 'à¤¾',\n",
            " 'à¤®',\n",
            " 'à¤¾',\n",
            " 'Ġà¤¯',\n",
            " 'à¥ĭ',\n",
            " 'Ġà¤ıà¤ķ',\n",
            " 'Ġà¤īà¤¦',\n",
            " 'à¤¾',\n",
            " 'à¤¹à¤°à¤£',\n",
            " 'Ġà¤¹',\n",
            " 'à¥ĭà¥¤']\n",
            ". Is there way to save my vocabulary in unicode character rather than bytes and show tokens in unicode characters too?\n",
            "Topic Timestamp: Dec 14, 2024 1:27 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: When I using the chat_template of llama 2 tokenizer the response of IT model is nothing\n",
            "Link: https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098\n",
            "Replies: 0\n",
            "Views: 76\n",
            "Activity: Created: Jul 13, 2024 3:52 am\n",
            "Latest: Jul 13, 2024 4:01 am\n",
            "Topic Owner: thaisonatk\n",
            "Topic Content: My model was instruction data follow the template blow, I can try to generate some answer but the answer always return repeated of question without answer.\n",
            "There is my code :\n",
            "import torch\n",
            "\n",
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "# from peft import PeftModel, PeftConfig\n",
            "\n",
            "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"/data/tqlong/project_gamma/Son/mem/checkpoint-9600\", trust_remote_code=True)\n",
            "\n",
            "model = AutoModelForCausalLM.from_pretrained(\"/data/tqlong/project_gamma/Son/mem/checkpoint-9600\", trust_remote_code=True).to(device)\n",
            "\n",
            "EOS_TOKEN = tokenizer.eos_token\n",
            "def format_prompt(prompt):\n",
            "    text = [\n",
            "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
            "        {\"role\": \"user\", \"content\": prompt},\n",
            "    ]\n",
            "    mess = tokenizer.apply_chat_template(text, tokenize=False) + EOS_TOKEN\n",
            "    return mess\n",
            "\n",
            "def generate(prompt, tokenizer, max_new_tokens):\n",
            "    text = format_prompt(prompt)\n",
            "    text += \"\\n<|im_start|>assistant\\n\"\n",
            "    input_ids = tokenizer(text, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
            "    outputs = model.generate(input_ids=input_ids, max_new_tokens=max_new_tokens,\n",
            "                             do_sample=False,pad_token_id=tokenizer.pad_token_id,\n",
            "                             forced_eos_token_id=tokenizer.eos_token_id)\n",
            "    outputs = outputs[0].tolist()\n",
            "    # Stop decoding when hitting the EOS token\n",
            "    return tokenizer.decode(outputs)\n",
            "    \n",
            "text = \"\"\"Who is Bill Gates ?\"\"\"\n",
            "print(generate(text, tokenizer, 256))\n",
            "The model is ELM and the tokenizer is llama-2\n",
            "And the answer is :\n",
            "<s><s> [INST] <<SYS>>\n",
            "You are a helpful assistant.\n",
            "<</SYS>>\n",
            "\n",
            "Who is Bill Gates [/INST]</s> \n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start|>assistant\n",
            "<|im_end|>\n",
            "\n",
            "You are a helpful assistant.\n",
            "<|im_start</s>\n",
            "I don’t know how to sovle my problem. Please help me!!\n",
            "Thank you\n",
            "Topic Timestamp: Jul 13, 2024 3:52 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Update encode function slowTokenizer vs FastTokenizer\n",
            "Link: https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946\n",
            "Replies: 0\n",
            "Views: 47\n",
            "Activity: Created: Jul 12, 2024 8:04 am\n",
            "Topic Owner: hathyzazda\n",
            "Topic Content: Hello,\n",
            "I am trying to build a personalized tokenizer on top on GPT2 Tokenizer, so that’s why I am modifying the code. I want to build encode words following a specific manner.\n",
            "The code for the slow tokenizer is here:\n",
            "github.com\n",
            "huggingface/transformers/blob/a695c18649fc6ab4b1fb1d9c8cfa9258c5908e2a/src/transformers/models/gpt2/tokenization_gpt2.py\n",
            "# coding=utf-8\n",
            "# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n",
            "\n",
            "import json\n",
            "import os\n",
            "from functools import lru_cache\n",
            "from typing import List, Optional, Tuple\n",
            "This file has been truncated. show original\n",
            "The code for the fast tokenizer is here:\n",
            "github.com\n",
            "huggingface/transformers/blob/a695c18649fc6ab4b1fb1d9c8cfa9258c5908e2a/src/transformers/models/gpt2/tokenization_gpt2_fast.py\n",
            "# coding=utf-8\n",
            "# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n",
            "#\n",
            "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
            "# you may not use this file except in compliance with the License.\n",
            "# You may obtain a copy of the License at\n",
            "#\n",
            "#     http://www.apache.org/licenses/LICENSE-2.0\n",
            "#\n",
            "# Unless required by applicable law or agreed to in writing, software\n",
            "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
            "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
            "# See the License for the specific language governing permissions and\n",
            "# limitations under the License.\n",
            "\"\"\"Tokenization classes for OpenAI GPT.\"\"\"\n",
            "\n",
            "import json\n",
            "from typing import Optional, Tuple\n",
            "\n",
            "from tokenizers import pre_tokenizers\n",
            "This file has been truncated. show original\n",
            "To do pre-tokenization for the slowTokenizer is use “_tokenize()” method.\n",
            "My question is how to do the same for the fast tokenizer?\n",
            "Topic Timestamp: Jul 12, 2024 8:04 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Errors with Tokenizers on Llama\n",
            "Link: https://discuss.huggingface.co/t/errors-with-tokenizers-on-llama/132264\n",
            "Replies: 1\n",
            "Views: 20\n",
            "Activity: Created: Dec 19, 2024 11:52 am\n",
            "Latest: Dec 19, 2024 1:19 pm\n",
            "Topic Owner: dristi\n",
            "Topic Content: I tried downloading llama3 on my server and train. I downloaded it from meta and I tried to convert the raw meta data. I have protobuf and sentencepiece downloaded still it says I have to download it.\n",
            "ValueError: Failed to instantiate tokenizer. Please, make sure you have sentencepiece and protobuf installed.\n",
            "Topic Timestamp: Dec 19, 2024 11:52 am\n",
            "Replies:\n",
            "  - Username: John6666\n",
            "    Answer: Assuming that PyTorch is installed. This should be fine, but there is a possibility that the actual error is occurring elsewhere.\n",
            "pip install protobuf sentencepiece transformers\n",
            "    Timestamp: Dec 19, 2024 1:19 pm\n",
            "----------------------------------------\n",
            "Title: HugginChat (Android App)\n",
            "Link: https://discuss.huggingface.co/t/hugginchat-android-app/101530\n",
            "Replies: 0\n",
            "Views: 35\n",
            "Activity: Created: Aug 8, 2024 3:19 pm\n",
            "Topic Owner: gabrielparca\n",
            "Topic Content: I get this 403 error when attempting logging in:\n",
            "Invalid or expired CSRF token\n",
            "HuggingChat v.1.2 (Play Store)\n",
            "Samsung SM-A346E, Android 14, ONE UI 6.1\n",
            "Topic Timestamp: Aug 8, 2024 3:19 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer\n",
            "Link: https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499\n",
            "Replies: 0\n",
            "Views: 31\n",
            "Activity: Created: Aug 29, 2024 12:58 pm\n",
            "Latest: Aug 29, 2024 1:01 pm\n",
            "Topic Owner: MilosKovacevic68\n",
            "Topic Content: I have a problem when training a new tokenizer for Gemma 2 2B or Phi 3 and 3.5 models using the following code:\n",
            "def corpus_gen(dataset, batch_size=300, n=300_000):\n",
            "    current = []\n",
            "    tot = 0\n",
            "    for ex in dataset:\n",
            "        current.append(ex['txt'])\n",
            "        tot += 1\n",
            "        if tot == n: break\n",
            "        if len(current) == batch_size:\n",
            "            yield current\n",
            "            current = []\n",
            "    if current:\n",
            "        yield current\n",
            "\n",
            "def train_tokenizer():\n",
            "    dataset = load_dataset(\n",
            "        \"json\", \n",
            "        split=\"train\",\n",
            "        streaming=True,\n",
            "        data_files=[\n",
            "            \"../serlama/tokenizer/paragraphs_tokenizer.jsonl\",\n",
            "            \"../serlama/tokenizer/pdrs_tokenizer.jsonl\",\n",
            "            \"../serlama/tokenizer/macocu_tokenizer.jsonl\",\n",
            "    ])\n",
            "\n",
            "    existing_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\") \n",
            "    \n",
            "    new_tokenizer = existing_tokenizer.train_new_from_iterator(\n",
            "        corpus_gen(dataset), \n",
            "        vocab_size=30000, \n",
            "        min_frequency=3\n",
            "    )\n",
            "    new_tokenizer.save_pretrained(\"sr_tokenizer\")\n",
            "\n",
            "train_tokenizer()\n",
            "After n= 100 000 (examples) my RAM steadily increases in blocks of few gigabytes and i cannot train the tokenizer.\n",
            "When i try the same code with Llama 3.1 tokenizer everything is ok and the RAM does not increase.\n",
            "My transformers version is 4.44.0\n",
            "Why is that?\n",
            "What is the problem with Gemma 2 2B and Phi3 tokenizers. Do they have a memory leak problems?\n",
            "Topic Timestamp: Aug 29, 2024 12:58 pm\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenizer Error [AGAIN!]\n",
            "Link: https://discuss.huggingface.co/t/tokenizer-error-again/106104\n",
            "Replies: 0\n",
            "Views: 32\n",
            "Activity: Created: Sep 10, 2024 5:10 am\n",
            "Topic Owner: gorski\n",
            "Topic Content: When running lucataco/flux-dev-multi-lora – Run with an API on Replicate, I’m getting the following error:\n",
            "gorski/flux-avatar-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models' If this is a private repository, make sure to pass a token having permission to this repo with tokenor log in withhuggingface-cli login.\n",
            "Based on my searches, it looks like Hugging Faces tokenizer error that keeps returning since 2023. Does anyone have any suggestions?\n",
            "Topic Timestamp: Sep 10, 2024 5:10 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Tokenization compared to sentencepiece\n",
            "Link: https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278\n",
            "Replies: 0\n",
            "Views: 30\n",
            "Activity: Created: Sep 11, 2024 8:43 am\n",
            "Topic Owner: marcin-ochman\n",
            "Topic Content: Hi!\n",
            "Recently I tried to use microsoft/Phi-3-mini-4k-instruct tokenizer. I’ve found that results of the transformers tokenizer is different than the output of sentencepiece tokenizer. To give an example, here’s the sample code reproducing that issue:\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
            "formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
            "# '<|user|>\\nWhat is capital city of France?<|end|>\\n<|assistant|>\\n'\n",
            "tokenizer(formatted_text, return_tensors=\"pt\")\n",
            "# {'input_ids': tensor([[32010,  1724,   338,  7483,  4272,   310,  3444, 29973, 32007, 32001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
            "\n",
            "sp = spm.SentencePieceProcessor(model_file=\"...\")\n",
            "sp.encode(formatted_text)\n",
            "# [32010, 13, 5618, 338, 7483, 4272, 310, 3444, 29973, 32007, 13, 32001, 13]\n",
            "Do you know why transformers modify the original text? Is this correct behaviour? It removes newlines and add spaces (13 is a newline character, while 5618 → “What” and 1724 → “_What”.\n",
            "Topic Timestamp: Sep 11, 2024 8:43 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: What does “trim_offsets” do in tokenizer post-processor?\n",
            "Link: https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849\n",
            "Replies: 0\n",
            "Views: 21\n",
            "Activity: Created: Aug 25, 2024 2:32 am\n",
            "Topic Owner: cloudydory\n",
            "Topic Content: In the documentation of “Post-processers” in Huggingface’s tokenizer library, many post-processors has a argument “trim_offset”. And the explanation is like:\n",
            "It also takes care of trimming the offsets. By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don’t want the offsets to include these whitespaces, then this PostProcessor should be initialized with trim_offsets=True\n",
            "I am still confused by the explanation. Specifically:\n",
            "Why does the “the ByteLevel BPE might include whitespaces in the produced tokens”? Is it specific to Byte-Level BPE or also happens in BPE?\n",
            "What does the argument “trim_offsets” actually do? Why do we “don’t want the offsets to include these whitespaces”?\n",
            "Topic Timestamp: Aug 25, 2024 2:32 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences?\n",
            "Link: https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442\n",
            "Replies: 0\n",
            "Views: 19\n",
            "Activity: Created: Jul 20, 2024 5:00 am\n",
            "Topic Owner: ourdearbenefactor\n",
            "Topic Content: I am fine tuning a T5 model to translate between a custom pair of languages (Classical Nahuatl and English); the T5 was not trained on Classical Nahuatl, of course. I trained my own tokenizers for both the input language (Classical Nahuatl) and the target language (English).\n",
            "I’m following the tutorial for translation. From my own implementation, it seems that the target texts are tokenized with the input language’s tokenizer in the following line of code: model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) in the preprocess_function function. That is, the target language’s tokenizer is not used at all, and only the input language’s tokenizer is used for both the input and target sentences.\n",
            "How do I apply the target language’s tokenizer on the target sentences? Will I need to tokenize the target sentences before implementing the code above?\n",
            "Topic Timestamp: Jul 20, 2024 5:00 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Decoding sequence of tokens produces question marks instead of actual tokens\n",
            "Link: https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087\n",
            "Replies: 1\n",
            "Views: 12\n",
            "Activity: Created: Sep 3, 2024 10:27 am\n",
            "Latest: Sep 3, 2024 10:28 am\n",
            "Topic Owner: souryadey\n",
            "Topic Content: I am working with the mistralai/Mistral-7B-v0.1 model. I loaded the tokenizer via:\n",
            "tokenizer = transformers.AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1')\n",
            "then ran the following code:\n",
            "tokenizer.decode([69])\n",
            "Produces output B.\n",
            "tokenizer.decode([198])\n",
            "Produces output �. This is perhaps understandable since token ID 198 corresponds to token <0xC3>, which is the hex ASCII code for Ã and may be unprint-able.\n",
            "But then:\n",
            "tokenizer.decode([69,198])\n",
            "Produces output ��. I don’t know why it’s producing this instead of B�.\n",
            "Any help will be appreciated!\n",
            "Topic Timestamp: Sep 3, 2024 10:27 am\n",
            "Replies:\n",
            "  - Username: souryadey\n",
            "    Answer: Additional details: My Python environment has tokenizers==0.19.1 and transformers==4.44.2.\n",
            "    Timestamp: Sep 3, 2024 10:28 am\n",
            "----------------------------------------\n",
            "Title: What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer\n",
            "Link: https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443\n",
            "Replies: 0\n",
            "Views: 16\n",
            "Activity: Created: Sep 12, 2024 9:37 am\n",
            "Topic Owner: JoonseoHyeon\n",
            "Topic Content: In the offcial document of XLM-RoBERTa:>\n",
            "Adapted from RobertaTokenizer and XLNetTokenizer. Based on SentencePiece.\n",
            "And each RobertaTokenizer and XLNetTokenizer are descripted below:\n",
            "Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding.\n",
            "Construct an XLNet tokenizer. Based on SentencePiece.\n",
            "And the description in article XLM-RoBERTa about tokenization is like below:\n",
            "The different language-specific tokenization tools used by mBERT and XLM-100\n",
            "make these models more difficult to use on raw text. Instead, we train a Sentence Piece model (SPM) and apply it directly on raw text data for all languages. We did not observe any loss in performance for models trained with SPM when compared to models trained with language-specific preprocessing and byte-pair encoding and hence use SPM for XLM-R.\n",
            "Conneau, A. “Unsupervised cross-lingual representation learning at scale.” arXiv preprint arXiv:1911.02116 (2019).\n",
            "I’m confused what is real XLM-RoBERTa Tokenizer builted. SentencePiece Tokenizer? XLNet Tokenizer? Or am I wrong?\n",
            "Topic Timestamp: Sep 12, 2024 9:37 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model?\n",
            "Link: https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333\n",
            "Replies: 0\n",
            "Views: 14\n",
            "Activity: Created: Aug 14, 2024 11:32 am\n",
            "Topic Owner: sidharthsajith7\n",
            "Topic Content: Hi everyone,\n",
            "I’m trying to retrain the GPT-2 tokeniser on a vast amount of Chinese data to adapt it to the Chinese language. My goal is to use this retrained tokeniser with a pre-trained GPT-2 XL or other models to create a Chinese-speaking model.\n",
            "My question is: will this approach work? Can I simply retrain the GPT-2 tokeniser on Chinese data and then use it with a pre-trained GPT-2 XL or other models to create a Chinese-speaking model? Or are there additional steps I need to take to ensure the model can understand and generate Chinese text?\n",
            "I’ve tried searching for similar questions and tutorials, but I couldn’t find any clear guidance on this specific topic. I’d appreciate any insights or advice from the community on whether this approach is feasible and what additional steps I might need to take to make it work.\n",
            "Some specific questions I have are:\n",
            "Will the retrained tokeniser be compatible with the pre-trained GPT-2 XL or other models?\n",
            "Are there any specific preprocessing steps I need to take when working with Chinese text?\n",
            "Are there any known issues or limitations when using a retrained tokeniser with pre-trained models?\n",
            "Any help or guidance would be greatly appreciated!\n",
            "Topic Timestamp: Aug 14, 2024 11:32 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Help to choose decoder for devnagari ocr\n",
            "Link: https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442\n",
            "Replies: 0\n",
            "Views: 13\n",
            "Activity: Created: Sep 20, 2024 5:13 am\n",
            "Topic Owner: nhujaw07\n",
            "Topic Content: I am trying to train Devnagari handwritten OCR but i am unable to get proper txt output. Can some one suggest which encoder and decoder should I use?\n",
            "Topic Timestamp: Sep 20, 2024 5:13 am\n",
            "Replies:\n",
            "----------------------------------------\n",
            "Title: Call rust function in python\n",
            "Link: https://discuss.huggingface.co/t/call-rust-function-in-python/103446\n",
            "Replies: 0\n",
            "Views: 8\n",
            "Activity: Created: Aug 22, 2024 6:01 am\n",
            "Topic Owner: Boltzmachine\n",
            "Topic Content: I want to write a rust function that takes the python tokenizer (the tokenizer might be set in python code). for example,\n",
            "define this in rust\n",
            "fn foo(tokenizer: PyTokenizer, ...) { ... }\n",
            "and call it in python\n",
            "tokenizer = AutoTokenizer.from_pretrained(...)\n",
            "# set tokenizer, e.g. pad_id\n",
            "foo(tokenizer, ...)\n",
            "Is there any guide to achieve this\n",
            "Topic Timestamp: Aug 22, 2024 6:01 am\n",
            "Replies:\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import csv\n",
        "import time\n",
        "\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "chrome_options.add_argument(\"--disable-gpu\")\n",
        "\n",
        "chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "chrome_options.add_argument(\"--disable-infobars\")\n",
        "chrome_options.add_argument(\"--disable-popup-blocking\")\n",
        "chrome_options.add_argument(\"--ignore-certificate-errors\")\n",
        "chrome_options.add_argument(\"--incognito\")\n",
        "\n",
        "chrome_path = \"/usr/bin/chromedriver\"\n",
        "\n",
        "driver = gs.Chrome(options=chrome_options)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "url = \"https://discuss.huggingface.co/c/tokenizers/11/l/top\"\n",
        "driver.get(url)\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "def scrape_titles_links_and_stats():\n",
        "    titles_links_stats = []\n",
        "    seen_titles_links = set()\n",
        "\n",
        "    while True:\n",
        "        # all topic rows\n",
        "        topic_rows = driver.find_elements(By.CSS_SELECTOR, \"tr.topic-list-item\")\n",
        "\n",
        "        for row in topic_rows:\n",
        "            try:\n",
        "                # title and link\n",
        "                title_elem = row.find_element(By.CSS_SELECTOR, \"span.link-top-line a.title.raw-link.raw-topic-link\")\n",
        "                topic_title = title_elem.text.strip()\n",
        "                topic_link = title_elem.get_attribute('href')\n",
        "\n",
        "                # replies\n",
        "                replies_elem = row.find_element(By.CSS_SELECTOR, \"td.num.posts span\")\n",
        "                replies = int(replies_elem.text.strip())\n",
        "\n",
        "                # views\n",
        "                views_elem = row.find_element(By.CSS_SELECTOR, \"td.num.views span\")\n",
        "                views = views_elem.text.strip()\n",
        "\n",
        "                # activity\n",
        "                activity_elem = row.find_element(By.CSS_SELECTOR, \"td.activity\")\n",
        "                activity = activity_elem.get_attribute(\"title\").strip()\n",
        "\n",
        "                # add to list if not already seen\n",
        "                if (topic_title, topic_link) not in seen_titles_links:\n",
        "                    titles_links_stats.append({\n",
        "                        \"topic_title\": topic_title,\n",
        "                        \"topic_link\": topic_link,\n",
        "                        \"replies\": replies,\n",
        "                        \"views\": views,\n",
        "                        \"activity\": activity\n",
        "                    })\n",
        "                    seen_titles_links.add((topic_title, topic_link))\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping a topic row: {e}\")\n",
        "\n",
        "        # scroll down to load more content\n",
        "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.END)\n",
        "        time.sleep(3)\n",
        "\n",
        "        # stop if no new topics are loaded\n",
        "        if len(topic_rows) == len(driver.find_elements(By.CSS_SELECTOR, \"tr.topic-list-item\")):\n",
        "            break\n",
        "\n",
        "    return titles_links_stats\n",
        "\n",
        "def get_topic_details(topic_link):\n",
        "    driver.get(topic_link)\n",
        "    try:\n",
        "        # topic_owner\n",
        "        owner_elem = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"span.first.username a\"))\n",
        "        )\n",
        "        topic_owner = owner_elem.text.strip()\n",
        "\n",
        "        # topic_content\n",
        "        content_elem = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.topic-post.clearfix div.topic-body.clearfix div.cooked\"))\n",
        "        )\n",
        "        topic_content = content_elem.text.strip()\n",
        "\n",
        "        # timestamp\n",
        "        post_date_elem = WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.post-info.post-date a span[title]\"))\n",
        "        )\n",
        "        timestamp = post_date_elem.get_attribute(\"title\").strip()\n",
        "\n",
        "        # fetch replies\n",
        "        replies = get_replies_details()\n",
        "\n",
        "        return {\n",
        "            \"topic_owner\": topic_owner,\n",
        "            \"topic_content\": topic_content,\n",
        "            \"timestamp\": timestamp,\n",
        "            \"replies\": replies\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping topic details for {topic_link}: {e}\")\n",
        "        return {\n",
        "            \"topic_owner\": None,\n",
        "            \"topic_content\": None,\n",
        "            \"timestamp\": None,\n",
        "            \"replies\": []\n",
        "        }\n",
        "\n",
        "def get_replies_details():\n",
        "    replies = []\n",
        "    try:\n",
        "        # all reply elements\n",
        "        reply_elements = driver.find_elements(By.CSS_SELECTOR, \"div.topic-body.clearfix\")\n",
        "\n",
        "        for reply in reply_elements[1:]:\n",
        "            try:\n",
        "                # username_topic_answer\n",
        "                username_elem = reply.find_element(By.CSS_SELECTOR, \"div.names.trigger-user-card span.first.username\")\n",
        "                username_topic_answer = username_elem.text.strip()\n",
        "\n",
        "                # topic_answer\n",
        "                answer_elem = reply.find_element(By.CSS_SELECTOR, \"div.regular.contents div.cooked\")\n",
        "                topic_answer = answer_elem.text.strip()\n",
        "\n",
        "                # timestamp_topic_answer\n",
        "                timestamp_span = reply.find_element(By.CSS_SELECTOR, \"div.post-info.post-date a.widget-link.post-date span[title]\")\n",
        "                timestamp_topic_answer = timestamp_span.get_attribute(\"title\").strip()\n",
        "\n",
        "                replies.append({\n",
        "                    \"username_topic_answer\": username_topic_answer,\n",
        "                    \"topic_answer\": topic_answer,\n",
        "                    \"timestamp_topic_answer\": timestamp_topic_answer\n",
        "                })\n",
        "            except Exception as e:\n",
        "                print(f\"Error scraping a reply: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding replies: {e}\")\n",
        "\n",
        "    return replies\n",
        "\n",
        "def save_to_csv(data, filename=\"scraped_data.csv\"):\n",
        "    with open(filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "        # max_replies based on the 'replies' field\n",
        "        max_replies = max(topic[\"replies\"] for topic in data)\n",
        "\n",
        "        headers = [\n",
        "            \"topic_title\", \"topic_link\", \"replies\", \"views\",\n",
        "            \"activity\", \"topic_owner\", \"topic_content\", \"timestamp\"\n",
        "        ]\n",
        "        for i in range(1, max_replies + 1):\n",
        "            headers.extend([\n",
        "                f\"username_topic_answer_{i}\",\n",
        "                f\"topic_answer_{i}\",\n",
        "                f\"timestamp_topic_answer_{i}\"\n",
        "            ])\n",
        "        writer.writerow(headers)\n",
        "\n",
        "        for topic in data:\n",
        "            row = [\n",
        "                topic[\"topic_title\"],\n",
        "                topic[\"topic_link\"],\n",
        "                topic[\"replies\"],\n",
        "                topic[\"views\"],\n",
        "                topic[\"activity\"].replace(\"\\n\", \" \"),\n",
        "                topic[\"details\"][\"topic_owner\"],\n",
        "                topic[\"details\"][\"topic_content\"].replace(\"\\n\", \" \"),\n",
        "                topic[\"details\"][\"timestamp\"].replace(\"\\n\", \" \"),\n",
        "            ]\n",
        "\n",
        "            for reply in topic[\"details\"][\"replies\"]:\n",
        "                row.extend([\n",
        "                    reply[\"username_topic_answer\"],\n",
        "                    reply[\"topic_answer\"].replace(\"\\n\", \" \"),\n",
        "                    reply[\"timestamp_topic_answer\"]\n",
        "                ])\n",
        "\n",
        "            extra_columns = (max_replies - len(topic[\"details\"][\"replies\"])) * 3\n",
        "            row.extend([\"\"] * extra_columns)\n",
        "\n",
        "            writer.writerow(row)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "titles_links_stats = scrape_titles_links_and_stats()\n",
        "\n",
        "for topic in titles_links_stats:\n",
        "    topic[\"details\"] = get_topic_details(topic[\"topic_link\"])\n",
        "\n",
        "for topic in titles_links_stats:\n",
        "    print(f\"Title: {topic['topic_title']}\")\n",
        "    print(f\"Link: {topic['topic_link']}\")\n",
        "    print(f\"Replies: {topic['replies']}\")\n",
        "    print(f\"Views: {topic['views']}\")\n",
        "    print(f\"Activity: {topic['activity']}\")\n",
        "    print(f\"Topic Owner: {topic['details']['topic_owner']}\")\n",
        "    print(f\"Topic Content: {topic['details']['topic_content']}\")\n",
        "    print(f\"Topic Timestamp: {topic['details']['timestamp']}\")\n",
        "    print(\"Replies:\")\n",
        "    for i, reply in enumerate(topic[\"details\"][\"replies\"], 1):\n",
        "        print(f\"  - Username: {reply['username_topic_answer']}\")\n",
        "        print(f\"    Answer: {reply['topic_answer']}\")\n",
        "        print(f\"    Timestamp: {reply['timestamp_topic_answer']}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "# save the data to CSV\n",
        "save_to_csv(titles_links_stats)\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "g4NHdr13xVvZ",
        "outputId": "f2cb9e02-f780-40c6-ab74-128773444769"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_744b4ecd-a719-41cd-a8fe-318af8b0eeaf\", \"scraped_data.csv\", 896153)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download(\"scraped_data.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
