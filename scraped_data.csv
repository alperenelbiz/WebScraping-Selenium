"topic_title","topic_link","replies","views","activity","topic_owner","topic_content","timestamp","username_topic_answer_1","topic_answer_1","timestamp_topic_answer_1","username_topic_answer_2","topic_answer_2","timestamp_topic_answer_2","username_topic_answer_3","topic_answer_3","timestamp_topic_answer_3","username_topic_answer_4","topic_answer_4","timestamp_topic_answer_4","username_topic_answer_5","topic_answer_5","timestamp_topic_answer_5","username_topic_answer_6","topic_answer_6","timestamp_topic_answer_6","username_topic_answer_7","topic_answer_7","timestamp_topic_answer_7","username_topic_answer_8","topic_answer_8","timestamp_topic_answer_8","username_topic_answer_9","topic_answer_9","timestamp_topic_answer_9","username_topic_answer_10","topic_answer_10","timestamp_topic_answer_10","username_topic_answer_11","topic_answer_11","timestamp_topic_answer_11","username_topic_answer_12","topic_answer_12","timestamp_topic_answer_12","username_topic_answer_13","topic_answer_13","timestamp_topic_answer_13","username_topic_answer_14","topic_answer_14","timestamp_topic_answer_14","username_topic_answer_15","topic_answer_15","timestamp_topic_answer_15","username_topic_answer_16","topic_answer_16","timestamp_topic_answer_16","username_topic_answer_17","topic_answer_17","timestamp_topic_answer_17","username_topic_answer_18","topic_answer_18","timestamp_topic_answer_18","username_topic_answer_19","topic_answer_19","timestamp_topic_answer_19","username_topic_answer_20","topic_answer_20","timestamp_topic_answer_20","username_topic_answer_21","topic_answer_21","timestamp_topic_answer_21","username_topic_answer_22","topic_answer_22","timestamp_topic_answer_22"
"Error with new tokenizers (URGENT!)","https://discuss.huggingface.co/t/error-with-new-tokenizers-urgent/2847","16","49.4k","Created: Dec 16, 2020 9:56 am Latest: Jul 22, 2024 8:16 am","denocris","Hi, recently all my pre-trained models undergo this error while loading their tokenizer: Couldn't instantiate the backend tokenizer from one of: (1) a tokenizers library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one. I tried to pip install sentencepiece but this does not solve the problem. Do you know any solution? (I am working on Google Colab) Note: In my humble opinion, changing so important things so fast can generate very dangerous problems. All my students (I teach DL stuff) and clients are stuck on my notebooks. I can understand that after a year a code can become outdated, but not just after two months. This requires a lot of maintenance work from my side!","Dec 16, 2020 9:56 am","FL33TW00D","There were some breaking changes in the V4 release, please find the details here: GitHub Release Transformers v4.0.0: Fast tokenizers, model outputs, file... Transformers v4.0.0-rc-1: Fast tokenizers, model outputs, file reorganization Breaking changes since v3.x Version v4.0.0 introduces several breaking changes that were necessary. 1. AutoTokenizers a...","Dec 16, 2020 10:17 am","denocris","Thank you so much @FL33TW00D. Two steps were required. latest pip version 20.3.3 (On Colab I had installed by default 19 and something); use_fast = False. Working a lot on Italian or personal custom models, all of them were disabled all of a sudden!","Dec 16, 2020 10:26 am","Ogayo","Hi @denocris I am facing the same problem. I am a student and I depend on this for my presentation tomorrow. I am also using Google Colab. Can you please explain in steps how to fix this sentencpiece problem? Thanks in advance","Dec 16, 2020 12:43 pm","Ogayo","denocris: latest pip version 20.3.3 (On Colab I had installed by default 19 and something); I resolved it. Uninstalled transformers Installed transformers sentencepiece like this : !pip install --no-cache-dir transformers sentencepiece Use_fast= False like this: tokenizer = AutoTokenizer.from_pretrained(‚ÄúXXXXX‚Äù, use_fast=False)","Dec 16, 2020 1:03 pm","denocris","Sorry @Ogayo, I have just read. I am happy you solved it. It is quite annoying to have these kind of issues from one day to another. I had this error during a live presentation. A couple of days before the notebook was working well!","Dec 16, 2020 4:41 pm","kroshan","Thanku @Ogayo","Apr 5, 2022 7:23 pm","evegarcianz","Thank you @Ogayo !!","Feb 15, 2023 8:25 pm","zeelthumar-04","THANKS A LOT @Ogayo","Jun 12, 2023 10:27 am","ashu3984","Thank you very much @Ogayo.","Oct 2, 2023 6:14 pm","VishNikhil","Hi @Ogayo ,im getting the same error after resolving the following steps also what should i do ?","Nov 22, 2023 1:15 pm","borat123","Ogayo: pip install --no-cache-dir transformers Same problem here. Solved it using: pip install sentencepiece","Mar 6, 2024 12:00 pm","Hope2000","I tried using use_fast=False but got the following error: SyntaxError: keyword argument repeated: use_fast I think it‚Äôs deprecated now! but the installation part works, so thank you.","Apr 4, 2024 7:42 am","MahadA","Hello Denocris Hope you are doing well i am getting this error KeyError Traceback (most recent call last) in <cell line: 3>() 1 from transformers import AutoTokenizer, AutoModel 2 ----> 3 tokenizer = AutoTokenizer.from_pretrained(‚Äúmusadac/vilanocr-multi-medical‚Äù, use_fast=False) 1 frames /usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py in getitem(self, key) 740 model_name = self._model_mapping[mtype] 741 return self._load_attr_from_module(mtype, model_name) ‚Üí 742 raise KeyError(key) 743 744 def _load_attr_from_module(self, model_type, attr): KeyError: <class ‚Äòtransformers.models.vision_encoder_decoder.configuration_vision_encoder_decoder.VisionEncoderDecoderConfig‚Äô>","May 15, 2024 9:40 am","joyz29","Ogayo: !pip install --no-cache-dir transformers sentencepiece it works! thankx sooo much","Jul 19, 2024 9:48 am","Kaxder23","Thank you very much for this‚Ä¶","Jul 20, 2024 9:59 am","Kaxder23","Hi, I‚Äôve faced the same issue while integrating it into a calculator-based website. Installing transformers with sentencepiece and restarting the runtime worked for me. Try this: ‚Äúpip install transformers sentencepiece‚Äù","Jul 22, 2024 7:58 am","","","","","","","","","","","","","","","","","",""
"Tokenizer splits up pre-split tokens","https://discuss.huggingface.co/t/tokenizer-splits-up-pre-split-tokens/2078","9","6.2k","Created: Nov 16, 2020 5:59 am Latest: Feb 9, 2024 2:04 pm","facehugger2020","I am working on a Named Entity Recognition (NER) problem, and I need tokenization to be quite precise in order to match tokens with per-token NER tags. I have the following sentence It costs 2.5 million., which I have already tokenized. tokens = ['It', 'costs', '2.5', 'million.'] I then run the list through a BERT tokenizer using the is_split_into_words=True option to get input IDs. When I try to reconstruct the original sentence using the tokenizer, I see that it has split the token 2.5 into the three tokens 2, ., and 5. It also split the token million. into two tokens million and .. tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)  result = tokenizer(tokens, is_split_into_words=True)  print(result.input_ids) # [101, 2009, 5366, 1016, 1012, 1019, 2454, 1012, 102]  print(tokenizer.decode(result.input_ids)) # [CLS] it costs 2. 5 million. [SEP]  print(tokenizer.convert_ids_to_tokens(result.input_ids)) # ['[CLS]', 'it', 'costs', '2', '.', '5', 'million', '.', '[SEP]'] I do not want that additional tokenization. Since I passed is_split_into_words=True to the tokenizer, I was expecting that the tokenizer would treat each token as atomic and not do any further tokenization. I want the original string to be treated as four tokens ['It', 'costs', '2.5', 'million.'] so that the tokens lines up with my NER tags, where 2.5 has an NER tag of number. How would I got about fixing my problem? Thank you.","Nov 16, 2020 5:59 am","rgwatwormhill","Hi facehugger2020, in order to fix your problem, you will need to train a BERT tokenizer yourself, and then train the BERT model too. When a BERT model is created and pre-trained, it uses a particular vocabulary. For example, the standard bert-base-uncased model has a vocabulary of 30000 tokens. ‚Äú2.5‚Äù is not part of that vocabulary, so the BERT tokenizer splits it up into smaller units. Training from scratch with the vocabulary you need is not impossible, but it will be tricky and probably expensive. Could you force your NER tags to fit with the BERT tokenization? For example, could you perform the tagging after the tokenization?","Nov 17, 2020 7:53 pm","facehugger2020","My training data is already pre-tokenized, and there is a 1-to-1 correspondence between token and NER tag. That‚Äôs why I don‚Äôt want the tokens to be split any further. I know that HuggingFace has an NER training example, but it reveals the same problem: tokens are broken down into subword pieces which do not match the NER tags.","Nov 18, 2020 2:53 am","rgwatwormhill","Well, ‚Äò2.5‚Äô is not in BERT‚Äôs vocabulary. Building a tokenizer is an integral part of building a BERT model. The way the model learns depends on the tokens it uses. You have 3 options. Change your pre-processing Create your own BERT with your own vocabulary Write some post-processing to re-align the NER tags Do you have enough data to train a BERT model from scratch? Do you definitely need to keep all your different numbers, or could you substitute a known token for each number before you pass the text to BERT? Chris McCormick has some nice explanations of tokenization, in blogs and you-tube videos. See this blog for example https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/","Nov 18, 2020 10:46 am","facehugger2020","Thank you for your help. I think my only recourse is option (3), writing post-processing code to re-align subword tokens to NER tags. That‚Äôs the approach taken in the Huggingface NER training example, but it‚Äôs a bit inelegant (you need give the NER tag of -100 to subword pieces). I already read all the McCormick tutorials on BERT. Thank you.","Nov 18, 2020 7:10 pm","kloee","Hi I have the same issue, but the problem is not that BERT tokenizer splits some pre-split tokens, the problem is that it doesn‚Äôt show the new split in the output: there‚Äôs no ## at the beginning of ‚Äò.‚Äô and ‚Äò5‚Äô in the given example, thus preventing for a simple post-processing. I don‚Äôt understand this behavior, and how one could expect a correct evaluation when it‚Äôs impossible to retrieve the original tokens ‚Ä¶","Mar 30, 2023 3:44 pm","fredymad","kloee: rect evaluation when it‚Äôs impossible to retrieve the original tokens ‚Ä¶ I am facing the same problem, any solution up to date?","Apr 9, 2023 2:29 pm","kloee","Hi, I found a solution: you can use ‚Äúword_ids‚Äù to realign the new tokens and the original ones. It‚Äôs a field of class transformers.BatchEncoding, that you can retrieve e.g. that way: tokenized_inputs = tokenizer(tokens, truncation=True, is_split_into_words=True) word_ids = tokenized_inputs.word_ids Then you have access to a list of indices that indicate that two tokens come from the same word. My understanding is that BERT-like tokenizers (1) tokenize based on punctuation (I think?) and (2) ‚Äòsub-tokenize‚Äô into subwords. They both make sense, in a way, but part (1) is less documented (especially the fact it happens even with the option saying that it‚Äôs already tokenized ‚Ä¶), there‚Äôs nothing like ## to indicate the split, and I‚Äôve had a hard time understanding what was going on. It seems that ‚Äòword_ids‚Äô only works with ‚ÄúFast‚Äù tokenizers as indicated here: Tokenizer I use it as a post-processing. It still bothers me that the model doesn‚Äôt learn on ‚Äúmy‚Äù input tokenisation, while at the end it is evaluated on it‚Ä¶ I hope it helps!","Apr 9, 2023 7:34 pm","theclueless","Hi, The ‚Äúword_ids‚Äù doesn‚Äôt work 2.5 which is split into 2,., and 5 have different word_ids, hence is no means to identify if they belong to a single entity. I am also in search of a solution.","Nov 23, 2023 5:52 am","giannigi","Did you guys find any solution to this? I‚Äôm trying to ‚Äúrecombine‚Äù the embeddings of split words too.","Feb 9, 2024 2:04 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Trying to use AutoTokenizer with TensorFlow gives: `ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).`","https://discuss.huggingface.co/t/trying-to-use-autotokenizer-with-tensorflow-gives-valueerror-text-input-must-of-type-str-single-example-list-str-batch-or-single-pretokenized-example-or-list-list-str-batch-of-pretokenized-examples/28269","11","17.7k","Created: Dec 22, 2022 7:37 am Latest: Oct 5, 2024 3:48 pm","maifeeulasad","Is there any way, where I can tokenize texts from tf.string? Cause in this way, we can use transformers inside existing TensorFlow models, and it will be a lot faster. This also leads to endless possibilities, as we will be able to use multiple models parallel with concat from TensorFlow Let‚Äôs say I have this piece of code: def get_model():     text_input = Input(shape=(), dtype=tf.string, name='text')          MODEL = ""ping/pong""     tokenizer = AutoTokenizer.from_pretrained(MODEL)     transformer_layer = TFAutoModel.from_pretrained(MODEL)          preprocessed_text = tokenizer(text_input)     outputs = transformer_layer(preprocessed_text)          output_sequence = outputs['sequence_output']     x = Flatten()(output_sequence)     x = Dense(NUM_CLASS,  activation='sigmoid')(x)      model = Model(inputs=[text_input], outputs = [x])     return model But this gives me an error saying: ValueError                                Traceback (most recent call last) /tmp/ipykernel_27/788693747.py in <module>       1 optimizer = Adam() ----> 2 model = get_model()       3 model.compile(loss=CategoricalCrossentropy(from_logits=True),optimizer=optimizer,metrics=[Accuracy(), ],)       4 model.summary()  /tmp/ipykernel_27/330097806.py in get_model()       6        7     text_input = Input(shape=(), dtype=tf.string, name='text') ----> 8     preprocessed_text = tokenizer(text_input)       9     outputs = transformer_layer(preprocessed_text)      10   /opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in __call__(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)    2466         if not _is_valid_text_input(text):    2467             raise ValueError( -> 2468                 ""text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) ""    2469                 ""or `List[List[str]]` (batch of pretokenized examples).""    2470             )  ValueError: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).","Dec 22, 2022 7:37 am","man0j","Do we have a solution for this issue? I‚Äôm facing the same issue.","Feb 12, 2023 1:51 am","AaronJosephMathew","I am facing the same issue, when I am trying to pass pandas column as an object.","Feb 12, 2023 9:34 am","AaronJosephMathew","This might be one way of doing in pandas, couldn‚Äôt convert object type to string and .values didn‚Äôt work the way I wanted. token_val = [str(i) for i in df[‚Äòcol_name‚Äô].values] And feed this into the tokenizer","Feb 12, 2023 9:43 am","luisbnzsa","AaronJosephMathew: [str(i) for i in df[[‚Äòcol_name‚Äô, ‚Äòcol_name2‚Äô]].values] Thanks. It worked for me. Above added if you have more than one column to tokenize","Feb 24, 2023 10:37 am","Prashant08","I was getting the same issue while using finbert but when I read the error carefully I understood what‚Äôs the issue‚Ä¶ The error itself is telling what it‚Äôs expecting i.e string and it is getting stuck because it is not finding string or list[list(str)] so if you‚Äôre applying the function on an excel column just add .dropna() at the end while reading the excel sheet which will drop all empty cells and make sure no of your rows in the column have only ‚Äòintegers‚Äô and not string so that will again throw error so just convert your numbers to string before proceeding. This worked for me. I spent more than a week behind this only to realise in the end how small of an error it was. Felt like an idiotüòÖ","Mar 2, 2023 7:26 am","AdveatPrasadKarnik","Hi Prashant! I don‚Äôt have any empty cells in my column, yet i keep encountering this error. I have made X as a list(list(int)), and even tried list(list(str)) yet i am not able to use the .fit. I‚Äôd love to get your input of why this is the case","Aug 14, 2023 4:29 pm","nayeem091","Hi, I was facing a similar error. Then I rechecked my input files. Checks out it expects string type value into the tokenizer. The order of my X_train, X_test, y_train, and y_test files was wrong. Hence I was facing an error. I hope it helps.","Aug 26, 2023 2:54 pm","skunkdara","I had the same issue. Turns out that one of the text strings ‚Äú135123‚Äù was intrepeted as an int. To check if this is the issue try df.text.map(len) where text is the name of the text column. If there is an error then this could be the cause. df.text = df.text.astype(str) solved the issue","Nov 10, 2023 4:56 am","Laurie","When I chage questions = df['question'].tolist() answers = df['answer'].tolist() to questions = [str(i) for i in df['question'].tolist()] answers = [str(i) for i in df['answer'].tolist()] It runs smoothly, so we must keep the values be str().","Mar 26, 2024 9:12 am","anon39499327","def tokenize(batch): texts = [str(text) for text in batch[‚Äútext‚Äù]] # convert all to str return tokenizer(texts, padding=True, truncation=True) emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None) IT WORKS!!","Sep 8, 2024 5:20 pm","Am09","def submit(title, text, file): if(title!=‚Äú‚Äù and text!=‚Äú‚Äù): input_text=title+"" ""+text input_text=preprocessing_pipeline(input_text) print(type(input_text)) input_text = tokenizer(str(input_text), return_tensors=‚Äòpt‚Äô, max_length=512, truncation=True) why am i getting this error after passing a string","Oct 5, 2024 3:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to train a LlamaTokenizer?","https://discuss.huggingface.co/t/how-to-train-a-llamatokenizer/64835","22","3.6k","Created: Dec 6, 2023 8:02 pm Latest: Aug 20, 2024 11:14 am","nicholasKluge","I have been trying to train a LlamaTokenizer but I keep running into infinite training times and out of memory problems. For some reason, my script consumes a lot of RAM. Can someone help me? I am trying to train a LlamaTokenizer in Portuguese so my language model (to be trained) is compatible with the entire Llama ecosystem. Here is my script: import yaml import argparse from tqdm import tqdm  import torch import datasets from datasets import load_dataset   from transformers import (     AutoTokenizer,     TrainingArguments, )  from specifications import ModelArguments, DataTrainingArguments, ExtraArguments  def main(spec_file):         # Load the arguments from the spec file     with open(spec_file, ""r"") as stream:         kwargs = yaml.safe_load(stream)          # Get the arguments for the model, data, training, and extra     model_args = ModelArguments(**kwargs['model_args'])     data_args = DataTrainingArguments(**kwargs['data_args'])     training_args = TrainingArguments(**kwargs['training_args'])     extra_args = ExtraArguments(**kwargs['extra_args'])      # Load the dataset from the huggingface Hub and prepare it for training     if data_args.dataset_name is not None and not data_args.dataset_is_tokenized:         dataset = load_dataset(data_args.dataset_name,              split=data_args.dataset_split,              use_auth_token=training_args.hub_token if training_args.hub_token else None,             cache_dir=model_args.cache_dir,             streaming=data_args.streaming,         )     else:         raise ValueError(""No dataset name provided or dataset is already tokenized"")       # Remove non text columns     dataset = dataset.remove_columns([col for col in dataset.column_names if col != ""text""])      # create a python generator to dynamically load the data     def batch_iterator(batch_size=10000):         for i in tqdm(range(0, len(dataset), batch_size)):             yield dataset[i : i + batch_size][""text""]          # Set the configuration kwargs for the tokenizer     tokenizer_kwargs = {         ""cache_dir"": model_args.cache_dir,         ""revision"": model_args.model_revision,         ""use_auth_token"": training_args.hub_token,         ""trust_remote_code"": model_args.trust_remote_code,         ""bos_token"": model_args.bos_token,         ""unk_token"": model_args.unk_token,         ""eos_token"": model_args.eos_token,         ""pad_token"": model_args.eos_token,     }      # Create a tokenizer from the model checkpoint you want to train     tokenizer = AutoTokenizer.from_pretrained(         model_args.tokenizer_name,          **tokenizer_kwargs,     )      new_tokenizer = tokenizer.train_new_from_iterator(         text_iterator=batch_iterator(),          vocab_size=model_args.vocab_size,     )      # Replace the new_tokenizer `max_model_input_sizes` for the `data_args.block_size`     new_tokenizer.max_model_input_sizes.clear()     new_tokenizer.max_model_input_sizes[extra_args.logger_name] = data_args.block_size     new_tokenizer.model_max_length = tokenizer.model_max_length     new_tokenizer.name_or_path = training_args.hub_model_id + ""-tokenizer""      # Save the new tokenizer     new_tokenizer.save_pretrained(training_args.output_dir)          # If hub_token is passed, upload the tokenizer to the hub     if training_args.hub_token is not None and training_args.hub_model_id is not None:                  new_tokenizer.push_to_hub(             repo_id=training_args.hub_model_id + '-tokenizer',             use_auth_token=training_args.hub_token,             commit_message=f""Trained tokenizer from scratch on {data_args.dataset_name}"",         )  if __name__ == ""__main__"":     parser = argparse.ArgumentParser(description=""Train a new Llama tokenizer"")     parser.add_argument(""--spec-file"", help=""Path to the spec YAML file"")     args = parser.parse_args()     main(args.spec_file) My dataset was less than 3M lines/rows. The only time I was able to make this script work was when I reduced the dataset to 100 lines. But that is to little. Note: This script works fine when using a GPT2 tokenizer as the initial tokenizer. Is the sentencepience approach that much slower/memory hungry? Does anyone know what is going on?!","Dec 6, 2023 8:02 pm","nicholasKluge","In case anyone also wants to train one of these, this is how I managed: import json import yaml import argparse from tqdm import tqdm  from datasets import load_dataset from tokenizers import SentencePieceBPETokenizer from transformers import LlamaTokenizerFast, TrainingArguments, AutoTokenizer  from specifications import ModelArguments, DataTrainingArguments, ExtraArguments  def main(spec_file):          # Load the arguments from the spec file     with open(spec_file, ""r"") as stream:         kwargs = yaml.safe_load(stream)          # Get the arguments for the model, data, training, and extra     model_args = ModelArguments(**kwargs['model_args'])     data_args = DataTrainingArguments(**kwargs['data_args'])     training_args = TrainingArguments(**kwargs['training_args'])     extra_args = ExtraArguments(**kwargs['extra_args'])      # Load the dataset from the huggingface Hub and prepare it for training     if data_args.dataset_name is not None and not data_args.dataset_is_tokenized:         dataset = load_dataset(data_args.dataset_name,              split=data_args.dataset_split,              use_auth_token=training_args.hub_token if training_args.hub_token else None,             cache_dir=model_args.cache_dir,             streaming=data_args.streaming,         )     else:         raise ValueError(""No dataset name provided or dataset is already tokenized"")       # Remove non text columns     dataset = dataset.remove_columns([col for col in dataset.column_names if col != ""text""])      # select 2_000_000 random samples from the dataset     dataset = dataset.shuffle(seed=training_args.seed).select(range(2_000_000))      # Create a SentencePieceBPETokenizer     tokenizer = SentencePieceBPETokenizer()      # Train the SentencePieceBPETokenizer on the dataset     tokenizer.train_from_iterator(         iterator=dataset['text'],         vocab_size=32_000,         show_progress=True,         special_tokens=[""<unk>"", ""<s>"", ""</s>"",  ""<pad>""],     )      # Save the tokenizer     tokenizer.save(extra_args.logger_name + ""-sentencepiece-tokenizer.json"", pretty=True)      # Load the new tokenizer as a LlamaTokenizerFast     new_llama_tokenizer = LlamaTokenizerFast(         tokenizer_file=extra_args.logger_name + ""-sentencepiece-tokenizer.json"",         name_or_path=training_args.hub_model_id + ""-tokenizer"",         unk_token=""<unk>"",         unk_token_id=0,         bos_token=""<s>"",         bos_token_id=1,         eos_token=""</s>"",         eos_token_id=2,         pad_token=""<pad>"",         pad_token_id=3,         padding_side=""right"",         max_model_input_sizes={extra_args.logger_name: data_args.block_size},     )      # Save the new tokenizer     new_llama_tokenizer.save_pretrained(extra_args.logger_name + ""-tokenizer"")  if __name__ == ""__main__"":     parser = argparse.ArgumentParser(description=""Train a new Llama tokenizer"")     parser.add_argument(""--spec-file"", help=""Path to the spec YAML file"")     args = parser.parse_args()     main(args.spec_file) It takes some time, but at least it gives you a tokenizer.","Dec 9, 2023 1:00 am","vingpan","Hi, Thanks for sharing this! Great starting point for me. Would it be possible to share a sample spec file? Thanks!","Jan 14, 2024 3:41 am","nicholasKluge","You can use it without one. Just pass the arguments you want directly. Here is a code snippet you can use: import json import argparse from tqdm import tqdm  from datasets import load_dataset from tokenizers import SentencePieceBPETokenizer from transformers import LlamaTokenizerFast, TrainingArguments, AutoTokenizer  def main(args):      # Load the dataset from the huggingface Hub and prepare it for training     if args.dataset_name is not None:         dataset = load_dataset(args.dataset_name,              split=args.dataset_split,              token=args.hub_token if args.hub_token else None,         )     else:         raise ValueError(""No dataset name provided or dataset is already tokenized"")       # Remove non text columns     dataset = dataset.remove_columns([col for col in dataset.column_names if col != ""text""])      # select `num_samples` from the dataset     dataset = dataset.shuffle(seed=42).select(range(arg.num_samples))      # Create a SentencePieceBPETokenizer     tokenizer = SentencePieceBPETokenizer()      # Train the SentencePieceBPETokenizer on the dataset     tokenizer.train_from_iterator(         iterator=dataset['text'],         vocab_size=args.vocab_size,         show_progress=True,         special_tokens=[""<unk>"", ""<s>"", ""</s>"",  ""<pad>""],     )      # Save the tokenizer     tokenizer.save(""new-sentencepiece-tokenizer.json"", pretty=True)      # Load reference tokenizer     if args.reference_tokenizer is not None and args.hub_token is not None:         reference_tokenizer = AutoTokenizer.from_pretrained(args.reference_tokenizer, token=args.hub_token if args.hub_token else None)         reference_tokenizer.save_pretrained(""reference-tokenizer"")     else:         raise ValueError(""No tokenizer name provided or no hub token provided. Try using `--reference_tokenizer 'meta-llama/Llama-2-7b-hf'"")      # Read and dump the json file for the new tokenizer and the reference tokenizer     with open(""new-sentencepiece-tokenizer.json"") as f:         new_llama_tokenizer_json = json.load(f)      with open(""reference-tokenizer/tokenizer.json"") as f:         reference_tokenizer_json = json.load(f)          # Add the reference tokenizer's config to the new tokenizer's config     new_llama_tokenizer_json[""normalizer""] = reference_tokenizer_json[""normalizer""]     new_llama_tokenizer_json[""pre_tokenizer""] = reference_tokenizer_json[""pre_tokenizer""]     new_llama_tokenizer_json[""post_processor""] = reference_tokenizer_json[""post_processor""]     new_llama_tokenizer_json[""decoder""] = reference_tokenizer_json[""decoder""]     new_llama_tokenizer_json[""model""]['fuse_unk'] = reference_tokenizer_json[""model""]['fuse_unk']     new_llama_tokenizer_json[""model""]['byte_fallback'] = reference_tokenizer_json[""model""]['byte_fallback']      # Dump the new tokenizer's config     with open(""new-sentencepiece-tokenizer.json"", ""w"") as f:         json.dump(new_llama_tokenizer_json, f, indent=2, ensure_ascii=False)      # Load the new tokenizer as a LlamaTokenizerFast     new_llama_tokenizer = LlamaTokenizerFast(         tokenizer_file=""new-sentencepiece-tokenizer.json"",         unk_token=""<unk>"",         unk_token_id=0,         bos_token=""<s>"",         bos_token_id=1,         eos_token=""</s>"",         eos_token_id=2,         pad_token=""<pad>"",         pad_token_id=3,         padding_side=""right"",     )      # Save the new tokenizer     new_llama_tokenizer.save_pretrained(""new-llama-tokenizer"")  if __name__ == ""__main__"":     parser = argparse.ArgumentParser(description=""Train a new Llama tokenizer"")     parser.add_argument(         ""--dataset_name"",         type=str,         default=None,         help=""The name of the dataset to be tokenized"",     )     parser.add_argument(         ""--dataset_split"",         type=str,         default=None,         help=""The split of the dataset to be tokenized"",     )     parser.add_argument(         ""--hub_token"",         type=str,         default=None,         help=""The token to access the dataset on the hub"",     )     parser.add_argument(         ""--reference_tokenizer"",         type=str,         default=None,         help=""The name of the reference tokenizer to use"",     )     parser.add_argument(         ""--num_samples"",         type=int,         default=None,         help=""Number of samples to use from the dataset"",     )     parser.add_argument(         ""--vocab_size"",         type=int,         default=None,         help=""Vocabulary size to use for the tokenizer"",     )     args = parser.parse_args()     main(args)  # How to run: # python train_sentencepiece.py --dataset_name ""NeelNanda/pile-10k"" --dataset_split ""train"" --hub_token ""hf_..."" --reference_tokenizer ""meta-llama/Llama-2-7b-hf"" --num_samples 2000000 --vocab_size 32000 Hope it helps!","Jan 14, 2024 8:31 pm","amitagh","Is there a way to update the vocablary by adding new tokens instead of creating a new tokenizer altogether?","Mar 22, 2024 2:14 pm","TristanBehrens","So cool! This is very helpful, @nicholasKluge! Do you see a way to save the ‚Äútokenizer.model‚Äù file? Looks like the LlamaTokenizer class could do this. github.com huggingface/transformers/blob/06b1192768220b77d8f5a22031ed081e79df1616/src/transformers/models/llama/tokenization_llama.py # coding=utf-8 # Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved. # # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX # and OPT implementations in this library. It has been modified from its # original forms to accommodate minor architectural differences compared # to GPT-NeoX and OPT used by the Meta AI team that trained the model. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. This file has been truncated. show original But I did not manage to integrate it into above code. Cheers! Tristan","Apr 15, 2024 12:29 pm","nicholasKluge","Thanks @TristanBehrens! Yes, that is a shortcoming of my implementation (it only gives you a fast tokenizer). Training a slow tokenizer is just (you guessed it) slow, while converting a fast tokenizer to a slow one is something I could not do until now. I‚Äôm hoping someone cracks this for the rest of us (and share it!) ‚Ä¶","Apr 16, 2024 1:23 pm","TristanBehrens","@nicholasKluge. I think I got it! You can train tokenizers with sentencepiece: GitHub GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural... Unsupervised text tokenizer for Neural Network-based text generation. - google/sentencepiece This will give you the tokenizer.model file. It can be loaded with LlamaTokenizer.","Apr 16, 2024 3:46 pm","nicholasKluge","Do you have some code snippets we can use to reproduce this training?","Apr 16, 2024 4:00 pm","TristanBehrens","Here it is: github.com google/sentencepiece/blob/master/python/README.md#model-training # SentencePiece Python Wrapper  Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.  ## Build and Install SentencePiece For Linux (x64/i686), macOS, and Windows(win32/x64) environment, you can simply use pip command to install SentencePiece python module.  ``` % pip install sentencepiece ```  To build and install the Python wrapper from source, try the following commands to build and install wheel package. ``` % git clone https://github.com/google/sentencepiece.git  % cd sentencepiece % mkdir build % cd build % cmake .. -DSPM_ENABLE_SHARED=OFF -DCMAKE_INSTALL_PREFIX=./root % make install % cd ../python This file has been truncated. show original It is most trivial. Which gives me mixed feelings becaused I worked on this for weeks","Apr 16, 2024 4:21 pm","nicholasKluge","I‚Äôm sorry, but I still don‚Äôt understand your solution. Could you please give the code to train a tokenizer on, for example, NeelNanda/pile-10k? If you can provide a working Colab notebook, that would help even more.","Apr 16, 2024 4:40 pm","TristanBehrens","Piece of cake! Google Colab","Apr 16, 2024 5:02 pm","nicholasKluge","Very nice! Were you able to convert this tokenizer into a fast one?","Apr 16, 2024 5:49 pm","TristanBehrens","A fast tokenizer was never a requirement of mine. Thus I did not try. Should work like you outlined above. Wanna give it a try?","Apr 17, 2024 11:46 am","nicholasKluge","In the end, you just have to load the new tokenizer as a fast one, and save it, and all is done: # Load the new sp tokenizer tokenizer = LlamaTokenizer(""./tokenizer.model"")  # Save it. tokenizer.save_pretrained(""./new-llama-tokenizer"")  # Create a new tokenizer using the `LlamaTokenizerFast` class. tokenizer = LlamaTokenizerFast(""./new-llama-tokenizer/tokenizer.model"")  # Save it. tokenizer.save_pretrained(""./new-llama-tokenizer"") And done! Now, you have a fast and slow tokenizer in the ‚Äúnew-llama-tokenizer‚Äù folder. This implementation is way simpler than my own. Congrats to @TristanBehrens for showing the way.","Apr 17, 2024 10:56 pm","TristanBehrens","@nicholasKluge it was a pleasure working with you!","Apr 18, 2024 5:53 am","MikeMpapa","Thank you @nicholasKluge & @TristanBehrens for the very insightful discussion!!Vey cool. I do need to do a similar thing for training a model on a ‚Äúcustom‚Äù language. I have followed the steps described in this post and trained my tokenizer. However, it is not intuitive to me how it will be combined with a Llama model if I need to fine-tune it. The pre-trained model will still expect the token IDs from the original tokenizer, while my new tokenizer will have completely new token ordering. Am I missing something here or my only option to train a Llama from scratch with the new tokenizer? Do you have an example where It shows how to combine a custom tokenizer with a Llama training job? Thank you very much for all the insights already!!","Jun 7, 2024 8:52 pm","MikeMpapa","nicholasKluge: # Add the reference tokenizer's config to the new tokenizer's config     new_llama_tokenizer_json[""normalizer""] = reference_tokenizer_json[""normalizer""]     new_llama_tokenizer_json[""pre_tokenizer""] = reference_tokenizer_json[""pre_tokenizer""]     new_llama_tokenizer_json[""post_processor""] = reference_tokenizer_json[""post_processor""]     new_llama_tokenizer_json[""decoder""] = reference_tokenizer_json[""decoder""]     new_llama_tokenizer_json[""model""]['fuse_unk'] = reference_tokenizer_json[""model""]['fuse_unk']     new_llama_tokenizer_json[""model""]['byte_fallback'] = reference_tokenizer_json[""model""]['byte_fallback'] Actually, is this the piece where you combine the original with the new tokenizer? Any hint is greatly appreciated!","Jun 7, 2024 9:47 pm","TristanBehrens","I have doubts that using a pre-trained model with a new custom tokenizer would work.","Jun 8, 2024 5:49 am","nicholasKluge","Your intuition, @MikeMpapa, and @TristanBehrens comments are correct. It does not work. A tokenizer is just a look-up table that maps pieces of words (or words themselves) into an index of the embedding matrix. This index contains the embedding vector, which was learned during training. Changing this mapping ‚Äúbreaks‚Äù the model. If you train a new tokenizer, you cannot simply use it on an already pre-trained model with its own tokenizer. A new tokenizer is something you do when you want to train a new model from scratch. Once that model learns to map specific tokens to specific embeddings, we don‚Äôt go and change the tokenizer. You can continually expand a tokenizer but remember that you also need to broaden the embedding matrix (and train those embeddings) if you wish your model to learn how to ‚Äúuse them‚Äù correctly. About these lines of code: new_llama_tokenizer_json[""normalizer""] = reference_tokenizer_json[""normalizer""] new_llama_tokenizer_json[""pre_tokenizer""] = reference_tokenizer_json[""pre_tokenizer""] new_llama_tokenizer_json[""post_processor""] = reference_tokenizer_json[""post_processor""] new_llama_tokenizer_json[""decoder""] = reference_tokenizer_json[""decoder""] new_llama_tokenizer_json[""model""]['fuse_unk'] = reference_tokenizer_json[""model""]['fuse_unk'] new_llama_tokenizer_json[""model""]['byte_fallback'] = reference_tokenizer_json[""model""]['byte_fallback'] Here, we are just inheriting some hypersettings from the original llama tokenizer (e.g., do byte fallback if < unk> token appears). None of this is related to the actual learning of the look-up table/mapping between pieces of words and integers. I hope this explanation helps!","Jun 8, 2024 9:06 am","","","","","","","","",""
"‚ÄúOSError: Model name ‚Äò./XX‚Äô was not found in tokenizers model name list‚Äù - cannot load custom tokenizer in Transformers","https://discuss.huggingface.co/t/oserror-model-name-xx-was-not-found-in-tokenizers-model-name-list-cannot-load-custom-tokenizer-in-transformers/2714","14","6.8k","Created: Dec 9, 2020 7:46 am Latest: Apr 25, 2023 12:18 pm","tlqnguyen","I‚Äôm trying to create tokenizer with my own dataset/vocabulary using Sentencepiece and then use it with AlbertTokenizer transformers. I followed really closely the tutorial on how to train a model from scratch: https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb#scrollTo=hO5M3vrAhcuj     # import relevant libraries        from pathlib import Path     from tokenizers import SentencePieceBPETokenizer     from tokenizers.implementations import SentencePieceBPETokenizer     from tokenizers.processors import BertProcessing     from transformers import AlbertTokenizer           paths = [str(x) for x in Path(""./data"").glob(""**/*.txt"")]          # Initialize a tokenizer     tokenizer = SentencePieceBPETokenizer(add_prefix_space=True)          # Customize training     tokenizer.train(files=paths,                      vocab_size=32000,                      min_frequency=2,                      show_progress=True,                     special_tokens=['<unk>'],)      # Saving model     tokenizer.save_model(""Sent-AlBERT"")      tokenizer = SentencePieceBPETokenizer(         ""./Sent-AlBERT/vocab.json"",         ""./Sent-AlBERT/merges.txt"",)      tokenizer.enable_truncation(max_length=512) Everything is fine up until this point when I tried to re-create the tokenizer in transformers     # Re-create our tokenizer in transformers         tokenizer = AlbertTokenizer.from_pretrained(""./Sent-AlBERT"", do_lower_case=True) This is the error message I kept receiving: OSError: Model name './Sent-AlBERT' was not found in tokenizers model name list (albert-base-v1, albert-large-v1, albert-xlarge-v1, albert-xxlarge-v1, albert-base-v2, albert-large-v2, albert-xlarge-v2, albert-xxlarge-v2). We assumed './Sent-AlBERT' was a path, a model identifier, or url to a directory containing vocabulary files named ['spiece.model'] but couldn't find such vocabulary files at this path or url. For some reason, it works with RobertaTokenizerFast but not with AlbertTokenzier. If anyone could give me a suggestion or any sort of direction on how to use Sentencepiece with AlberTokenizer I would really appreciate it. P.S: I also tried to use ByteLevelBPETokenizer with DistilBertTokenizer, but it couldn‚Äôt recognize the tokenizer in the transformer either. I‚Äôm not sure what I am missing here.","Dec 9, 2020 7:46 am","sgugger","You can‚Äôt directly use this tokenizer with a ‚Äúslow‚Äù tokenizer (not backed by rust) there is a conversion step to do (not super versed in it but maybe @thomwolf can chime in?). It should work with AlbertTokenizerFast (which has more functionality and is faster so it should be a win-win over all!)","Dec 9, 2020 12:50 pm","tlqnguyen","Thank you so much for your comment but I don‚Äôt think there is AlbertTokenizerFast? I didn‚Äôt see it in the official documentation: https://huggingface.co/transformers/model_doc/albert.html#alberttokenizer","Dec 9, 2020 1:24 pm","sgugger","Oh that‚Äôs a mistake in the docs. There is definitely one (as you can see on this table).","Dec 9, 2020 1:56 pm","tlqnguyen","I tried but received the following error: ImportError: cannot import name 'AlbertTokenizerFast'","Dec 9, 2020 2:33 pm","BramVanroy","That should work if tokenizers is installed. You can also try directly importing from transformers.models.albert.","Dec 9, 2020 2:59 pm","sgugger","No this should always work since the object is always present in the init (if tokenziers is not installed, an error is raised when you try to actually use that object). If it does not work, it means you don‚Äôt have v4 of transformers in your environment.","Dec 9, 2020 3:19 pm","tlqnguyen","Ah, it is because I was using transformers == 3.3.1 After upgrading to v4 and import AlbertTokenizerFast, I received the following error: from transformers import AlbertTokenizerFast  # Re-create our tokenizer in transformers tokenizer = AlbertTokenizerFast.from_pretrained(""./Sent-AlBERT"")  OSError: Can't load tokenizer for './Sent-AlBERT'. Make sure that:  - './Sent-AlBERT' is a correct model identifier listed on 'https://huggingface.co/models'  - or './Sent-AlBERT' is the correct path to a directory containing relevant tokenizer files","Dec 9, 2020 3:47 pm","BramVanroy","Isn‚Äôt AlbertTokenizerFast only available in from transformers if tokenizers is installed? I don‚Äôt see where else it is imported in transformers.init. https://github.com/huggingface/transformers/blob/e977ed2142a022aa969c03836340edcff4f479b2/src/transformers/init.py#L241-L242 @tlqnguyen Make sure the path is correct. You are using a relative path now, so make sure it is relative to your current script. If you can‚Äôt get it to work, try an absolute path.","Dec 22, 2020 9:04 am","sgugger","There is an else here that imports a dummy object with the same name and will tell you to install tokenizers if not available This is a script I added so that the init always has the same objects.","Dec 22, 2020 2:28 pm","BramVanroy","Aha, that makes sense. Thanks for the clarification.","Dec 22, 2020 3:45 pm","ningmoufubi","Is your question solvedÔºüWhat is the solution to this problem? The reason why the pre-training model failed to load have nothing to do with the Transformers version. I also hive this question, when i use pre-training model ,named ‚Äô uer/chinese_roberta_L-8_H-512‚Äô. my Transformers version is 3.4.0.","Jan 25, 2021 7:21 am","tlqnguyen","Actually, I think it might have to do with the Transformers version. As I upgraded Transformers to 4.0.0, it worked and I was able to load the tokenizer","Jan 25, 2021 8:41 am","Sumit6597","I am trying to use the below pre trained model but its giving the same error : (OSError: Can‚Äôt load tokenizer for ‚Äòcopenlu/citebert-cite-only‚Äô. If you were trying to load it from ‚ÄòModels - Hugging Face‚Äô, make sure you don‚Äôt have a local directory with the same name. Otherwise, make sure ‚Äòcopenlu/citebert-cite-only‚Äô is the correct path to a directory containing all relevant files for a BertTokenizerFast tokenizer.) My transformer version is 4.17.0. Please let me know. from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained(‚Äúcopenlu/citebert-cite-only‚Äù) model = AutoModel.from_pretrained(‚Äúcopenlu/citebert-cite-only‚Äù)","Mar 27, 2022 7:33 pm","Unspoiled-Egg","Is your question solved?","Apr 25, 2023 12:18 pm","","","","","","","","","","","","","","","","","","","","","","","",""
"How to know if a subtoken is a word or part of a word?","https://discuss.huggingface.co/t/how-to-know-if-a-subtoken-is-a-word-or-part-of-a-word/923","10","6.6k","Created: Aug 30, 2020 1:34 am Latest: Aug 29, 2022 9:38 am","wgpubs","For example, using BERT in a token classification task, I get something like this ‚Ä¶ [('Dar√ºber', 17), ('hinaus', 17), ('fanden', 17), ('die', 17), ('Er', 17), ('##mitt', -100), ('##ler', -100), ('eine', 17), ('Ver', 17), ('##legung', -100), ('##sli', -100), ('##ste', -100), (',', 17), ('die', 17), ('best√§tigt', 17), (',', 17), ('dass', 17), ('Dem', 8), ('##jan', -100), ('##juk', -100), ('am', 17), ('27', 17), ('.', -100), ('M√§rz', 17), ('1943', 17), ('an', 17), ('die', 17), ('Dienst', 17), ('##stelle', -100), ('So', 0), ('##bi', -100), ('##bor', -100), ('ab', 17), ('##kom', -100), ('##mand', -100), ('##iert', -100), ('wurde', 17), ('.', -100)] ‚Ä¶ in the format of (sub-token, label id). Is there a way I can automatically know that ‚Äú##mitt‚Äù and ‚Äú##ler‚Äù are part of ‚ÄúEr‚Äù (thus making up the word ‚ÄúErmittler‚Äù) that would work across all tokenizers (not just BERT)?","Aug 30, 2020 1:34 am","rgwatwormhill","what do you mean by ‚Äúautomatically know‚Äù? I guess you already know that ##xx tokens are continuation tokens. I don‚Äôt think it is possible to detect from (‚ÄòEr‚Äô, 17) that it has a continuation. If you feed the data into an untrained Bert model, [I think] the embedding layer will create an embedding vector for (‚ÄòEr‚Äô,17) that does not depend on the continuation tokens. If you feed the data into a trained Bert model, the embedding layer might create different embedding vectors for different instances of (‚ÄòEr‚Äô, 17), depending on their context, which includes depending on any continuation tokens. There is a nice tutorial by Chris McCormick here https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ that discusses embeddings in more detail. If you were asking about something else, please clarify. [I am not an expert, and I could be wrong]","Aug 30, 2020 11:16 am","wgpubs","I mean programmatically be able to determine if a subtoken is ‚Äúpart‚Äù of a word or a word itself, regardless of the tokenizer. So given this: ... ('die', 17), ('Er', 17), ('##mitt', -100), ('##ler', -100) ... A function that uses either the subtoken or the id, to return True/False if it is part of a word or a word itself ‚Ä¶ tokenizer.is_token_part('die') #    => False tokenizer.is_token_part('Er') #     => False tokenizer.is_token_part('##mitt') # => True tokenizer.is_token_part('##ler') #  => True And again, this would work across all tokenizers.","Aug 30, 2020 6:00 pm","rgwatwormhill","I think that is impossible. For example, (‚ÄòEr‚Äô) will sometimes be a whole word and sometimes be part of Ermittler. If your function only sees ‚ÄòEr‚Äô or ‚Äò17‚Äô, it can‚Äôt know which is true. On the other hand, if your function sees the original text as well, it can easily detect whether it is a whole word or not. Different tokenizers will split words in different ways, but any function that sees both the whole text and the tokenized text could comment on it. Why do you need such a function?","Aug 30, 2020 7:41 pm","wgpubs","rgwatwormhill: Why do you need such a function? For reassembling token classification predictions, where given the 4 subtokens above there would only be two predictions (one for ‚Äúdie‚Äù and one for ‚ÄúErmittler‚Äù)","Aug 30, 2020 9:25 pm","rgwatwormhill","Oh, I see, thanks. I think you will have to look at the next token (##mitt) to know the answer for Er.","Aug 31, 2020 10:12 am","joeddav","This doesn‚Äôt entirely answer your question, but one arg that might be helpful is the return_offset_mappings arg which you can pass to tokenizer and which will return the character offset from the original sequence for each token. What you could do is just do some kind of simple (non-subword) tokenization first, and then pass the resulting tokens to the subword tokenizer with return_offset_mappings=True and is_pretokenized=True. E.g., encodings = tokenizer(['die', 'Ermittler'], is_pretokenized=True, return_offsets_mapping=True) print(tokenizer.convert_ids_to_tokens(encodings['input_ids'])) # ['[CLS]', 'die', 'er', '##mit', '##tler', '[SEP]'] print(encodings['offset_mapping']) # [(0, 0), (0, 3), (0, 2), (2, 5), (5, 9), (0, 0)] is_subword = np.array(encodings['offset_mapping'])[:,0] != 0 print(is_subword) # array([False, False, False,  True,  True, False]) Note that you need to be using the fast tokenizers to use this feature, e.g. BertTokenizerFast. There‚Äôs an example of this in the NER example of our custom datasets tutorial in (and in the paragraph before) the encode_tags function definition. Not a complete solution but hope that helps.","Aug 31, 2020 2:31 pm","wgpubs","Ah thanks ‚Ä¶ that is exactly what I was looking for. Is this going to get into the standard tokenizers as well? If not, 1) why and 2) when should folks choose one over the other (e.g, the ‚Äúfast‚Äù vs. the standard tokenizers)? Thanks again!","Sep 1, 2020 8:37 pm","joeddav","I think the short answer is that the the fast (Rust-based) tokenizers are newer and will at some point completely replace the Python-based tokenizers. They are much faster and I‚Äôm not sure if there‚Äôs a good reason not to use them at this point, but I‚Äôm not the expert there. cc @mfuntowicz","Sep 1, 2020 11:29 pm","wydwww","There is a word_ids key of the returned BatchEncoding that exactly does this. A list indicating the word corresponding to each token. huggingface.co Tokenizer We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science. But it looks like to be available only in PreTrainedTokenizerFast. I didn‚Äôt find this word_ids in the returned value when using PreTrainedTokenizer. The documentation says that this kind of mapping back to the original texts is only available in the fast version: When the tokenizer is a ‚ÄúFast‚Äù tokenizer (i.e., backed by HuggingFace tokenizers library), this class provides in addition several advanced alignment methods which can be used to map between the original string (character and words) and the token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding to a given token).","Dec 17, 2021 8:25 am","Depshad","The embedded link is no longer accessible. Can you please share the new working link for custom datasets tutorial?","Aug 29, 2022 9:38 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Add new tokens for subwords","https://discuss.huggingface.co/t/add-new-tokens-for-subwords/489","9","6.7k","Created: Jul 27, 2020 11:04 am Latest: Aug 11, 2020 12:15 am","Muennighoff","Hey, I am trying to add subword tokens to bert base uncased as follows: num = tokenizer_bert.add_tokens(‚Äò##committed‚Äô) tokenizer_bert.tokenize(‚Äò##committed‚Äô) [‚Äò##committed‚Äô] tokenizer_bert.tokenize(‚Äòhellocommitted‚Äô) [‚Äòhello‚Äô, ‚Äò##com‚Äô, ‚Äò##mit‚Äô, ‚Äò##ted‚Äô] It seems like the tokenizer is literally adding the hashtags, when I would want to create a new subword called ##commited. I am doing this to deal with hashtags & thinking of initializing those new subwords to their original words. Any solutions / better ways to deal with hashtags, would be really appreciated! Thanks ‚Äì","Jul 27, 2020 11:04 am","valhalla","I‚Äôm not sure if adding subwords directly is possible, you can try to add them as special tokens instead so only id will be created for them instead of splitting. Pinging @anthony for more details.","Jul 27, 2020 3:00 pm","anthony","The tokens you add with add_tokens are not added directly to the original vocabulary, but instead they are part of a special vocabulary. They end up being handled first, so that what you define manually always has the priority. As you noticed, if you specify ##committed in the input text, it will use your token, but not without the ##. This is simply because they are treated literally, just as you added them. So, you should be able to achieve what you want by doing: tokenizer.add_tokens([ 'committed' ]) tokenizer.tokenizer('hellocommitted') # [ 'hello', 'commited' ]","Jul 29, 2020 11:56 pm","Muennighoff","Thanks for the help. I tried both of your answers, but neither works: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') tokenizer.add_tokens([  'committed'  ]) tokenizer.tokenize('hellocommitted') >> ['hello', '##com', '##mit', '##ted']   tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') tokenizer.add_special_tokens({'unk_token': 'committed' }) tokenizer.tokenize('hellocommitted') >> ['hello', '##com', '##mit', '##ted'] (Note: add_special_tokens failed with an assertion error, so i used unk_token as key) Any other ideas? Edit: What does work though is if I tokenize two new words, as follows: tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') num = tokenizer.add_tokens(['helloxxx', 'committed']) tokenizer.tokenize('helloxxxcommitted') >> ['helloxxx', 'committed'] So I guess I could delete every word in the vocab and re-add it to achieve my goal, but I imagine there must be a better solution?","Jul 30, 2020 8:19 am","Muennighoff","Any other ideas for solving this? Thanks!","Aug 4, 2020 3:10 pm","rgwatwormhill","Hi, silly question: why do you have num = tokenizer.add_tokens for your first and last attempts, but just tokenizer.add_tokens for the middle one?","Aug 6, 2020 4:05 pm","Muennighoff","Add_tokens returns the number of tokens you have added; For the middle one I just copied the answers from @anthony & @valhalla, so I didnt add it.","Aug 7, 2020 6:11 am","rgwatwormhill","OK, thanks for replying. What happens if you do tokenizer.tokenize(‚Äòhellocommitted‚Äô) immediately after doing tokenizer.tokenize(‚Äòhelloxxxcommitted‚Äô) What is the result of tokenizer.tokenize(‚Äòuncommitted‚Äô) Clearly, it has managed to create the ‚Äòcommitted‚Äô token, so it seems very odd that it wouldn‚Äôt use it except when it arrives with helloxxx. Is that really happening, or is it just that your earlier attempt failed for some reason, ie when you did tokenizer.add_tokens([ ‚Äòcommitted‚Äô ]) Maybe it doesn‚Äôt like the spaces, or maybe there was a system failure at the wrong moment. I just noticed something else odd: your last example should have output ‚Äòhelloxxx‚Äô, ‚Äò##committed‚Äô (if the ‚Äòcommitted‚Äô token has been added at all). Are you sure that‚Äôs what it said? On a different tack, what do you mean by ‚Äúdeal with hashtags‚Äù ? Are you using twitter data? What does your bert model currently do with # inputs? Are you sure that that wouldn‚Äôt be fine? Do you have enough data to retrain the model to deal with the new ‚Äòcommitted‚Äô token? (see the answer to this post https://stackoverflow.com/questions/60068129/transformers-pretrainedtokenizer-add-tokens-functionality ). Could you preprocess all your input data so that the input # characters become something else that doesn‚Äôt have a special meaning within transformers?","Aug 7, 2020 12:01 pm","Muennighoff","Thanks for your help! Here are all outputs & commands you mentioned: from transformers import BertTokenizer tokenizer = BertTokenizer.from_pretrained(‚Äòbert-base-uncased‚Äô) num = tokenizer.add_tokens([‚Äòhelloxxx‚Äô, ‚Äòcommitted‚Äô]) tokenizer.tokenize(‚Äòhelloxxxcommitted‚Äô) [‚Äòhelloxxx‚Äô, ‚Äòcommitted‚Äô] tokenizer.tokenize(‚Äòhellocommitted‚Äô) [‚Äòhello‚Äô, ‚Äò##com‚Äô, ‚Äò##mit‚Äô, ‚Äò##ted‚Äô] tokenizer.tokenize(‚Äòuncommitted‚Äô) [‚Äòun‚Äô, ‚Äò##com‚Äô, ‚Äò##mit‚Äô, ‚Äò##ted‚Äô] The reason it does not work is that the tokenizer already has the word commited, hence it does not add it, even though I‚Äôd like to add it as an additional substring. By dealing with hashtags, I mean that I have words like this: #justnorthkoreathings and they get tokenized as: [‚Äô#‚Äô, ‚Äòjust‚Äô, ‚Äò##nor‚Äô, ‚Äò##th‚Äô, ‚Äò##kor‚Äô, ‚Äò##ea‚Äô, ‚Äò##thing‚Äô, ‚Äò##s‚Äô]. While I have a lot of confidence in BERT, I doubt this tokenization is a very good signal, as a) BERT is pretrained on Wikipedia articles & books where hashtags aren‚Äôt the norm b) I‚Äôd argue that attention is at its best, the fewer words it needs to combine - here it needs to combine 4 elements to understand north korea‚Ä¶ Would appreciate any tips, how to deal with this!","Aug 10, 2020 7:02 pm","rgwatwormhill","Hello again, I didn‚Äôt quite believe the system would be doing what your data showed, but I‚Äôve checked it myself and it really is! I don‚Äôt think it should be working quite like that: Following an added token, the next token is shown without continuation ## and is a complete word. (so, as you said, when ‚Äòhelloxxx‚Äô is an added token, ‚Äòhelloxxxcommitted‚Äô is tokenized as [‚Äòhelloxxx‚Äô, ‚Äòcommitted‚Äô] ) The same effect happens following a ‚Äò#‚Äô (‚Äô#justnorth‚Äô is tokenized as [‚Äô#‚Äô, ‚Äòjust‚Äô, ‚Äò##nor‚Äô, ‚Äò##th‚Äô]). However, that‚Äôs no help to you. It doesn‚Äôt seem to be possible to add continuation tokens. If I were working on this, I would start by considering whether to drop all the hashtags completely. Do they actually add any information that isn‚Äôt also in the rest of the tweet? Another possibility would be to replace all of them with a single-word string, such as ‚Äò#hashtag‚Äô . If I decided the words in the hashtags were required, I would pre-process them before passing them to the tokenizer. One option would be to split them into words. (For partial code for this see eg https://www.techiedelight.com/word-break-problem/ ). There would then be issues with choosing which of the possible splits, but I expect that choosing the fewest subwords each time would be good enough. If you have a very small dataset, you might consider replacing the hashtags manually, with meaningful phrases. I think you are probably correct in thinking that Bert will not have seen ‚Äònor ##th ##kor ##ea‚Äô very often. On the other hand, it might not have seen ‚Äònorth korea‚Äô often enough to develop a particularly useful representation. If you have a really large dataset, you could train your own tokenizer, including the hashtags in the vocab, but that would be a last resort. I am not an expert, but I‚Äôm not sure that a large number of tokens per word is actually a problem. After all, Bert is used to working with very long strings of tokens, making some kind of overall representation of a text, so it isn‚Äôt restricted to single word meanings. If the vocab is only about 30000, then there must be a large number of words that have to be represented by two or more tokens, so Bert must be quite good at dealing with these.","Aug 11, 2020 12:15 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with KOSMOS-2 encoding and decoding","https://discuss.huggingface.co/t/issue-with-kosmos-2-encoding-and-decoding/70019","11","449","Created: Jan 22, 2024 1:42 am Latest: Jan 26, 2024 12:21 pm","Mit1208","Hi @ydshieh, First of all I would like to say thank you for your work on Kosmos-2. I want to using Kosmos to for document AI. I am using trying to convert doclaynet dataset to embeddings. I read your code and research paper and found that first I need to convert bounding box to patch_XXXX thing. I tried that using processor but when I decode input_ids then I found miss match on the bounding box on the image. test = processor(images = [example['image']], text = [text], bboxes = [float_val]) test_decode = processor.decode(test['input_ids'][0]) As you can see in the image that bounding boxes were overlapped and all. Can you help me to figure this out? Here is the code: Google Colab Thank you so much.","Jan 22, 2024 1:42 am","ydshieh","Hi @Mit1208 If I understand correctly, the attached contains the bounding boxes obtained via the processing(s) of Kosmos2Processor processed_text, entities = processor.post_process_generation(test_decode) And you want to demonstrate via this image that the bounding boxes are not matching the original input of bounding boxes (where you specified via set_box then go through normalized_box(convert_box). Is this correct? I can see your image width/height is set to 1025 (I didn‚Äôt verify however). Kosmos2Processor splits the image into a 32 x 32 grid. With the original input size 224, each cell is of siez 7x7. However, with your image size 1025, each cell will have size 32x32 which is quite large in document AI I believe. In order to verify this, you can compare the original bboxes input agains the final processed/computed output bboxes, and see if their differences are in the ragen of 32 (or even 64). If the differences are all inside this range, I wouldn‚Äôt say there is something wrong in the code of Kosmos2Processor. In this case, it‚Äôs just the limitation of such processing. If the differences are large than 32 or 64, something is likely to be wrong and I can take a more close look. If you don‚Äôt mind to train a model from scratch, there are some arguments could be changed to modify the default properties of Kosmos2Processor. I can share more information if you are donw to this.","Jan 24, 2024 1:59 pm","Mit1208","Thank you so much @ydshieh for lookin into this issue. Yes your understanding is correct. You are right that the image size is 1025. About bounding boxes - labeled bounding box is ranging from 0-1025. I normalized it to 0-1 from Kosmos2Processor. I tried resizing image to 224 but found same results. You mentioned below, how can I do that? In order to verify this, you can compare the original bboxes input agains the final processed/computed output bboxes, and see if their differences are in the ragen of 32 (or even 64). I think training it from scratch is difficult because I don‚Äôt have GPUs. Do you think I can train the model on free Colab?","Jan 24, 2024 2:25 pm","ydshieh","It would be helpuful if you can: check first what I suggest above In order to verify this, you can compare the original bboxes input agains the final processed/computed output bboxes, and see if their differences are in the ragen of 32 (or even 64). ‚Ä¶ For I tried resizing image to 224 but found same results. Could you add the extra cells (from the end of the original notebook) showing this However, I think with 224 document image, the word/line gaps are very smaller too. The ideal situation is that word/line gaps are large enough (compared to the word font size), so a bit off of the bounding bboxes won‚Äôt have undesired visual effect.","Jan 24, 2024 2:48 pm","ydshieh","I think training it from scratch is difficult because I don‚Äôt have GPUs. Do you think I can train the model on free Colab? First, this model is not trained on documents, I would doubt if it will work well (even with finetuning). train the model from scratch on free Colab ‚Üí it probably could run but will be quite slow (data processing, training speed etc.)","Jan 24, 2024 2:52 pm","Mit1208","I have added 224*224 resizing code and output, it‚Äôs the same as the normal image. Can you see if my bbox conversion from (x, y, w, h) to [x, y, x+w, y+h] is correct and do I need to repeat that step after decoding from Kosmos2Processor? It seems I am very close but missing something small. I just wanted to see how this model reacts to the document compared to LayoutLMv3.","Jan 24, 2024 3:04 pm","ydshieh","Hi , thanks. I see you post the desired outputs. Looking both visually, I think it‚Äôs really just the limitation of Kosmos2Processor (which using cells in a 32x32 grid), which works well in a common images with smaller size. For documents, the cell approaches isn‚Äôt good as we need higher precision of the locations.","Jan 24, 2024 5:04 pm","Mit1208","Hi, is there a workaround for this like, creating more patches to handle this? I don‚Äôt have a lot of experience other than fine-tuning models but I can give it a shot. Can you refer to some documentation or tutorial which I can follow to learn and overcome this issue? Thanks.","Jan 24, 2024 5:32 pm","ydshieh","Yes, you can specify num_patch_index_tokens when creating Kosmos2Processor. (I think it works when we use from_pretrained and passing num_patch_index_tokens) And while doing process(es), we need to pass num_patches_per_side. (see clean_text_and_extract_entities_with_bboxes) (post_process_generation doesn‚Äôt have this argument, which I should probably update this method)","Jan 24, 2024 6:09 pm","ydshieh","Let me know how it goes!","Jan 24, 2024 6:09 pm","Mit1208","Thanks, sure I will keep you posted.","Jan 26, 2024 12:21 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Get ‚Äúusing the `__call__` method is faster‚Äù warning with DataCollatorWithPadding","https://discuss.huggingface.co/t/get-using-the-call-method-is-faster-warning-with-datacollatorwithpadding/23924","8","15.5k","Created: Oct 3, 2022 8:43 am Latest: Jun 3, 2024 9:21 pm","SteveR","When I use the out-of-the-box DataCollatorWithPadding I get my output filled with the warning: You‚Äôre using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the __call__ method is faster than using a method to encode the text followed by a call to the pad method to get a padded encoding. If I change to use a different, custom collator, then the warning goes away. Would anyone know what I could be doing wrong that‚Äôs causing this warning? Or alternatively, if I can‚Äôt fix the problem that‚Äôs causing this warning, is there a way to hide it? I‚Äôve tried a few different ways of turning off warnings, but so far I‚Äôve had no luck and because it gets written out multiple times it starts to swamp the actual output from my training.","Oct 3, 2022 8:43 am","mnaylor","I‚Äôm getting the same thing using a BertTokenizerFast with DataCollatorWithPadding - the error appears once for each worker every time I loop over a DataLoader. I would prefer not to silence warnings in my training code, but here‚Äôs how I‚Äôm getting around it (based on this line in the PretrainedTokenizerBase class, referencing this section in the custom logger): import os os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = 'true'","Oct 5, 2022 6:38 pm","Bikash","Same situation - happened with usage of DistilBertTokenizerFast tokenizer on using DataCollatorWithPadding. I don‚Äôt know what does this warning actually mean? And if I wanted to follow the suggestion as hinted in the warning message, what should I do?","Oct 7, 2022 10:07 am","KarmaCST","I am getting following suggestion while using NllbTokenizerFast tokenize You‚Äôre using a NllbTokenizerFast tokenizer. Please note that with a fast tokenizer, using the __call__ method is faster than using a method to encode the text followed by a call to the pad method to get a padded encoding. Can anyone explain the given suggestion?","Nov 24, 2022 6:04 am","fredguth","I guess the problem most people in this thread have is not with the warnings but with the lack of knowledge (at least at my part) if there is a better way of doing what I am doing. I also use a preprocess function.","May 9, 2023 3:29 pm","ollibolli","It would be nice to have an answer from someone on this? So my guess is that doing tokenizer(input) is faster than wrapping the datacollator around?","May 13, 2023 2:08 pm","filippobistaffa","Any news on this? Personally I found this warning more annoying than informative, so I ended up commenting it out in the source code, since I couldn‚Äôt find any information.","Sep 21, 2023 8:45 am","d3nigma","Hello, the warning is confusing because it does not tell you how to use the tokenizer(...). I think it just means, that one should use the padding feature of the tokenizer when tokenizing your dataset. So, instead of using a collator to pad your inputs, skip this step and use tokenizer(..., padding=True) to add padding while tokenizing. Apparently, this is faster for FastTokenizers (i.e. tokenizers written in Rust). Remark: In case you didn‚Äôt know when calling tokenizer(...) this internally calls Tokenizer.__call__. This is a magic python method: python-callable-instances Disclaimer: That‚Äôs just my interpretation of what it could mean. I would be happy if someone could confirm this.","Feb 1, 2024 7:19 pm","AtxTom","You can turn off the warning with tokenizer.deprecation_warnings[""Asking-to-pad-a-fast-tokenizer""] = True See When using transformers.DataCollatorWithPadding normally, always get annoying warning ¬∑ Issue #22638 ¬∑ huggingface/transformers ¬∑ GitHub","Jun 3, 2024 9:21 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Importing tokenizers version >0.10.3 fails due to openssl","https://discuss.huggingface.co/t/importing-tokenizers-version-0-10-3-fails-due-to-openssl/17820","3","6.4k","Created: May 11, 2022 1:15 pm Latest: Jun 2, 2022 5:19 pm","Gback","So, the only tokenizer version I can install is 0.10.3 or lower. Any version higher (0.11 or higher) runs into an libssl error (specifically libssl.so.3 does not exist). As far as I understand this is related to openssl1.1.1 being installed instead of 3. However, some packages installed by hugging faces (i.e. tensorflow) require openssl1.1.1 or lower, meaning that I cannot install openssl3. Anybody run into similar problems and has a solution?","May 11, 2022 1:15 pm","nouamanetazi","For me I solved the error by installing tokenizers using pip instead of conda","May 21, 2022 7:43 pm","jonathanli","I‚Äôm having this exact problem, though it‚Äôs unable to locate openssl.so.10. If I create a symlink as described in this ask ubuntu page, I get another error: File ~/anaconda3/lib/python3.9/site-packages/tokenizers/__init__.py:79, in <module>      75     MERGED_WITH_NEXT = ""merged_with_next""      76     CONTIGUOUS = ""contiguous"" ---> 79 from .tokenizers import (      80     Tokenizer,      81     Encoding,      82     AddedToken,      83     Regex,      84     NormalizedString,      85     PreTokenizedString,      86     Token,      87 )      88 from .tokenizers import decoders      89 from .tokenizers import models  ImportError: /home/user/anaconda3/lib/python3.9/lib-dynload/../../libssl.so.1.1: version `libssl.so.10' not found (required by /home/user/anaconda3/lib/python3.9/site-packages/tokenizers/tokenizers.cpython-39-x86_64-linux-gnu.so)","Jun 1, 2022 11:35 pm","jonathanli","For those who are looking for something that works, I just downgraded transformers to 4.16.2 (which is the latest version that supports tokenizers>0.10.3) E.g. with conda: conda install -c huggingface transformers==4.14.1 tokenizers==0.10.3","Jun 2, 2022 5:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Converting TikToken to Huggingface Tokenizer","https://discuss.huggingface.co/t/converting-tiktoken-to-huggingface-tokenizer/33535","1","2.5k","Created: Mar 10, 2023 8:39 pm Latest: Apr 22, 2024 2:03 pm","ncoop57","Hiya! We‚Äôve trained a model using the TikToken cl100k_base tokenizer. However, we want to convert it into a capabitible huggingface model + tokenizer. We‚Äôve got the model converted, but we aren‚Äôt sure how to go about converting the TikToken tokenizer to one that works in the huggingface ecosystem. Any help would be greatly appreciated!","Mar 10, 2023 8:39 pm","StephennFernandes","hey were you able to convert this ?","Apr 22, 2024 2:03 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Training sentencePiece from scratch?","https://discuss.huggingface.co/t/training-sentencepiece-from-scratch/3477","8","17.5k","Created: Feb 2, 2021 4:07 pm Latest: Dec 19, 2023 10:23 am","Johncwok","Hi! I would like to train a sentencePiece tokenizer from scratch but I‚Äôm a bit lost from the documentation and don‚Äôt know where to start. There are already examples on how to train a BPE tokenizer on the huggingface website but I don‚Äôt know if I can simply transfer it 1 to 1. Also I don‚Äôt even know where to find the trainable class for sentencePiece. Have you already trained a sentencePiece tokenizer?","Feb 2, 2021 4:07 pm","finiteautomata","@Johncwok check this page: Using tokenizers from ü§ó Tokenizers ‚Äî transformers 4.7.0 documentation You can train a SentencePiece tokenizer from tokenizers import SentencePieceBPETokenizer  tokenizer = SentencePieceBPETokenizer() tokenizer.train_from_iterator(     text,     vocab_size=30_000,     min_frequency=5,     show_progress=True,     limit_alphabet=500, ) and then just wrap it with a PreTrainedTokenizerFast from transformers import PreTrainedTokenizerFast  transformer_tokenizer = PreTrainedTokenizerFast(     tokenizer_object=tokenizer ) Documentation is not quite clear about this","Jul 27, 2021 2:45 pm","jbmaxwell","Sorry to jump on this old question, but I‚Äôm wondering how to properly get a tokenizer that I‚Äôve trained this way loaded from file? I‚Äôve been using the same approach you indicate here, but saving it with tokenizer.save('/path/to/tokenizer.json'), then loading it with the tokenizer_file option. It loads, but I‚Äôm noticing that it loads with all the special tokens set to None. Also, although using from transformers import PreTrainedTokenizerFast does seem to actually work, the compiler indicates that it‚Äôs supposed to be from transformers.utils.dummy_tokenizers_objects instead‚Ä¶ but that doesn‚Äôt actually seem to work. It‚Äôs all a bit muddy. Any help appreciated.","May 16, 2022 6:24 pm","jbmaxwell","Okay, what seems to give me a fully configured, functioning Transformers tokenizer is: special_tokens = [""<s>"", ""<pad>"", ""</s>"", ""<unk>"", ""<cls>"", ""<sep>"", ""<mask>""] tk_tokenizer = SentencePieceBPETokenizer() tk_tokenizer.train_from_iterator(     text,     vocab_size=4000,     min_frequency=2,     show_progress=True,     special_tokens=special_tokens ) tk_tokenizer.save(tokenizer_path) # convert tokenizer = transformers.PreTrainedTokenizerFast(tokenizer_object=tk_tokenizer, model_max_length=model_length, special_tokens=special_tokens) tokenizer.bos_token = ""<s>"" tokenizer.bos_token_id = tk_tokenizer.token_to_id(""<s>"") tokenizer.pad_token = ""<pad>"" tokenizer.pad_token_id = tk_tokenizer.token_to_id(""<pad>"") tokenizer.eos_token = ""</s>"" tokenizer.eos_token_id = tk_tokenizer.token_to_id(""</s>"") tokenizer.unk_token = ""<unk>"" tokenizer.unk_token_id = tk_tokenizer.token_to_id(""<unk>"") tokenizer.cls_token = ""<cls>"" tokenizer.cls_token_id = tk_tokenizer.token_to_id(""<cls>"") tokenizer.sep_token = ""<sep>"" tokenizer.sep_token_id = tk_tokenizer.token_to_id(""<sep>"") tokenizer.mask_token = ""<mask>"" tokenizer.mask_token_id = tk_tokenizer.token_to_id(""<mask>"") # and save for later! tokenizer.save_pretrained(""./path/to/transformers/version/"") This seems to load properly using AutoTokenizer.from_pretrained() which should make life a lot easier.","May 16, 2022 7:49 pm","appledora","Thanks for this code snippet! How did you format the dataset (text) here? One sentence per line? I was also wondering whether I could train it iteratively for multiple chunks of text input?","Jan 16, 2023 8:56 pm","jbmaxwell","You‚Äôre welcome! I know it always helps to see code. In my case it was just one line per sentence in a single flat text file.","Jan 16, 2023 9:24 pm","derVokler","Hi everyone, I found this thread when I was looking for information how to best use a custom Sentencepiece model with the huggingface models / tokenizers. I found a different solution that works for me: Train a Sentencepiece model with the Sentencepiece library Load it one time into the tokenizer that I want Save that tokenizer with .save_pretrained() After that it can be loaded with .from_pretrained(). Here are the steps with a little more detail. First, training the Sentencepiece model: import sentencepiece as spm  spm.SentencePieceTrainer.Train(     input=<FILE_NAME>,     model_prefix='spModel',     vocab_size=1000,     pad_id=0,                     unk_id=1,     bos_id=2,     eos_id=3,     pad_piece='[PAD]',     unk_piece='[UNK]',     bos_piece='[CLS]',     eos_piece='[SEP]',     user_defined_symbols='[MASK]',     model_type='unigram' ) Now load it into a tokenizer, here a DebertaV2Tokenizer (why this needs to be the vocab file, I am not sure, but it works; I also specify the maximum length here): from transformers import DebertaV2Tokenizer  tokenizer_deberta = DebertaV2Tokenizer(     vocab_file  = ""spModel.model"",     max_len = 512, ) Now save as a pretrained tokenizer: tokenizer_deberta.save_pretrained( PATH ) And from that point on you can load it as any pretrained tokenizer: tokenizer_loaded = DebertaV2Tokenizer.from_pretrained(     PATH ) When I print that guy, it looks to me like all special tokens and the sequence length are correct: DebertaV2Tokenizer(name_or_path=PATH, vocab_size=1000, model_max_length=512, is_fast=False, padding_side=‚Äòright‚Äô, truncation_side=‚Äòright‚Äô, special_tokens={‚Äòbos_token‚Äô: ‚Äò[CLS]‚Äô, ‚Äòeos_token‚Äô: ‚Äò[SEP]‚Äô, ‚Äòunk_token‚Äô: ‚Äò[UNK]‚Äô, ‚Äòsep_token‚Äô: ‚Äò[SEP]‚Äô, ‚Äòpad_token‚Äô: ‚Äò[PAD]‚Äô, ‚Äòcls_token‚Äô: ‚Äò[CLS]‚Äô, ‚Äòmask_token‚Äô: ‚Äò[MASK]‚Äô}) Maybe this still helps. I think the nice thing about this approach is that you can get a custom unigram tokenizer.","Feb 1, 2023 2:23 pm","VS9205","How did you format the dataset (text ) here?","Nov 5, 2023 11:05 am","jubueche","I couldn‚Äôt figure out how to pass an iterator to this function. Do you know how to do that? Otherwise one always has to create a file first.","Dec 19, 2023 10:23 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Special tokens warning","https://discuss.huggingface.co/t/special-tokens-warning/24939","0","2.2k","Created: Oct 25, 2022 10:55 am","antoine2323231","I am getting the following warning. Is there something I should do? ‚ÄúSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.‚Äù","Oct 25, 2022 10:55 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Mask only specific words","https://discuss.huggingface.co/t/mask-only-specific-words/173","4","3.6k","Created: Jul 10, 2020 8:47 am Latest: Nov 7, 2021 12:30 pm","Gabrer","What would be the best strategy to mask only specific words during the LM training? My aim is to mask only words of interest which I have previously collected in a list. The issue arises since the tokenizer, not only splits a single word in multiple tokens, but it also adds special characters if the word does not occur at the begging of a sentence. E.g.: The word ‚ÄúValkyria‚Äù: at the beginning of a sentences gets split as [‚ÄòV‚Äô, ‚Äòalky‚Äô, ‚Äòria‚Äô] with corresponding IDs: [846, 44068, 6374]. while in the middle of a sentence as [‚Äòƒ†V‚Äô, ‚Äòalky‚Äô, ‚Äòria‚Äô] with corresponding IDs: [468, 44068, 6374], This is just one of the issues forcing me to have multiple entries in my list of to-be-filtered IDs. I have already had a look at the mask_tokens() function into the DataCollatorForLanguageModeling class, which is the function actually masking the tokens during each batch, but I cannot find any efficient and smart way to mask only specific words and their corresponding IDs.","Jul 10, 2020 8:47 am","anthony","When using the ‚Äúfast‚Äù variant of the tokenizers available in huggingface/transformers, whenever you encode some text, you get back a BatchEncoding. This BatchEncoding provides some helpful mappings that we can use in this kind of situation. So, you should be able to: Find the word associated with any token using token_to_word. This method returns the index of the word in the input sequence. Once you know the word‚Äôs index, you can actually retrieve its span with word_to_chars. This will let you extract the word from the input sequence.","Jul 10, 2020 3:38 pm","Gabrer","Hi @Anthony, Thank you for your prompt reply! The approach you proposed, if I‚Äôm not mistaken, would be helpful in permanently masking words when reading the dataset. Instead, I am interested in dynamically masking words at batch time. To be more clear, I would like to implement (efficiently) a mask_tokens() function (as the one defined in the DataCollatorForLanguageModeling class) which masks only IDs corresponding to words provided in a specific list. These IDs would not be masked during each batch but following some stochastic strategy, unlike the approach which would mask them when the dataset is read at the beginning. I am wondering whether there is an efficient and neat way to do so, exploiting the Huggingface functions and taking also into account that some words get split in multiple IDs depending on the tokenizer at hand.","Jul 11, 2020 10:31 am","RylanSchaeffer","@Gabrer did you find a solution?","Nov 6, 2021 7:30 pm","BramVanroy","You can create your own custom mask and merge that with the automatic mask. tokenizer = AutoTokenizer.from_pretrained(""roberta-base"") word = ""Valkyria"" sentences = [""Valkyria at the beginning of sentence"", ""And here Valkyria in the middle of one.""] # Index of word in the sentences (word-tokenized!) word_idxs_in_sent = [sent.split("" "").index(word) for sent in sentences]  encoded = tokenizer(sentences, return_tensors=""pt"", padding=True) print(""Original mask"", encoded[""attention_mask""])  # For each sentence, set a subword token to False if it belongs to the word (becomes 0 in LongTensor) match_idxs = torch.LongTensor([[wid != word_idxs_in_sent[batch_idx] for wid in encoded.word_ids(batch_idx)]               for batch_idx in range(len(sentences))]) print(""Subword indices of matching word"", match_idxs)  # Merge: if a word is zero in our custom match, merge, if not, use the original mask # This ensures that we mask the word IDs but keep the original mask for special tokens (cls, pad, etc.) merged = torch.where(match_idxs == 0, match_idxs, encoded[""attention_mask""]) print(""Merged mask"", merged) Results: original mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0],         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]) Subword indices of matching word tensor([[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1],         [1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]]) Merged mask tensor([[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],         [1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])","Nov 7, 2021 12:30 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizers v0.8.0 is out!","https://discuss.huggingface.co/t/tokenizers-v0-8-0-is-out/51","0","1.5k","Created: Jul 7, 2020 6:48 pm","sgugger","Highlights of this release We can now encode both pre-tokenized inputs, and raw strings. This is especially usefull when processing datasets that are already pre-tokenized like for NER (Name Entity Recognition), and helps while applying labels to each word. Full tokenizer serialization. It is now easy to save a tokenizer to a single JSON file, to later load it back with just one line of code. That‚Äôs what sharing a Tokenizer means now: 1 line of code. With the serialization comes the compatibility with Pickle! The Tokenizer, all of its components, Encodings, everything can be pickled! Training a tokenizer is now even faster (up to 5-10x) than before! Compatibility with multiprocessing, even when using the fork start method. Since this library makes heavy use of the multithreading capacities of our computers to allows a very fast tokenization, this led to problems (deadlocks) when used with multiprocessing. This version now allows to disable the parallelism, and will warn you if this is necessary. And a lot of other improvements, and fixes. Fixed #286: Fix various crash when training a BPE model #309: Fixed a few bugs related to additional vocabulary/tokens Added #272: Serialization of the Tokenizer and all the parts (PreTokenizer, Normalizer, ‚Ä¶). This adds some methods to easily save/load an entire tokenizer (from_str, from_file). #273: Tokenizer and its parts are now pickable #289: Ability to pad to a multiple of a specified value. This is especially useful to ensure activation of the Tensor Cores, while ensuring padding to a multiple of 8. Use with enable_padding(pad_to_multiple_of=8) for example. #298: Ability to get the currently set truncation/padding params #311: Ability to enable/disable the parallelism using the TOKENIZERS_PARALLELISM environment variable. This is especially usefull when using multiprocessing capabilities, with the fork start method, which happens to be the default on Linux systems. Without disabling the parallelism, the process dead-locks while encoding. (Cf [#187] for more information) Changed Improved errors generated during truncation: When the provided max length is too low are now handled properly. #249 encode and encode_batch now accept pre-tokenized inputs. When the input is pre-tokenized, the argument is_pretokenized=True must be specified. #276: Improve BPE training speeds, by reading files sequentially, but parallelizing the processing of each file #280: Use onig for byte-level pre-tokenization to remove all the differences with the original implementation from GPT-2 #309: Improved the management of the additional vocabulary. This introduces an option normalized, controlling whether a token should be extracted from the normalized version of the input text.","Jul 7, 2020 6:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to decode with custom pad tokens","https://discuss.huggingface.co/t/how-to-decode-with-custom-pad-tokens/15504","3","4.0k","Created: Mar 8, 2022 1:15 pm Latest: Dec 22, 2023 8:47 am","pietrolesci","Hi there, I am implementing a seq2seq model. I am padding the target sequence using DataCollatorForSeq2Seq(     tokenizer=t5_tokenizer,     padding=self.padding,     label_pad_token_id=-100,     return_tensors=""pt"", ) However, when I decode the target sequence (as I want to compute BLUE score, for example) I get OverflowError: out of range integral type conversion attempted because of the -100. Is there a direct way to tokenizer.batch_decode passing a custom padding token? (the alternative would be to manually substitute -100 with 0 before decoding - as done here - but I am looking for something more straightforward, if it exists). Thanks a lot in advance!","Mar 8, 2022 1:15 pm","sheoran95","Hey @pietrolesci did you find any solution for this? I‚Äôm stuck with the same problem.","Apr 13, 2023 7:03 pm","isspek","I am getting same issue, is there any solution?","Nov 2, 2023 12:14 pm","Nevermetyou","Got same problem","Dec 22, 2023 8:47 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to add additional custom pre-tokenization processing?","https://discuss.huggingface.co/t/how-to-add-additional-custom-pre-tokenization-processing/1637","6","4.8k","Created: Oct 19, 2020 9:14 pm Latest: Mar 7, 2023 4:40 am","reSearch2vec","I would like to add a few custom functions for pre-tokenization. For example, I would like to split numerical text from any non-numerical test. Eg ‚Äò1000mg‚Äô would become [‚Äò1000‚Äô, ‚Äòmg‚Äô]. I am trying to figure out the proper way to do this for the python binding; I think it may be a bit tricky since its a binding for the original rust version. I am looking at the pretokenizer function /huggingface/tokenizers/blob/2ccd16bf5c3dd97759d7bdf5229e2feeba314b4a/bindings/python/py_src/tokenizers/pre_tokenizers/init.pyi#L6 Which I am guessing may be where I could potentially as some pretokenization functions, but it doesn‚Äôt seem to return anything. I noticed that it‚Äôs expecting an instance of the PreTokenizedString defined here /huggingface/tokenizers/blob/2ccd16bf5c3dd97759d7bdf5229e2feeba314b4a/bindings/python/py_src/tokenizers/init.pyi#L55 Which does seem to have some text processing functions. But they don‚Äôt seem to return anything. I am guessing that any additional rules need to be implemented in the original rust version itself? I am looking at the rust pretokenizers code, it seems that I have to add any additional preprocessing code here github.com huggingface/tokenizers/blob/master/tokenizers/src/pre_tokenizers/unicode_scripts/pre_tokenizer.rs use crate::pre_tokenizers::unicode_scripts::scripts::{get_script, Script}; use crate::tokenizer::{normalizer::Range, PreTokenizedString, PreTokenizer, Result};  #[derive(Clone, Debug)] pub struct UnicodeScripts; impl_serde_unit_struct!(UnicodeScriptsVisitor, UnicodeScripts);  impl UnicodeScripts {     pub fn new() -> Self {         Self {}     } }  impl Default for UnicodeScripts {     fn default() -> Self {         Self::new()     } }  // This code exists in the Unigram default IsValidSentencePiece. This file has been truncated. show original Does this seem like the right track for adding additional preprocessing code? It it makes a difference, what I am trying to do is train a brand new tokenizer.","Oct 19, 2020 9:14 pm","anthony","Hi @reSearch2vec There are multiple ways to customize the pre-tokenization process: Using existing components The tokenizers library provides many different PreTokenizer that you can use, and even combine as you wish to. There is a list of components in the official documentation Using custom components written in Python It is possible to customize some of the components (Normalizer, PreTokenizer, and Decoder) using Python code. This hasn‚Äôt been documented yet, but you can find an example here. It lets you directly manipulate the NormalizedString or PreTokenizedString to normalize and pre-tokenize as you wish. Now for the example you mentioned (ie ‚Äò1000mg‚Äô would become [‚Äò1000‚Äô, ‚Äòmg‚Äô]), you can probably use the Digits PreTokenizer that does exactly this. If you didn‚Äôt get a chance to familiarize yourself with the Getting started part of our documentation, I think you will love it as it explains a bit more how to customize your tokenizer, and gives concrete examples.","Oct 20, 2020 4:44 pm","reSearch2vec","Thanks Anthony! A lot of great info. I didn‚Äôt know the tokenizers library had official documentation , it doesn‚Äôt seem to be listed on the github or pip pages, and googling ‚Äòhuggingface tokenizers documentation‚Äô just gives links to the transformers library instead. It doesn‚Äôt seem to be on the huggingface.co main page either. Very much looking forward to reading it.","Oct 21, 2020 8:03 am","vitali","[quote=‚Äúanthony, post:2, topic:1637‚Äù] official documentation [/quote] points to localhost, could you update the link","Feb 20, 2021 5:08 am","anthony","Thanks for letting me know! Just fixed it.","Feb 20, 2021 6:21 pm","shtoshni","Hi, I was able to create a Custom Pretokenizer based on the example linked above. But I‚Äôm not able to save the tokenizer due to the exception ‚ÄúCustom PreTokenizer cannot be serialized‚Äù. I‚Äôm wondering how to bypass this.","Mar 30, 2021 3:13 pm","Davidg707","Are there plans for this to become a documented part of the API? I notice that the CustomDecoder code no longer works (I believe the method name changed), it would be great to have a stable API for this stuff (although I get it‚Äôs a pretty niche thing)","Mar 7, 2023 4:40 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding new tokens while preserving tokenization of adjacent tokens","https://discuss.huggingface.co/t/adding-new-tokens-while-preserving-tokenization-of-adjacent-tokens/12604","4","17.4k","Created: Dec 7, 2021 4:21 am Latest: Jan 25, 2024 3:46 pm","mawilson","I‚Äôm trying to add some new tokens to BERT and RoBERTa tokenizers so that I can fine-tune the models on a new word. The idea is to fine-tune the models on a limited set of sentences with the new word, and then see what it predicts about the word in other, different contexts, to examine the state of the model‚Äôs knowledge of certain properties of language. In order to do this, I‚Äôd like to add the new tokens and essentially treat them like new ordinary words (that the model just hasn‚Äôt happened to encounter yet). They should behave exactly like normal words once added, with the exception that their embedding matrices will be randomly initialized and then be learned during fine-tuning. However, I‚Äôm running into some issues doing this. In particular, the tokens surrounding the newly added tokens do not behave as expected when initializing the tokenizer with do_basic_tokenize=False. The problem can be observed in the following example; in the case of BERT, the period following the newly added token is not tokenized as a subword (i.e., it is tokenized as . instead of as the expected ##.), and in the case of RoBERTa, the word following the newly added subword is treated as though it does not have a preceding space (i.e., it is tokenized as a instead of as ƒ†a. from transformers import BertTokenizer, RobertaTokenizer  new_word = 'mynewword' bert = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize = False) bert.tokenize('mynewword') # does not exist yet # ['my', '##ne', '##w', '##word'] bert.tokenize('testing.') # ['testing', '##.']  bert.add_tokens(new_word) bert.tokenize('mynewword') # now it does # ['mynewword'] bert.tokenize('mynewword.') # ['mynewword', '.']  roberta = RobertaTokenizer.from_pretrained('roberta-base', do_basic_tokenize = False) roberta.tokenize('mynewword') # does not exist yet # ['my', 'new', 'word'] roberta.tokenize('A testing a') # ['A', 'ƒ†testing', 'ƒ†a']  roberta.add_tokens(new_word) roberta.tokenize('mynewword') # now it does # ['mynewword'] roberta.tokenize('A mynewword a') # ['A', 'mynewword', 'a'] Is there a way for me to add the new tokens while getting the behavior of the surrounding tokens to match what it would be if there were not an added token there? I feel like it‚Äôs important because the model could end up learning that (for instance), the new token can occur before ., while most others can only occur before ##. That seems like it would affect how it generalizes. In addition, I could turn on basic tokenization to solve the BERT problem here, but that wouldn‚Äôt really reflect the full state of the model‚Äôs knowledge, since it collapses the distinction between different tokens. And that doesn‚Äôt help with the RoBERTa problem, which is still there regardless. In addition, I‚Äôd ideally be able to add the RoBERTa token as ƒ†mynewword, but I‚Äôm assuming that as long as it never occurs as the first word in a sentence, that shouldn‚Äôt matter.","Dec 7, 2021 4:21 am","lewtun","Hey @mawilson if you want to add new tokens to the vocabulary, then in general you‚Äôll need to resize the embedding layers with model.resize_token_embeddings(len(tokenizer)) You can see a full example in the docs - does that help solve your problem?","Dec 7, 2021 8:31 am","mawilson","Unfortunately, it doesn‚Äôt seem to‚ÄîI‚Äôve been resizing the embedding layers using the code you provided, but that doesn‚Äôt seem to affect the behavior of the tokenizer itself, so the inputs to the model are still affected. The example in the docs you provided doesn‚Äôt seem to run into the same issue, but only because BertTokenizerFast doesn‚Äôt appear to tokenize periods as subword tokens to begin with. If I run that example but swap in BertTokenizer (with do_basic_tokenize=False) or RobertaTokenizer, the same issue is still present. (I left the models out of the code from before since the behavior doesn‚Äôt seem to rely on them, but in case it‚Äôs useful here‚Äôs an updated version of the example above that shows the same behavior.) from transformers import BertForMaskedLM, BertTokenizer, RobertaForMaskedLM, RobertaTokenizer  new_word = 'mynewword' bert_t = BertTokenizer.from_pretrained('bert-base-uncased', do_basic_tokenize = False) bert_m = BertForMaskedLM.from_pretrained('bert-base-uncased') bert_t.tokenize('mynewword') # does not exist yet # ['my', '##ne', '##w', '##word'] bert_t.tokenize('testing.') # ['testing', '##.']  bert_t.add_tokens(new_word) bert_m.resize_token_embeddings(len(bert_t)) bert_t.tokenize('mynewword') # now it does # ['mynewword'] bert_t.tokenize('mynewword.') # ['mynewword', '.']  roberta_t = RobertaTokenizer.from_pretrained('roberta-base', do_basic_tokenize = False) roberta_m = RobertaForMaskedLM.from_pretrained('roberta-base') roberta_t.tokenize('mynewword') # does not exist yet # ['my', 'new', 'word'] roberta_t.tokenize('A testing a') # ['A', 'ƒ†testing', 'ƒ†a']  roberta_t.add_tokens(new_word) roberta_m.resize_token_embeddings(len(roberta_t)) roberta_t.tokenize('mynewword') # now it does # ['mynewword'] roberta_t.tokenize('A mynewword a') # ['A', 'mynewword', 'a']","Dec 7, 2021 12:29 pm","mkarlos","Hi @mawilson I‚Äôm facing the same issue at the moment. Have you found a solution yet?","Oct 16, 2023 4:52 pm","hugosousa","Same issue here. It is also not clear if one should add the ‚Äúmynewword‚Äù to the tokenizer or ‚Äúƒ†mynewword‚Äù. Any feedback on that?","Jan 25, 2024 3:46 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WordLevel error: Missing [UNK] token from the vocabulary","https://discuss.huggingface.co/t/wordlevel-error-missing-unk-token-from-the-vocabulary/5107","4","3.1k","Created: Mar 26, 2021 12:24 am Latest: Oct 27, 2022 9:09 am","Athena","Hi, I am trying to train a basic Word Level tokenizer based on a file data.txt containing 5174 5155 4749 4814 4832 4761 4523 4999 4860 4699 5024 4788 [UNK] When I run my code from tokenizers import Tokenizer from tokenizers.models import WordLevel  tokenizer = Tokenizer(WordLevel(unk_token='[UNK]')) tokenizer.train(files=['data.txt']) tokenizer.encode('5155') I get the error Exception: WordLevel error: Missing [UNK] token from the vocabulary Why is it still missing despite having [UNK] in data.txt and also setting unk_token='[UNK]'? Any help is very appreciated!","Mar 26, 2021 12:24 am","lesscomfortable","Hi Athena, I‚Äôm having the same issue‚Ä¶ did you find the root of the problem?","Jun 17, 2021 8:48 pm","echizen","I am experiencing this too","Aug 26, 2021 8:02 pm","antoine2323231","Having the same issue‚Ä¶","Sep 21, 2022 10:50 am","lianghsun","from tokenizers import Tokenizer from tokenizers.models import WordLevel from tokenizers.trainers import WordLevelTrainer  tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))  ########## Specify [UNK] here ############ trainer = WordLevelTrainer(     special_tokens=['[UNK]'] ) ##########################################  files = ['./datasets/AAABBBCCC.txt'] tokenizer.train(files, trainer) # <--- specify trainer tokenizer.encode('41').ids","Oct 27, 2022 9:09 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to employ different vocabs for encoder and decoder respectively?","https://discuss.huggingface.co/t/how-to-employ-different-vocabs-for-encoder-and-decoder-respectively/11497","0","670","Created: Nov 9, 2021 8:37 am Latest: Nov 9, 2021 8:43 am","yijiang","In my case, the target tokens are only a small subset of the entire vocab file so it would be beneficial to use a smaller vocab in the decoding process in terms of the size reduction of the final projection layer of decoder and the efficiency increase of the softmax layer. To achieve this, I suppose that the decoder needs a tokenizer with a smaller vocab and as a result, the embedding layer of the decoder will also have a smaller size. Is there any existing way to implement this? If not, do I need to train a specific tokenizer on my own and replace the pretrained decoder-embedding layer?","Nov 9, 2021 8:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SOLVED: Module ‚Äònumpy‚Äô has no attribute ‚Äòobject‚Äô. `np.object` was a deprecated alias for the builtin `object`. for train_dataset.map(tokenize, batched=True) in notebook","https://discuss.huggingface.co/t/solved-module-numpy-has-no-attribute-object-np-object-was-a-deprecated-alias-for-the-builtin-object-for-train-dataset-map-tokenize-batched-true-in-notebook/69669","1","7.5k","Created: Jan 18, 2024 5:05 pm Latest: Jan 18, 2024 6:08 pm","a-rhodes","Hey everyone, the title pretty much says it all. I am using a SageMaker notebook and my kernel is conda_pytorch_p310. When ever I run train_dataset.map(tokenize, batched=True) I get the error message below. I tried trouble shooting this on my own and pip installing an older version of numpy but I‚Äôm still not able to resolve this. Any input is appreciated. AttributeError: module 'numpy' has no attribute 'object'. `np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe.  The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:     https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations edit: i read through the discord, the solution is to pip install datasets=2.15","Jan 18, 2024 5:05 pm","Jamesshaw","It looks like you‚Äôve encountered an issue related to the version of the datasets library and its compatibility with the version of numpy. The error message is indicating that there‚Äôs a problem with the np.object attribute. The suggested solution you mentioned is to install version 2.15 of the datasets library. You can do this using the following command in your SageMaker notebook: bashCopy code !pip install datasets==2.15 This will install the specified version of the datasets library, and it should help resolve the issue you‚Äôre facing. After installing the correct version, you can try running your code again. Keep in mind that library versions can sometimes introduce breaking changes or incompatibilities with other libraries, so specifying a version that is known to work with your current environment can be crucial. If you encounter any further issues or have specific requirements for library versions, you may need to check the documentation of the libraries you are using or seek further assistance from the community or support channels related to the specific libraries involved.","Jan 18, 2024 5:59 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Loading pretrained SentencePiece tokenizer from Fairseq","https://discuss.huggingface.co/t/loading-pretrained-sentencepiece-tokenizer-from-fairseq/1326","5","6.1k","Created: Oct 1, 2020 7:15 am Latest: Oct 21, 2020 12:55 pm","proxyht","Hello. I have a pretrained RoBERTa model on fairseq, which contains dict.txt, model.pt, sentencepiece.bpe.model. I have found a way to convert a fairseq checkpoint to huggingface format in https://github.com/huggingface/transformers/blob/master/src/transformers/convert_roberta_original_pytorch_checkpoint_to_pytorch.py Howerver, I couldn‚Äôt find a similar method to convert the tokenizer in fairseq sentencepiece.bpe.model to huggingface‚Äôs format. Is there any existing solution? Or do I have to convert it by myself? Thanks.","Oct 1, 2020 7:15 am","gurvinder","@proxyht were you able to convert sentencepiece model to huggingface tokenizer. As I am facing similar issues as well.","Oct 7, 2020 9:01 am","proxyht","A colleague of mine has figured out a way to work around this issue. Although both Huggingface and Fairseq use spm from google, the tokenizer in Fairseq map the id from spm to the token id in the dict.txt file, while Huggingface‚Äôs does not We will have to write a custom Tokenizer in Huggingface to simulate the behavior as in Fairseq","Oct 7, 2020 4:16 pm","gurvinder","Thanks. I will look into creating the custom tokenizer then using sentence piece model and tokenize data before passing it to training.","Oct 7, 2020 8:12 pm","gurvinder","@proxyht seems like new version released today added support for loading sentenncepiece model. Details in this PR https://github.com/huggingface/tokenizers/pull/292 Haven‚Äôt tested myself yet. Plan to test in a week or so. Let me know how it goes if you test before.","Oct 9, 2020 6:52 pm","gurvinder","For reference in tokenizer version 0.9.2 there is a function to load spm model. You need to run these two commands before to install the dependencies pip install protobuf wget https://raw.githubusercontent.com/google/sentencepiece/master/python/sentencepiece_model_pb2.py then you can instantiate huggingface tokenizer using pretrain sentencepiece model as tok = tokenizers.SentencePieceUnigramTokenizer.from_spm(""spm.model"")","Oct 21, 2020 12:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Sentence splitting","https://discuss.huggingface.co/t/sentence-splitting/5393","7","29.5k","Created: Apr 9, 2021 3:02 pm Latest: Sep 15, 2022 9:57 pm","carschno","I am following the Trainer example to fine-tune a Bert model on my data for text classification, using the pre-trained tokenizer (bert-base-uncased). In all examples I have found, the input texts are either single sentences or lists of sentences. However, my data is one string per document, comprising multiple sentences. When I inspect the tokenizer output, there are no [SEP] tokens put in between the sentences, e.g.: This is how I tokenize my dataset: def encode(examples):   return tokenizer(examples['text'], truncation=True, padding='max_length')     train_dataset = train_dataset.map(encode, batched=True) And this is an example result of the tokenization: tokenizer.decode(train_dataset[0][""input_ids""])  [CLS] this is the first sentence . this is the second sentence. [SEP] Given the special tokens in the beginning and the end, and that the output is lower-cased, I see that the input has been tokenized as expected. However, I was expecting to see a [SEP] between each sentence, as is the case when the input comprises a list of sentences. What is the recommended approach? Should I split the input documents into sentences, and run the tokenizer on each of them? Or does the Transformer model handle the continuous stream of sentences? I have seen posts like this: Split document into sentences for sentence embedding Just use a parser like stanza or spacy to tokenize/sentence segment your data. This is typically the first step in many NLP tasks. And: Summarization on long documents The disadvantage is that there is no sentence boundary detection. You can theoretically solve that with the NLTK (or SpaCy) approach and splitting sentences. However, it is not clear to me if this applied for a standard pipeline.","Apr 9, 2021 3:02 pm","lewtun","hey @carschno, how long are your documents (on average) and what kind of performance do you get with the current approach (i.e. tokenizing + truncating the whole document)? if performance is a problem then since you‚Äôre doing text classification, you could try chunking each document into smaller passages with a sliding window (see this tutorial for details on how that‚Äôs done), and then aggregate the [CLS] representations for each window in a manner similar to this paper. it‚Äôs not the most memory efficient strategy, but if your documents are not super long it might be a viable alternative to simple truncation","Apr 9, 2021 4:08 pm","carschno","Hi @lewtun , Thanks for the suggestion, I think the sliding window approach looks very promising indeed. My documents are body texts from scraped web pages, so the length varies widely, from a few tokens till very long texts. Current performance of the fine-tuned classifier is around 0.8-0.9 accuracy (2 classes). I‚Äôll have to look deeper into whether this is good enough for my application, and do more data analysis. Anyway for clarification: i understand the tokenizer behavior I described above is expected and the Bert model is supposed to handle input texts with multiple sentences in a single string well, right?","Apr 10, 2021 12:00 pm","lewtun","carschno: Anyway for clarification: i understand the tokenizer behavior I described above is expected and the Bert model is supposed to handle input texts with multiple sentences in a single string well, right? Yes you‚Äôre totally right . From the tokenizer‚Äôs perspective, it doesn‚Äôt matter if the input string is composed of one or more sentences - it will split it into words/subwords according to the underlying tokenization algorithm (WordPiece in BERT‚Äôs case). In case you want to see the tokens directly, you can use the tokenizer‚Äôs convert_ids_to_tokens function on the input_ids returned by the tokenizer","Apr 12, 2021 8:57 pm","carschno","I actually used the tokenizer‚Äôs decode method on the tokenized dataset: tokenizer.decode(train_dataset[0][""input_ids""])  [CLS] <...> [SEP] [SEP] [PAD] [PAD] <...> I suppose both are the same.","Apr 13, 2021 7:14 am","lewtun","they‚Äôre similar, but convert_ids_to_tokens let‚Äôs you see the subwords, e.g.: tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-uncased"") txt = ""The Higgs field gives mass to subatomic particles"" txt_enc = tokenizer(txt)  tokenizer.convert_ids_to_tokens(txt_enc.input_ids) # returns ['[CLS]', 'the', 'hi', '##ggs', 'field', 'gives', 'mass', 'to', 'sub', '##ato', '##mic', 'particles', '[SEP]'] tok.decode(txt_enc.input_ids) # returns ""[CLS] the higgs field gives mass to subatomic particles [SEP]"" the decode method combines the subwords to produce a single string and adds the special tokens as in your example. i find both method quite handy for debugging!","Apr 13, 2021 9:31 am","hhaven","From the tokenizer‚Äôs perspective, it doesn‚Äôt matter if the input string is composed of one or more sentences - it will split it into words/subwords according to the underlying tokenization algorithm (WordPiece in BERT‚Äôs case). Am I correct that if I have data that doesn‚Äôt exceed max_seq_length but each observation consists of 2 or more sentences, establishing the sentence separation isn‚Äôt necessary (BERT, DistilBERT)? I guess I‚Äôm confused about add_special_tokens=True option in case if sentence separation doesn‚Äôt matter.","Jun 3, 2021 1:49 pm","Kwame","Sorry for necro posting but just wanted to point out that pyBSD is a great library for sentence tokenization. Be sure to set clean=True when using the Segmenter class.","Sep 15, 2022 9:57 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Add new tokens and learn the embeddings of the new tokens and keeping all the other parametes frozen","https://discuss.huggingface.co/t/add-new-tokens-and-learn-the-embeddings-of-the-new-tokens-and-keeping-all-the-other-parametes-frozen/5896","0","451","Created: Apr 30, 2021 11:35 pm","nrjvarshney","I want to add new tokens and learn their embeddings (by finetuning on a dataset) and keeping all the other parameters (including the already existing embeddings) fixed. Essentially, I want to fine-tune the embeddings of the new tokens on a dataset by keeping the model parameters same. How can I achieve that?","Apr 30, 2021 11:35 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to truncate from the head in AutoTokenizer?","https://discuss.huggingface.co/t/how-to-truncate-from-the-head-in-autotokenizer/676","2","4.6k","Created: Aug 11, 2020 1:59 pm Latest: Sep 26, 2020 11:44 am","acarjasdfy","When we are tokenizing the input like this. If the text token number exceeds set max_lenth, the tokenizer will truncate from the tail end to limit the number of tokens to the max_length. tokenizer = AutoTokenizer.from_pretrained('MODEL_PATH') inputs = tokenizer(text, max_length=max_length, truncation=True,                                 padding=True, return_tensors='pt') Is there a way to change the behavior and truncate from the head end? For example if text = ['cat', 'dog', 'human'] and max_length=2, currently the last word 'human' will be dropped, is it possible to add a truncation_end parameter to AutoTokenizer, and when truncation_end='head' drops 'cat' from the head of the sentence?","Aug 11, 2020 1:59 pm","rbint","I‚Äôm not sure if there is a built in way to do this, but are you able to reverse the list before calling the tokenizer? I.e., change text to [‚Äòhuman‚Äô, ‚Äòdog‚Äô, ‚Äòcat‚Äô] then call the tokenizer so cat is dropped from the tail.","Aug 11, 2020 5:49 pm","Karthik12","I was wondering if you would want to manipulate the result after you tokenize the text, as below: from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(‚Äòbert-base-uncased‚Äô) encoded_sent = tokenizer.encode_plus (text=‚Äòcat dog human‚Äô, truncation=False, padding=False) print(‚ÄòBefore Truncation: \n‚Äô) print(encoded_sent) print(tokenizer.convert_ids_to_tokens(encoded_sent[‚Äòinput_ids‚Äô])) print(‚Äô\nAfter truncation: \n‚Äô) #If maxlen greater than text size, use ‚ÄòPAD‚Äô token #If maxlen lesser than text size, truncate the first token #Remember, we reserve two tokens for [CLS] and [SEP] as we use BERT here maxlen =10 ids = encoded_sent[‚Äòinput_ids‚Äô] if len(ids)>=maxlen: ids = [ids[0]] + ids[2:maxlen] + [102] else: ids = ids + ([0] * (maxlen-len(ids))) encoded_sent[‚Äòinput_ids‚Äô]=ids ids = encoded_sent[‚Äòtoken_type_ids‚Äô] if len(ids)>=maxlen: ids = [ids[0]] + ids[2:maxlen] + [0] else: ids = ids + ([0] * (maxlen-len(ids))) encoded_sent[‚Äòtoken_type_ids‚Äô]=ids ids = encoded_sent[‚Äòattention_mask‚Äô] if len(ids)>=maxlen: ids = [ids[0]] + ids[2:maxlen] + [1] else: ids = ids + ([0] * (maxlen-len(ids))) encoded_sent[‚Äòattention_mask‚Äô]=ids print(encoded_sent) print(tokenizer.convert_ids_to_tokens(encoded_sent[‚Äòinput_ids‚Äô]))","Sep 26, 2020 11:44 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DNA long sequence tokenization","https://discuss.huggingface.co/t/dna-long-sequence-tokenization/16154","2","2.5k","Created: Mar 28, 2022 9:13 am Latest: Aug 6, 2023 3:08 am","mdelas","Hello everyone! I am struggling with transformers in DNA data for a supervised binary classification problem. I have very long DNA sequences (the mean is 6E7 characters) and, to be able to pass longer sequences as input to the Neural Network, I am trying to tokenize using different algorithms to work with longer sequences tokens rather than only (C, G, A, T) ones. At the moment I am trying with HuggingFace to implement BPE, WordPiece, and Unigram algorithms. However, before training those models I do have to apply a pretokenizer to my data. All of them are based into ‚Äúclassic‚Äù language structures like Whitespace() but in my case I only have a list of DNA sequences like (small chunk): ['CCAGCAGCTCGGTGCGCTTGCCGCTCCAGTCGCCCAGCAGCTCGGTGCGCTTGCCGCCCCAGTCGC'] My intention is to group those characters to work with bigger tokens than only 1 single character. However, when I use for example Whitespace() , my model does not learn‚Ä¶ Could you recommend me some pre_tokenizer for passing as input to BPE, WPiece and UNIGRAM only characters? Also, would you recommend padding sequence before or after tokenization process? Thank you very much","Mar 28, 2022 9:13 am","ratish","Hello @mdelas, I was wondering if you got the solution to your problem? I have a similar doubt, I am currently working with the Bacteria Genome sequences for which I need to Pre-Train a Model from Scratch. Thanks in advance!","Jun 29, 2023 6:17 pm","marisming","BPE just combine the short segments to long segment, So the pre tokenization is not necessary. If you want, k-mers method could be used. For example ‚ÄúATCG‚Äù is segmented into ATC, TCG by 3-mers method. The k could be 6-13. dnabert model just use this method. There are also some pretained BPE tokenizer in huggingface, for example: from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('dnagpt/human_gpt2-v1') #AIRI-Institute/gena-lm-bert-base,zhihan1996/DNABERT-2-117M tokenizer.tokenize(""GAGCACATTCGCCTGCGTGCGCACTCACACACACGTTCAAAAAGAGTCCATTCGATTCTGGCAGTAG"") #result: [G','AGCAC','ATTCGCC',....]   for classification problem, here is some examples: https://huggingface.co/spaces/dnagpt/dnabert_pretrain_v1/blob/main/dnagpt/class6.ipynb   https://github.com/AIRI-Institute/GENA_LM/blob/main/notebooks/GENA_sequence_classification_example.ipynb","Aug 6, 2023 3:08 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Add BOS and EOS when encoding a sentence","https://discuss.huggingface.co/t/add-bos-and-eos-when-encoding-a-sentence/21833","2","12.9k","Created: Aug 19, 2022 1:39 pm Latest: Aug 22, 2022 5:18 pm","alexgrishin","I‚Äôm training a tokenizer from scratch. I‚Äôm using BPE tokenizer with ByteLevel pre-tokenizer. How do I add [BOS] at the beginning of each sentence and [EOS] at the end? I can do it manually, of course, but is there a way to tell encoder do it automatically? It seems that this tokenizer with this pre-tokenizer do actually add the same token at the end of each sentence (token ‚Äúƒä‚Äù with token_id=163). I would prefer to have control over how the end of sentence token looks like and its id. How do I do that?","Aug 19, 2022 1:39 pm","lianghsun","Hi @alexgrishin, is the following code what you desire to be? # I use tutorial code from https://huggingface.co/docs/tokenizers/quicktour as example from tokenizers import Tokenizer from tokenizers.models import BPE from tokenizers.trainers import BpeTrainer from tokenizers.pre_tokenizers import Whitespace  tokenizer = Tokenizer(BPE(unk_token=""[UNK]"")) trainer = BpeTrainer(special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]"", ""[BOS]"", ""[EOS]""]) #  Adding [BOS] and [EOS] here tokenizer.pre_tokenizer = Whitespace()  # Please using TemplateProcessing # https://huggingface.co/docs/tokenizers/api/post-processors#tokenizers.processors.TemplateProcessing from tokenizers.processors import TemplateProcessing tokenizer.post_processor = TemplateProcessing(     single=""[BOS] $A [EOS]"",     special_tokens=[(""[BOS]"", 1), (""[EOS]"", 2)], ) ##################################################  files = [f""datasets/wikitext-103-raw/wiki.{split}.raw"" for split in [""test"", ""train"", ""valid""]] tokenizer.train(files, trainer)  output = tokenizer.encode(""Hello, y'all! How are you üòÅ ?"") print(output.ids) # >> [1, 27255, 18, 95, 13, 5099, 7, 7963, 5114, 6220, 0, 37, 2] <-- you can see there are token [1] in the begining and token [2] at the end of the sequence print(tokenizer.decode(output.ids)) # >> no [BOS] and [EOS] after decoding","Aug 22, 2022 3:07 am","alexgrishin","Thank you! That‚Äôs exactly what I was after!","Aug 22, 2022 5:18 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AutoTokenizer is very slow when loading llama tokenizer","https://discuss.huggingface.co/t/autotokenizer-is-very-slow-when-loading-llama-tokenizer/41547","2","1.7k","Created: May 31, 2023 2:38 am Latest: Oct 31, 2023 5:39 am","coincheung","Code is like this:     tokenizer = LlamaTokenizer.from_pretrained('decapoda-research/llama-13b-hf', use_fast=True)     tokenizer.save_pretrained('./llama_tok')      tokenizer = LlamaTokenizer.from_pretrained('./llama_tok/') # very fast to load     tokenizer = AutoTokenizer.from_pretrained('./llama_tok/') # very slow to load What is the cause of this and how could I fix it please?","May 31, 2023 2:38 am","asbljy","I also found this issue.","Jun 22, 2023 10:34 pm","reece124","Same issue here also for loading Vicuna tokenizers. Got it workaround by replacing AutoTokenizer with LlamaTokenizer.","Oct 31, 2023 5:39 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to tokenize large contexts without running out of memory","https://discuss.huggingface.co/t/how-to-tokenize-large-contexts-without-running-out-of-memory/5882","2","1.5k","Created: Apr 30, 2021 11:19 am Latest: Aug 8, 2022 8:26 am","samuel","I am currently trying to tokenize the Squad-dataset for finetuning the Reformer-model. The Problem I have is that the Reformer-Model needs a fixed imput-length and the pre-trained one there is needs ca. 500000 tokens. My Idea was to just fuse together the Context of the Wiki-artikles and pad the rest to get to the token threshold. The issue i have run into now is i can only at most tokenize 500 examples at a time or else i run out of RAM (64 Gig). Is there a Possibility to use the Tokenizer on smaller subsets of the Data and then merge the resulting Batch-encodings afterwards? Would it be possible to just append the attentionmasks and the input_ids?","Apr 30, 2021 11:19 am","Jiraya","Hi Samuel, I am facing a similar issue wherein I am running out of memory while attempting to tokenize 28M datapoints with 64GB ram. Were you able to solve this problem? If yes, could you please let me know how? Thank you, Pradyuman","Aug 8, 2022 1:11 am","samuel","Hi sadly i wasnt able to do it, but I managed to train a reformer model on a smaller Input with a different tokenizer. If just a little bit of RAM is missing and you are using a Linux based system, you could make a big swap partition to double your RAM, however this will slow down the processing considerably depending on how fast your disk/ssd is.","Aug 8, 2022 8:26 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Token indices sequence length is longer than the specified maximum sequence length","https://discuss.huggingface.co/t/token-indices-sequence-length-is-longer-than-the-specified-maximum-sequence-length/25133","4","19.8k","Created: Oct 28, 2022 1:11 pm Latest: Feb 15, 2023 10:41 pm","antoine2323231","Hi, when running the run_t5_mlm_flax.py script I am getting this error: Token indices sequence length is longer than the specified maximum sequence length for this model (523 > 512). Running this sequence through the model will result in indexing errors. I have specified model_max_length =512 within the tokenizer. And passed --max_seq_length=‚Äú512‚Äù \ to the run_t5_mlm_flax.py script. Unfortunately I still get the same warning.","Oct 28, 2022 1:11 pm","lianghsun","Hi @antoine2323231 , can you try the following code to see if it works? tokenizer(batch_sentences, padding='max_length', truncation=True)","Oct 28, 2022 11:11 pm","antoine2323231","lianghsun: tokenizer(batch_sentences, padding='max_length', truncation=True) Hey lianghsun, I tried that but getting the same result. It is strange‚Ä¶","Oct 30, 2022 7:43 am","bobbytonylowe-rgdl","I am also getting a similar error. Did you resolve this?","Dec 19, 2022 5:21 pm","tq2000","My walk-around is to reduce the length of the prompt. For example, if you are doing question answering, the context + question should be short than 512 tokens.","Feb 15, 2023 10:41 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Padding with sliding window","https://discuss.huggingface.co/t/padding-with-sliding-window/18921","1","2.5k","Created: Jun 9, 2022 4:11 pm Latest: Sep 3, 2022 9:45 pm","davibarreira","Hello, friends. I‚Äôm new to the forum, so if there is some issue with this question, please let me know. So, I want to tokenize a long sequence, and I‚Äôm trying to use the sliding window option. tokenizer = AutoTokenizer.from_pretrained('../../data/model/', do_lower_case=False) tokenized_example = tokenizer(data[0], return_overflowing_tokens=True, max_length=512) Everything worked fine, but I then wish to obtain the tensor for such encoding. tokenized_example = tokenizer(data[0], return_overflowing_tokens=True, max_length=512,  return_tensors=""pt"") Now, this returns an error, because the last window is not padded, hence, instead of 512 tokens, it has 490. Ok, so I need padding, but tokenized_example = tokenizer(data[0], padding='max_length', return_overflowing_tokens=True, max_length=512,  return_tensors=""pt"") The code above does not work, because it seems that the window sliding is not sliding when we have the padding activated. Hence, instead of, say, 12 tensors of 512, I have only one tensor of 6000. How can one circumvent this?","Jun 9, 2022 4:11 pm","lucasresck","Opa, e a√≠, Davi? I also had an undesirable tensor shape with these arguments: tokenized = tokenizer(     ""test ""*1000,     return_overflowing_tokens=True,     max_length=512,     return_tensors='pt',     padding='max_length', ) print(tokenized['input_ids'].shape) >> torch.Size([1, 2002]) Try explicitly activating the truncation: tokenized = tokenizer(     ""Test ""*1000,     return_overflowing_tokens=True,     max_length=512,     return_tensors='pt',     padding='max_length',     truncation=True, ) print(tokenized['input_ids'].shape) >> torch.Size([4, 512]) To fully benefit from a sliding window, also try the stride parameter. It controls the overlap between two consecutive ‚Äúwindows.‚Äù","Sep 3, 2022 9:45 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer shrinking recipes","https://discuss.huggingface.co/t/tokenizer-shrinking-recipes/8564","7","2.2k","Created: Jul 21, 2021 6:29 pm Latest: Dec 24, 2023 4:54 am","stas","As I‚Äôve building tiny-models for hf-internal-testing (Hugging Face Internal Testing Organization) I need to shrink/truncate the original tokenizers and the vocab in order to get truly tiny models and often it was taking quite a long time to figure out. So I reached out for help and got several great recipes which I thought I‚Äôd share here in case others might need something similar. Anthony Moi‚Äôs version @anthony‚Äôs tokenizer shrinker: import json from transformers import AutoTokenizer from tokenizers import Tokenizer  vocab_keep_items = 5000 mname = ""microsoft/deberta-base""  tokenizer = AutoTokenizer.from_pretrained(mname, use_fast=True) assert tokenizer.is_fast, ""This only works for fast tokenizers."" tokenizer_json = json.loads(tokenizer._tokenizer.to_str()) vocab = tokenizer_json[""model""][""vocab""] if tokenizer_json[""model""][""type""] == ""BPE"":     new_vocab = { token: i for token, i in vocab.items() if i < vocab_keep_items }     merges = tokenizer_json[""model""][""merges""]     new_merges = []     for i in range(len(merges)):         a, b = merges[i].split()         new_token = """".join((a, b))         if a in new_vocab and b in new_vocab and new_token in new_vocab:             new_merges.append(merges[i])     tokenizer_json[""model""][""merges""] = new_merges elif tokenizer_json[""model""][""type""] == ""Unigram"":     new_vocab = vocab[:vocab_keep_items] elif tokenizer_json[""model""][""type""] == ""WordPiece"" or tokenizer_json[""model""][""type""] == ""WordLevel"":     new_vocab = { token: i for token, i in vocab.items() if i < vocab_keep_items } else:     raise ValueError(f""don't know how to handle {tokenizer_json['model']['type']}"") tokenizer_json[""model""][""vocab""] = new_vocab tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json)) tokenizer.save_pretrained(""."") LysandreJik‚Äôs version Using the recently added train_new_from_iterator suggested by @lysandre from transformers import AutoTokenizer  mname = ""microsoft/deberta-base"" # or any checkpoint that has a fast tokenizer. vocab_keep_items = 5000  tokenizer = AutoTokenizer.from_pretrained(mname) assert tokenizer.is_fast, ""This only works for fast tokenizers."" tokenizer.save_pretrained(""big-tokenizer"") # Should be a generator of list of texts. training_corpus = [     [""This is the first sentence."", ""This is the second one.""],     [""This sentence (contains #) over symbols and numbers 12 3."", ""But not this one.""], ] new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=vocab_keep_items) new_tokenizer.save_pretrained(""small-tokenizer"") but this one requires a training corpus, so I had an idea to cheat and train the new tokenizer on its own original vocab: from transformers import AutoTokenizer  mname = ""microsoft/deberta-base"" vocab_keep_items = 5000  tokenizer = AutoTokenizer.from_pretrained(mname) assert tokenizer.is_fast, ""This only works for fast tokenizers."" vocab = tokenizer.get_vocab() training_corpus = [ vocab.keys() ] # Should be a generator of list of texts. new_tokenizer = tokenizer.train_new_from_iterator(training_corpus, vocab_size=vocab_keep_items) new_tokenizer.save_pretrained(""small-tokenizer"") which is almost perfect, except it now doesn‚Äôt have any information about the frequency for each word/char (that‚Äôs how most tokenizers compute their vocab, which if you need this info you can fix by having each key appearing len(vocab) - ID times, i.e.: training_corpus = [ (k for i in range(vocab_len-v)) for k,v in vocab.items() ]  which will make the script much much longer to complete. But for the needs of a tiny model (testing) the frequency doesn‚Äôt matter at all. hack the tokenizer file version Some tokenizers can be be just manually truncated at the file level, e.g. Electra: # Shrink the orig vocab to keep things small (just enough to tokenize any word, so letters+symbols) # ElectraTokenizerFast is fully defined by a tokenizer.json, which contains the vocab and the ids, so we just need to truncate it wisely import subprocess from transformers import ElectraTokenizerFast  mname = ""google/electra-small-generator"" vocab_keep_items = 3000  tokenizer_fast = ElectraTokenizerFast.from_pretrained(mname) tmp_dir = f""/tmp/{mname}"" tokenizer_fast.save_pretrained(tmp_dir) # resize tokenizer.json (vocab.txt will be automatically resized on save_pretrained) # perl -pi -e 's|(2999).*|$1}}}|' tokenizer.json # 0-indexed, so vocab_keep_items-1! closing_pat = ""}}}"" cmd = (f""perl -pi -e s|({vocab_keep_items-1}).*|$1{closing_pat}| {tmp_dir}/tokenizer.json"").split() result = subprocess.run(cmd, capture_output=True, text=True) # reload with modified tokenizer tokenizer_fast_tiny = ElectraTokenizerFast.from_pretrained(tmp_dir) tokenizer_fast_tiny.save_pretrained(""."") spm vocab shrinking First clone sentencepiece into a parent dir: git clone https://github.com/google/sentencepiece now to the shrinking # workaround for fast tokenizer protobuf issue, and it's much faster too! os.environ[""PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION""] = ""python""  from transformers import XLMRobertaTokenizerFast  mname = ""xlm-roberta-base""  # Shrink the orig vocab to keep things small vocab_keep_items = 5000 tmp_dir = f""/tmp/{mname}"" vocab_orig_path = f""{tmp_dir}/sentencepiece.bpe.model"" # this name can be different vocab_short_path = f""{tmp_dir}/spiece-short.model"" # HACK: need the sentencepiece source to get sentencepiece_model_pb2, as it doesn't get installed sys.path.append(""../sentencepiece/python/src/sentencepiece"") import sentencepiece_model_pb2 as model tokenizer_orig = XLMRobertaTokenizerFast.from_pretrained(mname) tokenizer_orig.save_pretrained(tmp_dir) with open(vocab_orig_path, 'rb') as f: data = f.read() # adapted from https://blog.ceshine.net/post/trim-down-sentencepiece-vocabulary/ m = model.ModelProto() m.ParseFromString(data) print(f""Shrinking vocab from original {len(m.pieces)} dict items"") for i in range(len(m.pieces) - vocab_keep_items): _ = m.pieces.pop() print(f""new dict {len(m.pieces)}"") with open(vocab_short_path, 'wb') as f: f.write(m.SerializeToString()) m = None  tokenizer_fast_tiny = XLMRobertaTokenizerFast(vocab_file=vocab_short_path) tokenizer_fast_tiny.save_pretrained(""."") If you have other related recipes please don‚Äôt hesitate to add those in the comments below. p.s. if you create custom models that are derivations from original ones if possible please upload the script that created the derivative with the model files, so that in the future it‚Äôs easy to update or replicate or adapt to other models. e.g. make-tiny-deberta.py ¬∑ hf-internal-testing/tiny-deberta at main created hf-internal-testing/tiny-deberta ¬∑ Hugging Face.","Jul 21, 2021 6:29 pm","stas","gpt2 seems to have a special token ""<|endoftext|>"" stashed at the very end of the vocab, so it gets dropped and code breaks. So I hacked it back in with:     if ""gpt2"" in mname:         new_vocab = { token: i for token, i in vocab.items() if i < vocab_keep_items-1 }         new_vocab[""<|endoftext|>""] = vocab_keep_items-1     else:         new_vocab = { token: i for token, i in vocab.items() if i < vocab_keep_items }","Aug 12, 2021 8:49 pm","KhaiKit","stas:     for i in range(len(merges)):         a, b = merges[i].split()         new_token = """".join((a, b))         if a in new_vocab and b in new_vocab and new_token in new_vocab:             new_merges.append(merges[i]) Hi thank you for sharing such useful information! With reference to anthony‚Äôs tokenizer shrinker, do you know how I can instead add tokens to my BPE model?","Dec 22, 2023 3:35 pm","stas","like you‚Äôd do with any other tokenizer? tokenizer.add_tokens()","Dec 22, 2023 6:32 pm","KhaiKit","Hi I‚Äôve tried that method but it seems to produce an output that I did not intend, not sure if it is a bug that needs to be fixed. For context, I am originally trying to add Chinese tokens to the tokenizer but for illustration purposes, I will demonstrate the ‚Äúbug‚Äù in English. Chinese words are not separated by spaces and hence in the example you will see me trying to add a token that is a subword. The code to reproduce the bug would be: from transformers import AutoTokenizer  checkpoint = ""facebook/nllb-200-distilled-600M"" tokenizer = AutoTokenizer.from_pretrained(checkpoint, src_lang = ""eng_Latn"", tgt_lang = ""zho_Hans"") tokenizer.add_tokens([""abcd""])  sent = 'I like to walk abcdgym along the beach' print(""tokenizer: "", tokenizer.tokenize(sent)) print(""tokenizer: "", tokenizer.decode(tokenizer.encode(sent)[1:-1]))  sent = 'I like to walk gymabcd along the beach' print(""tokenizer: "", tokenizer.tokenize(sent)) print(""tokenizer: "", tokenizer.decode(tokenizer.encode(sent)[1:-1])) Evidently, tokenizer.add_tokens() works well if there will always be space after the added token but it doesn‚Äôt work as intended if there isn‚Äôt space after the added token (where the tokenizer will then introduce the additional space on its own). I read the docs and figured out it is probably because the added tokens are isolated before the tokenization algorithm is applied Hence I wanted to add tokens directly to the BPE model","Dec 23, 2023 2:17 am","stas","Thank you for explaining the issue, @KhaiKit I‚Äôd recommend to file an issue and ask for supporting this type of tokens? This use case sounds like it won‚Äôt be an isolated case to me. But I could be wrong.","Dec 23, 2023 3:20 am","KhaiKit","Cool, I have just filed it at Tokenizer adds an additional space after the added token ¬∑ Issue #28218 ¬∑ huggingface/transformers ¬∑ GitHub On a side note, this issue that I pointed out can be circumvented if I add tokens directly to the Sentencepiece or BPE model. I found a neat solution introduced in a youtube video that adds tokens to a Sentencepiece model but I am not really sure how to add tokens to a BPE model without retraining it. Wondering if you could share how to? Reason being that NLLB fast tokenizer is based on BPE while the NLLB python based tokenizer is based on Sentencepiece.","Dec 23, 2023 3:49 am","stas","As I didn‚Äôt write that version of the code I haven‚Äôt delved into the details, but briefly looking at it I don‚Äôt see any reason why you shouldn‚Äôt be able to do that - have you tried? Also why can‚Äôt you just pick some long word token and replace it with your new token - assuming you supply this new tokenizer with the model, shouldn‚Äôt it just work? I haven‚Äôt tried it, so this is just an idea. Older tokenizers had it much easier - they had some 100 empty slots designed specifically for new tokens - it was trivial to extend those without needing to chnage the vocab size.","Dec 24, 2023 4:54 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does AutoTokenizer.from_pretrained add [cls] tokens?","https://discuss.huggingface.co/t/does-autotokenizer-from-pretrained-add-cls-tokens/4056","7","5.0k","Created: Mar 1, 2021 2:31 pm Latest: Mar 2, 2021 12:48 pm","theudster","Hello, I am currently working on a classification problem using ProtBERT and I am following the Fine-Tuning Tutorial. I have called the tokenised using tokenizer = AutoTokenizer.from_pretrained and then tokenised like the tutorial says train_encodings = tokenizer(seq_train, truncation=True, padding=True,  max_length=1024, return_tensors=""pt"") Unfortunately, the model doesn‚Äôt seem to be learning (I froze the BERT layers). From reading around, I saw that I need to add the [CLS] token and found such an option using tokenised.encode(add_special_tokens=True) Yet the tutorial I am following doesn‚Äôt seem to require and I was wondering wyy is there a discrepancy and perhaps maybe this is why my model isn‚Äôt learning. Thank you","Mar 1, 2021 2:31 pm","lewtun","Hi @theudster, I‚Äôm pretty sure that ProtBERT has a CLS token since you can see it in the tokenizer‚Äôs special tokens map: from transformers import AutoTokenizer, AutoModelForMaskedLM  tokenizer = AutoTokenizer.from_pretrained(""Rostlab/prot_bert"") # returns {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'} tokenizer.special_tokens_map You can also see it by encoding a text and then decoding it: text = ""I love Adelaide!"" # add_special_tokens=True is set by default text_enc = tokenizer.encode(text)  for tok in text_enc:     print(tok, tokenizer.decode(tok)) You say you froze the BERT layers, so I‚Äôm wondering how you‚Äôre doing fine-tuning? I‚Äôve sometimes found that the tutorials in the docs aren‚Äôt always complete, so for fine-tuning with text classification I would recommend following Sylvain‚Äôs tutorial here: https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb","Mar 1, 2021 3:10 pm","theudster","Thank you @lewtun. So are you saying that tokenized adds [CLS]? Also, I am freezing the BERT layer but fine-tuning the classification layer. The reason I am doing that is because Colab crashes as it runs out of GPU if I don‚Äôt. If I try using just the CPU, I get ‚ÄúYour session crashed after using all available RAM.‚Äù I therefore add for param in model.bert.parameters():         param.requires_grad = False Which fixes that, but then the model doesn‚Äôt seem to learn anything","Mar 1, 2021 3:31 pm","lewtun","Yes, the tokenizers in transformers add the special tokens by default (see the docs here). I‚Äôm not familiar with ProtBERT but I‚Äôm surprised its crashing Colab because the repo has some Colab examples: ProtTrans/ProtBert-BFD-FineTuning-MS.ipynb at master ¬∑ agemagician/ProtTrans ¬∑ GitHub If you‚Äôre still having problems fine-tuning with a GPU, perhaps you can reduce the batch size to avoid the OOM errors","Mar 1, 2021 8:38 pm","theudster","thank you @lewtun. I have gone through their implementation and noticed that their per_device_train_batch_size is 1. Once I changed that, It works and I am actually getting some amazing results (its on its final epoch now, but currently hitting 80% accuracy ). I was hoping you could perhaps explain to me what this per_device_train_batch_size does and why was that the issue?","Mar 1, 2021 10:52 pm","lewtun","the per_device_train_batch_size specifies the batch size for the device you are training on (e.g. GPU/TPU/CPU), so if your training set has 1,000 examples and and per_device_train_batch_size=1 then it will take 1,000 steps to complete one epoch. by increasing the value of per_device_train_batch_size you are able to train faster since it takes less steps to complete each epoch (e.g. if per_device_train_batch_size=4 then we only need 250 steps / epoch in our example), but this can sometimes lead to worse performance since the gradients are averaged / summed in a minibatch. in your case, my guess is that with per_device_train_batch_size=1 you need to train for a very long time to see the model learn anything.","Mar 2, 2021 10:18 am","theudster","well, on Colab, it took a bit over an hour to train 3 epochs, not sure if that is long or not","Mar 2, 2021 10:32 am","lewtun","oh what i meant by ‚Äúlong‚Äù is that you may need to run for many epochs before you start seeing any flattening out of you training / validation loss (i.e ‚Äúconvergence‚Äù). if you saw the validation loss drop during those 3 epochs then i am not sure what else might have gone wrong in your ProtBERT example","Mar 2, 2021 12:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How long to expect training to take, and guidance on subset size?","https://discuss.huggingface.co/t/how-long-to-expect-training-to-take-and-guidance-on-subset-size/35380","1","1.7k","Created: Apr 3, 2023 6:37 am Latest: May 23, 2024 5:44 pm","sradc","I‚Äôve trained a BPE tokenizer from scratch on bookcorpus+wikipedia, and it took 5.5 hours on the full dataset (it took ~1hr20min to ingest the text from the iterator). Vocab size ~30,000. Is this the kind of time frame we‚Äôd expect? (I searched the internet, but there doesn‚Äôt seem to be much info on tokenizer training times for large datasets). Is there any guidance on subsetting the data to train the tokenizer faster? I.e. what‚Äôs a recommended size of data to use? E.g. I‚Äôm thinking for now to reduce the dataset to 1/4 or something, to make the training quicker. Edit 1: I removed the normalizer I was using, and training is down to 40 minutes. This was the normalizer I removed: # Normalizer based on: # https://github.com/JonasGeiping/cramming/blob/50bd06a65a4cd4a3dd6ee9ecce1809e1a9085374/cramming/data/tokenizer_preparation.py#L52 normalizer = normalizers.Sequence(     [         normalizers.Replace(""``"", '""'),         normalizers.Replace(""''"", '""'),         normalizers.NFD(),         normalizers.Lowercase(),         normalizers.StripAccents(),         normalizers.Replace(Regex("" {2,}""), "" ""),         normalizers.Replace(             Regex(r""[^\x00-\x7F]+""), """"         ),  # start from 00 instead of 1F to include tab     ] ) Hmm. I would like to use this normalizer still, but not for it to be so slow‚Ä¶ Edit 2: Hmm, I switched to preprocessing the dataset with the normalizer (using Python‚Äôs re instead of the regex ones, which was faster). However, now when trying to train the tokenizer on the dataset it seems to not be iterating through as quickly; and although it‚Äôs using multiple cpu cores, only 1 seems to be at 100% at a time, while the others are at 1-2% (this changes over time). Whereas with the training mentioned above that took only 40 minutes, all the cores were near to 100% the whole time. Checked the iteration speed of the mapped dataset, which seems just as fast as the original dataset. Not yet sure what the issue is.","Apr 3, 2023 6:37 am","maveriq","Hi. Have you found any information on training times? I also observe that during pre-processing sequences step, only 1 core is activated. Is it expected?","May 23, 2024 5:44 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Newbie: Main difference between tokenizers?","https://discuss.huggingface.co/t/newbie-main-difference-between-tokenizers/6035","0","780","Created: May 6, 2021 5:32 pm","wusuowei60","Hi, HuggingFace community. I‚Äôm new to NLP and have learned there are many packages that can do tokenization, such as spacy, NLTK, torchtext and of course HuggingFace tokenizer. My questions are: What is the main difference between them? It seems to me that tokenizers are fast and match with corresponding models. Any difference other than that? Why do different models need different tokenizers? I heard that using wrong tokenizers may hurt performance, so tokenizers make a difference. Why is that the case, any intuition? Which should I try first? If I am to design my model (toy ones) instead of directly using provided API, which tokenizer should I try first.","May 6, 2021 5:32 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Is that possible to embed the tokenizer into the model to have it running on GCP using TensorFlow Serving?","https://discuss.huggingface.co/t/is-that-possible-to-embed-the-tokenizer-into-the-model-to-have-it-running-on-gcp-using-tensorflow-serving/10532","4","3.2k","Created: Oct 5, 2021 3:44 pm Latest: Jan 12, 2023 12:00 am","justnoxx","Hello! Thanks in advance for your help! At the beginning I‚Äôve created an issue on github with this question: Question: Is that possible to embed a tokenizer into the model for tensorflow serving? ¬∑ Issue #13843 ¬∑ huggingface/transformers ¬∑ GitHub and I‚Äôve got a suggestion to tag @Rocketknight1 who is an expert in TensorFlow for questions like this. I already using TF-BERT model (uncased version) with tensorflow serving. I found that I need to modify some inputs to get something like that:     callable = tf.function(self.model.call)     concrete_function = callable.get_concrete_function([         tf.TensorSpec([None, self.max_input_length], tf.int32, name=""input_ids""),         tf.TensorSpec([None, self.max_input_length], tf.int32, name=""attention_mask"")     ])     self.model.save(save_directory, signatures=concrete_function) Also I found the following example (blog/tf-serving.md at master ¬∑ huggingface/blog ¬∑ GitHub), that allows me to change input signature of a model especially for serving: from transformers import TFBertForSequenceClassification import tensorflow as tf  # Creation of a subclass in order to define a new serving signature class MyOwnModel(TFBertForSequenceClassification):     # Decorate the serving method with the new input_signature     # an input_signature represents the name, the data type and the shape of an expected input     @tf.function(input_signature=[{         ""input_ids"": tf.TensorSpec((None, None), tf.int32, name=""input_ids""),         ""attention_mask"": tf.TensorSpec((None, None), tf.int32, name=""attention_mask""),         ""token_type_ids"": tf.TensorSpec((None, None), tf.int32, name=""token_type_ids""),     }])     def serving(self, inputs):         # call the model to process the inputs         output = self.call(inputs)         test_out = self.serving_output(output)         # return the formated output         return test_out   # Instantiate the model with the new serving method model = MyOwnModel.from_pretrained(""bert-base-cased"") # save it with saved_model=True in order to have a SavedModel version along with the h5 weights. model.save_pretrained(""/tmp/my_model6"", saved_model=True) In my current workflow I have a need of python because I have to prepare input for a model by using tokenizer. It does mean that for now I need to have a REST service that gets text request and then sends it to the serving instance. After I switched to GCP AI Platform I think that it is reasonable and worth to try to embed the tokenizer inside the model and let GCP AI Platform serving it. I did some tries and looks like that it more difficult than it looks like. The goal is to have the model with tokenizer on GCP AI platform and get rid of python REST API service because all other infrastructure is written using Erlang/Rust. I need to supply the text to the model serving instance (not the object with input_ids, attention_mask, etc.) and get logits. Or softmaxed logits. So could someone please answer is that possible and if it is possible to provide some guidance how to achieve this? Thanks a lot for your help! /Dmitriy","Oct 5, 2021 3:44 pm","dian","I have been looking for something like this but couldn‚Äôt find it either. Based on the blog you found blog/tf-serving.md at master ¬∑ huggingface/blog ¬∑ GitHub, the author mentioned that it‚Äôs a possible next step improvement so not sure if this is already possible.","Oct 21, 2021 2:55 pm","feeeper","Does anybody solve the problem? I‚Äôm looking for solution for some days and find nothing about embed tokenizer into model so I could serve it in TensorFlow Serving","Jun 8, 2022 7:32 am","benl","tensorflow_text.WordpieceTokenizer may be a solution.","Sep 1, 2022 9:16 am","vlasenkoalexey","See this project, looks like it was created for solving this issue: GitHub - Hugging-Face-Supporter/tftokenizers: Use Huggingface Transformer and Tokenizers as Tensorflow Reusable SavedModels","Jan 12, 2023 12:00 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tunning tokenizer on my own dataset","https://discuss.huggingface.co/t/tunning-tokenizer-on-my-own-dataset/3367","0","690","Created: Jan 25, 2021 12:09 am Latest: Jan 25, 2021 12:14 am","Miriam","I have an English-written dataset with a vocabulary that contains some words that may be missing from the standard vocabulary used for RobertaTokenizer. Hence, I‚Äôd like to include additional tokens in the tokenizer. I‚Äôd like to avoid training the tokenizer from scratch, as in such case I won‚Äôt be able to fine-tune pretrained roberta model on top of it. Since I do not know ahead what is the entire list of tokens I‚Äôd like to add, I thought I can do the following: Train a tokenizer from scratch on my new dataset, and then to look at the created vocab file and add all the new tokens (those that do not exist in the standard RobertaTokenizer vocab) via tokenizer = RobertaTokenizer.from_pretrained(""roberta-base"") tokenizer.add_tokens(list_of_new_tokens, special_tokens=True) (like described here: Huggingface BERT Tokenizer add new token - Stack Overflow). Does this approach makes sense? Or am I missing something? Is there a better way to approach this issue?","Jan 25, 2021 12:09 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"OSError: Model name ‚Äògpt2‚Äô was not found in tokenizers model name list (gpt2,‚Ä¶)","https://discuss.huggingface.co/t/oserror-model-name-gpt2-was-not-found-in-tokenizers-model-name-list-gpt2/2164","8","7.1k","Created: Nov 22, 2020 6:28 pm Latest: Aug 10, 2023 9:02 am","PabloAMC","I‚Äôm trying to replicate part of the transformers tutorial from fastai, and there is a place one writes: from transformers import GPT2LMHeadModel, GPT2TokenizerFast pretrained_weights = 'gpt2' tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights) model = GPT2LMHeadModel.from_pretrained(pretrained_weights) However, trying to run it I get --------------------------------------------------------------------------- OSError                                   Traceback (most recent call last) <ipython-input-31-b475580d46e5> in <module>       1 from transformers import GPT2LMHeadModel, GPT2TokenizerFast       2 pretrained_weights = 'gpt2' ----> 3 tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights)       4 model = GPT2LMHeadModel.from_pretrained(pretrained_weights)  /opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)    1589                     "", "".join(s3_models),    1590                     pretrained_model_name_or_path, -> 1591                     list(cls.vocab_files_names.values()),    1592                 )    1593             )  OSError: Model name 'gpt2' was not found in tokenizers model name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). We assumed 'gpt2' was a path, a model identifier, or url to a directory containing vocabulary files named ['vocab.json', 'merges.txt', 'tokenizer.json'] but couldn't find such vocabulary files at this path or url. I find this confusing because gpt2 is in the list. In fact, I encounter the same problem with any transformer model I choose, like for instance distilgpt2 or from another family. In fact, if I comment out that line I also get an error --------------------------------------------------------------------------- OSError                                   Traceback (most recent call last) /opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)     372             if resolved_config_file is None: --> 373                 raise EnvironmentError     374             config_dict = cls._dict_from_json_file(resolved_config_file)  OSError:   During handling of the above exception, another exception occurred:  OSError                                   Traceback (most recent call last) <ipython-input-32-a4869c5495d6> in <module>       2 pretrained_weights = 'gpt2'       3 #tokenizer = GPT2TokenizerFast.from_pretrained(pretrained_weights) ----> 4 model = GPT2LMHeadModel.from_pretrained(pretrained_weights)  /opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)     874                 proxies=proxies,     875                 local_files_only=local_files_only, --> 876                 **kwargs,     877             )     878         else:  /opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py in from_pretrained(cls, pretrained_model_name_or_path, **kwargs)     327      328         """""" --> 329         config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)     330         return cls.from_dict(config_dict, **kwargs)     331   /opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py in get_config_dict(cls, pretrained_model_name_or_path, **kwargs)     380                 f""- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\n\n""     381             ) --> 382             raise EnvironmentError(msg)     383      384         except json.JSONDecodeError:  OSError: Can't load config for 'gpt2'. Make sure that:  - 'gpt2' is a correct model identifier listed on 'https://huggingface.co/models'  - or 'gpt2' is the correct path to a directory containing a config.json file Everything is run on Kaggle notebooks, in case it‚Äôs important Thanks in advance!","Nov 22, 2020 6:28 pm","thomwolf","Can you try to share a Google colab reproducing the error?","Nov 23, 2020 2:37 pm","PabloAMC","Hi @thomwolf thanks for the recommendation. Actually I am quite confused because in colab (https://colab.research.google.com/drive/1gFStbsnfuo2CA9cUYc3TXqUM-wICYrfK?usp=sharing) it seems to work fine, but in kaggle (https://www.kaggle.com/pabloamc/fastai-and-transformers, cell 9) it doesn‚Äôt.","Nov 23, 2020 4:21 pm","thomwolf","maybe @abhishek (who knows a ‚Äúlittle bit‚Äù about kaggle hahah) has an idea ?","Nov 23, 2020 4:22 pm","abhishek","@PabloAMC Please turn the internet on in Kaggle Kernels. I just ran the code example above and it works fine.","Nov 24, 2020 8:37 am","PabloAMC","Hi @abhishek. You were right. Sorry for the relatively simple issue.","Nov 24, 2020 10:17 am","cdiazj88","Hi, I am new to Hugging Face and I am having the same issue with running this model on my local mac. I am using : torch==1.7.0 nltk==3.4.5 colorama==0.4.4 transformers==3.4.0 torchtext==0.3.1 Could any one please help me ?","Apr 23, 2022 2:12 am","Capacito","I have met the same error. I solved it because the path I stored the onnx is ./GPT2. At the beginning, I didn‚Äôt know it is case-insensitive. I just change the path name to solve this.","May 26, 2023 1:32 pm","lilsyoss","you should log in kaggle account, and in [setting] you should vertify you phone number, then reopen you notebook ,you can see internet switch on your left screen ~~~~","Aug 10, 2023 9:02 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Exporting tokenizer to an onnx model","https://discuss.huggingface.co/t/exporting-tokenizer-to-an-onnx-model/15467","1","1.4k","Created: Mar 7, 2022 7:46 pm Latest: Jun 23, 2024 11:09 pm","spencer-gable-cook","Hello! I am using Transformers and Tokenizers on Amazon Web Services (AWS), however I am struggling to import the ‚Äútransformers‚Äù or ‚Äútokenizers‚Äù library into my AWS Lambda function. I have tried methods with Docker and by zipping the transformers package into Lambda, however none of these approaches have worked for me. I have already trained my model on my local computer, so I do not require any training functionality of transformers on AWS; I only require inference from my trained transformer on AWS. So far, I have successfully converted my trained transformer model to an .onnx file. This way, I do not have to import ‚Äútransformers‚Äù into AWS. I followed the tutorial here. However, I still require my tokenizer to feed data into my .onnx file, and I am not able to import the ‚Äútokenizers‚Äù library. Could I convert my tokenizer to an .onnx file as well? Or is there another way that I could use my tokenizer without requiring the ‚Äútokenizer‚Äù library? Thank you very much for all of your help!","Mar 7, 2022 7:46 pm","Franck-Dernoncourt","Have you found a solution to export a tokenizer to an onnx model?","Jun 23, 2024 11:09 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NER tag , aggregation stratergy","https://discuss.huggingface.co/t/ner-tag-aggregation-stratergy/14199","2","6.0k","Created: Jan 28, 2022 7:38 pm Latest: Feb 1, 2022 6:38 pm","prashanth","I‚Äôm trying to do NER tagging, I have been using the pipeline to predict the output of my models, issue: aggregation stratergy="" simple"" does a good job but the tags are grouped. How can I avoid the tags being grouped? I want tags like I-PER, not PER, on the other hand, I tried the aggregation strategy ‚Äúnone‚Äù, here the tags are generated the way I want but words are split due to tokenization. @sgugger can you please help me out with this?","Jan 28, 2022 7:38 pm","prashanth","I have tried, the first and max options as well, but of no use","Jan 28, 2022 8:57 pm","StivenLancheros","Maybe I am not understanding your question correctly, but aggregation_strategy is used to group the entities in the predictions. So removing aggregation_stategy should give you the BIO (ES) tags individually. Take a look at the code: Pipeline Token Classification use aggregation_strategy instead. Whether or not to group the tokens corresponding to the same entity together in the predictions or not. aggregation_strategy (str, optional, defaults to ""none""): The strategy to fuse (or not) tokens based on the model prediction. If you meant something different please let me know.","Feb 1, 2022 6:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RoBERTa Tokenizer Java Implementation","https://discuss.huggingface.co/t/roberta-tokenizer-java-implementation/16737","1","2.3k","Created: Apr 12, 2022 1:55 pm Latest: Nov 29, 2022 1:50 pm","RazivTri","Hi everyone I have a RoBERTa model working great in Python and I want to move it to my service - which is written in Java. For that I need to imitate the RobertaTokenizer Python class - since I didn‚Äôt find a Java implementation for it. From what I understand, and I‚Äôm pretty new to Transformers, the RobertaTokenizer is similar to SentencePiece but not exactly like it. I have as reference a Java Tokenizer implementation for CamemBERT which uses SentencePiece, and hugging face documentation says that the CamemBERT tokenizer inherits from the RoBERTa tokenizer. My question here is, what would be the best way to implement a RoBERTa tokenizer in Java? Can I use the SentencePiece class like used in CamemBERT? Thanks (;","Apr 12, 2022 1:55 pm","RazivTri","Hi all We managed to implement a working RoBERTa tokenizer in Java. Deployed it to a Maven Central Repoistory: GitHub GitHub - purecloudlabs/roberta-tokenizer: Tokenizer to convert the input... Tokenizer to convert the input words into numbers. - GitHub - purecloudlabs/roberta-tokenizer: Tokenizer to convert the input words into numbers. Hope this can help others","Nov 29, 2022 1:50 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to ‚Äúfurther pretrain‚Äù a tokenizer (do I need to do so?)","https://discuss.huggingface.co/t/how-to-further-pretrain-a-tokenizer-do-i-need-to-do-so/14719","5","4.1k","Created: Feb 14, 2022 5:55 pm Latest: Feb 20, 2022 10:29 am","lucasresck","Hi there, I have a few simple questions about the tokenizer when I‚Äôm performing a further pretraining/fine-tuning of a MLM model. I would love to have a feedback from you. Thanks in advance TL;DR Considering the task of further pretraining a model in a domain-specific dataset: How can I know if I need to perform any kind of customization to the original model‚Äôs tokenizer? What kind of update should I perform to the tokenizer? Is it possible to ‚Äúfurther pretrain‚Äù the original tokenizer to especialize it in my dataset? Context I‚Äôm planning to further pretrain (a.k.a. fine-tune) a BERT language model in a domain-specific dataset in the same language. The general idea is to use the pretrained BERT model and to especialize it in my dataset to increase performance in future downstream tasks, like text classification. For this, my plan is to use the example script provided by Hugging Face, which seems to be very popular and standard for this pretrain task. Questions Because my domain-specific dataset is specialized, with many specific and technical words, it‚Äôs possible I‚Äôll have to deal with the tokenizer. I don‚Äôt know if I must change the tokenizer and, if positive, which changes I should perform. I noticed the example script assumes an already defined tokenizer. The only thing the script changes is one of the dimensions of the token embeddings‚Äô matrix of the model in the case that the original tokenizer has been modified: github.com huggingface/transformers/blob/b090b790228bbe420f1667f8b0335c8b8e5bb5eb/examples/pytorch/language-modeling/run_mlm.py#L361                from_tf=bool("".ckpt"" in model_args.model_name_or_path),         config=config,         cache_dir=model_args.cache_dir,         revision=model_args.model_revision,         use_auth_token=True if model_args.use_auth_token else None,     ) else:     logger.info(""Training new model from scratch"")     model = AutoModelForMaskedLM.from_config(config)              model.resize_token_embeddings(len(tokenizer))              # Preprocessing the datasets. # First we tokenize all the texts. if training_args.do_train:     column_names = raw_datasets[""train""].column_names else:     column_names = raw_datasets[""validation""].column_names text_column_name = ""text"" if ""text"" in column_names else column_names[0]              if data_args.max_seq_length is None:      Here it comes my first question: how can I know if I need to perform any kind of customization to the original model‚Äôs tokenizer? Is there any way or metric to quantify if the standard model‚Äôs tokenizer is appropriated for my dataset? Or is it sufficient to explore how the tokenizer behaves in my dataset? In the case I will need to update the tokenizer, what kind of update should I perform? In my understanding, if I train the tokenizer from scratch, all current model‚Äôs token embeddings will be useless and part of my MLM training will be retraining these vectors. So I‚Äôm assuming retraining the tokenizer is not the right thing to do. Is it possible to ‚Äúfurther pretrain‚Äù the original tokenizer to especialize it in my dataset? I think I exposed all of my questions, thanks in advance Possibly related Using a pretrained tokenizer vs training a one from scratch how can i finetune BertTokenizer? ¬∑ Issue #2691 ¬∑ huggingface/transformers ¬∑ GitHub nlp - What is the simplest way to continue training a pre-trained BERT model, on a specific domain? - Stack Overflow","Feb 14, 2022 5:55 pm","beneyal","Hi! From what I know, and from here, BERT‚Äôs vocabulary has 994 [unused#] tokens, # being 0-993. These are token ids 1-998 excluding 100, 101, 102, 103 which are BERT‚Äôs special tokens. You could change those tokens and fine-tune BERT so it will pick up on the new tokens, without needing to pre-train BERT from scratch.","Feb 14, 2022 6:31 pm","lucasresck","Hi there, thanks for the answer! This is a possible solution indeed. An inconvenience would be to choose the top 994 words (and check if it‚Äôs enough), but it‚Äôs solvable. You said (also supported by your reference): beneyal: without needing to pre-train BERT from scratch If I train a new tokenizer from scratch, would I really need to pretrain BERT from scratch?","Feb 20, 2022 2:59 am","beneyal","Yes, as far as I know. BERT relies on the fact that token id 12,476 is ‚Äúawesome‚Äù and not something else. New tokenizer means new token ‚Üî id mapping, and suddenly token id 12,476 is no longer ‚Äúawesome‚Äù, so BERT will need to go through all its pre-training data to learn the contexts of the new token 12,476.","Feb 20, 2022 3:15 am","lucasresck","Maybe there‚Äôs a way to train a new tokenizer but keeping the old tokenizer‚Äôs token-ID mappings? Have you seen something like this before?","Feb 20, 2022 7:18 am","beneyal","Sorry, I haven‚Äôt‚Ä¶","Feb 20, 2022 10:29 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Load tokenizer from file : Exception: data did not match any variant of untagged enum ModelWrapper","https://discuss.huggingface.co/t/load-tokenizer-from-file-exception-data-did-not-match-any-variant-of-untagged-enum-modelwrapper/25325","3","8.3k","Created: Nov 1, 2022 11:23 am Latest: Aug 1, 2023 8:23 am","Chrode","Hello ! I am having issue loading a Tokenizer.from_file() BPE tokenizer. When I try I am encountering this error where the line 11743 is the last last one: Exception: data did not match any variant of untagged enum ModelWrapper at line 11743 column 3 I have no idea what is the problem and how to solve it does anyone have some clue? I did not train directly the BPE but the structure is the correct one so vocab and merges in a json. What I did was from a BPE trained by me (that was working) change completely the vocab and the merges based on something manually created by me (without a proper train). But I don‚Äôt see the problem since the structure should be the same as the original one. My tokenizer version is: 0.13.1 {   ""version"":""1.0"",   ""truncation"":null,   ""padding"":null,   ""added_tokens"":[     {       ""id"":0,       ""content"":""[UNK]"",       ""single_word"":false,       ""lstrip"":false,       ""rstrip"":false,       ""normalized"":false,       ""special"":true     },     {       ""id"":1,       ""content"":""[CLS]"",       ""single_word"":false,       ""lstrip"":false,       ""rstrip"":false,       ""normalized"":false,       ""special"":true     },     {       ""id"":2,       ""content"":""[SEP]"",       ""single_word"":false,       ""lstrip"":false,       ""rstrip"":false,       ""normalized"":false,       ""special"":true     },     {       ""id"":3,       ""content"":""[PAD]"",       ""single_word"":false,       ""lstrip"":false,       ""rstrip"":false,       ""normalized"":false,       ""special"":true     },     {       ""id"":4,       ""content"":""[MASK]"",       ""single_word"":false,       ""lstrip"":false,       ""rstrip"":false,       ""normalized"":false,       ""special"":true     }   ],   ""normalizer"":null,   ""pre_tokenizer"":{     ""type"":""Whitespace""   },   ""post_processor"":null,   ""decoder"":null,   ""model"":{     ""type"":""BPE"",     ""dropout"":null,     ""unk_token"":""[UNK]"",     ""continuing_subword_prefix"":null,     ""end_of_word_suffix"":null,     ""fuse_unk"":false,     ""vocab"":{       ""[UNK]"":0,       ""[CLS]"":1,       ""[SEP]"":2,       ""[PAD]"":3,       ""[MASK]"":4,       ""AA"":5,       ""A"":6,       ""C"":7,       ""D"":8, ..... merges: ....       ""QD FLPDSITF"",       ""QPHY AS"",       ""LR SE"",       ""A DRV""     ] #11742   } #11743 } #11744","Nov 1, 2022 11:23 am","arnab9learns","@Chrode Would you please let me know, how did you solved this problem?","Apr 26, 2023 10:14 pm","gundeep","I had to retrain the tokenizer to make it work. During the retraining, I had to initialize the tokenizer with a pre_tokenizer; tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = Whitespace() # this is the line I was missing which caused the error trainer = trainers.BpeTrainer(     vocab_size=10_000,     min_frequency=5 ) tokenizer.train(['./input.txt'], trainer) tokenizer.save('./bpe_tokenizer.json') tokenizer1 = Tokenizer.from_file('./bpe_tokenizer.json') tokenizer2 = PreTrainedTokenizerFast(tokenizer_file='./bpe_tokenizer.json')","Jul 12, 2023 4:48 pm","Chrode","@arnab9learns unfortunately i have not but @gundeep this works thanks!","Aug 1, 2023 8:23 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer.pad_token=what?","https://discuss.huggingface.co/t/tokenizer-pad-token-what/24367","2","9.0k","Created: Oct 13, 2022 12:49 am Latest: Nov 8, 2022 11:20 am","MLEnthusiast","Hey everyone! Trying out some fine-tuning and I‚Äôm not exactly sure how I fix this error: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as ‚Äòpad_token‚Äô ‚Äò(tokenizer.pad_token = tokenizer.eos_token e.g.)‚Äô or add a new pad token via ‚Äòtokenizer.add_special_tokens ({‚Äòpad_token‚Äô: ‚Äò[PAD]‚Äô})‚Äô I‚Äôm trying to fine-tune openai-gpt when receiving this error. I‚Äôm relatively new to FT and not really understanding the concept so far behind tokenizers and padding. Would love some clarification with any examples/solutions! (I‚Äôve found Huggingface tutorials on this to be too high level for a beginner). Cheers!","Oct 13, 2022 12:49 am","lianghsun","Actually there is no short cut for you to learn HF Tokenizers library‚Ä¶ I‚Äôll suggest you should take a look at HF Course and HF Tokenizers documentation to learn how to use it. However I‚Äôll give you a hint how to deal with padding: first of all, you should specify [PAD] token in BpeTrainer trainer = BpeTrainer(special_tokens=[..., '[PAD]', ...]) Second, after training the tokenizer, you can specify tokenzier.enable_padding(pad_id=tokenizer.token_to_id('[PAD]')). Then you encode any sentence may contain padding follow by. Please note why we set .enable_padding() after training? This because when training to learn BPE vocab, it‚Äôs no need to consider padding in progress. Padding is the thing that to consider after tokenizing the sentense (truncation as well).","Oct 25, 2022 8:14 pm","MLEnthusiast","Thanks very much for your response Lianghsun","Nov 8, 2022 11:20 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Cannot load tokenizer for llama2","https://discuss.huggingface.co/t/cannot-load-tokenizer-for-llama2/55046","6","5.5k","Created: Sep 14, 2023 5:12 pm Latest: Sep 13, 2024 3:48 pm","saireddy","Hi, I am trying to load tokenizer for llama2 using AutoTokenizer but I am facing this issue ‚Äú‚Äù‚ÄúOSError: Can‚Äôt load tokenizer for ‚Äòmeta-llama/Llama-2-7b-hf‚Äô. If you were trying to load it from ‚ÄòModels - Hugging Face‚Äô, make sure you don‚Äôt have a local directory with the same name. Otherwise, make sure ‚Äòmeta-llama/Llama-2-7b-hf‚Äô is the correct path to a directory containing all relevant files for a LlamaTokenizer tokenizer. ‚Äú‚Äù‚Äù any help is appreciated! Thanks","Sep 14, 2023 5:12 pm","linying111","Hello, have you solved this problem,I‚Äôm having the same issue too?","Oct 21, 2023 7:47 am","saireddy","yes, we need to pass access_token and proxy(if applicable) for tokenizers as well","Oct 25, 2023 7:56 pm","Hemanth05","hi, how did you do that?","Feb 2, 2024 3:27 pm","saireddy","tokenizer = AutoTokenizer.from_pretrained(model_id, token=access_token) access_token can be generated from huggingface UI","Mar 20, 2024 2:14 pm","shinieaggarwal","hi, what did you put in the variable model_id?? i am trying it on colab and facing this same issue","Sep 7, 2024 7:17 pm","saireddy","model_id should be the name from hf model repository for example : meta-llama/Meta-Llama-3-8B","Sep 13, 2024 3:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What does `tokenizers.normalizer.normalize` do?","https://discuss.huggingface.co/t/what-does-tokenizers-normalizer-normalize-do/1463","5","3.3k","Created: Oct 10, 2020 5:10 pm Latest: Oct 12, 2020 4:04 pm","morgan","Hey all! Loving the updated tokenizer docs and playing around with normalizers at the moment. I‚Äôd like to update my article here about text preprocessing and using Datasets but I had a quick question: I know .normalize_str works like so: normalizer.normalize_str(""H√©ll√≤ h√¥w are √º?"") # ""Hello how are u?"" But  normalizer.normalize doesn‚Äôt seem to be documented? Is this something that maybe I should be using, or is more for internal use? Just wondering if normalizer.normalize_str is the most efficient way to use a normalizer with datasets.map or if normalizer.normalize can do some magic? Is there a way to use batched=True within datasets.map to make things even faster? Or if I added normalizer to a pretrained tokenizer and then call the tokenizer with datasets, will that also carry out the normalization before doing the tokenization?","Oct 10, 2020 5:10 pm","sgugger","The Normalizer.normalize function is for our internal use in the pipeline: it doesn‚Äôt take a string but a string with the offsets, as it adds some functionality to keep track of the original offsets wrt to the original text and works in place (so you can combine several normalizers easily). I don‚Äôt think normalize_str is less efficient. Also don‚Äôt think you can make things faster with batched=True here as it will just iterate on the elements of a batch. Pinging @anthony and @Narsil that may have more insight","Oct 12, 2020 12:15 pm","Narsil","morgan: Or if I added normalizer to a pretrained tokenizer and then call the tokenizer with datasets, will that also carry out the normalization before doing the tokenization? Yes that is exactly the way to go. To get maximum efficiency you want to avoid round trips between python and Rust as much as possible. We are in the process of getting a documentation out that will hopefully improve that. Just a gist here, the de-facto method to create a new Tokenizer in 0.9.0 is: from tokenizers import Tokenizer, models, normalizers, ....  tokenizer = Tokenizer(models.BPE()) tokenizer.normalizer = normalizers.NFKC() tokenizer.pre_tokenizer = ... # some pre tokenizer tokenizer.decoder = ... tokenizer.processor = ... etc...  # Optional training phase, not required  # if you loaded from an existing model.  trainer = trainers.BpeTrainer(vocab_size=80000 tokenizer.train(trainer, [""myfile.txt""])  # Training  tokenizer.encode(""my string"").tokens Then everything will be run within rust, and should get you the best speedup possible. If you save your tokenizer with tokenizer.save('mytokenizer.json') it should save the whole tokenization pipeline. Its another way of checking the various options of existing tokenizers.","Oct 12, 2020 12:54 pm","morgan","Thanks @sgugger, @Narsil great to know. @Narsil can I add a normalizer to a pretrained tokenizer and expect the same behaviour? Do pretrained tokenizers have their own normalizers that I might over-write by accident? Would something like the below work as expected or would I be interfering with what the pretrained tokenizer does? tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') tokenizer.normalizer = normalizers.NFKC() tokenizer('Am I working?')","Oct 12, 2020 3:27 pm","Narsil","It would not work as is. By default from_pretrained does not use (yet) the fast tokenizers (from the tokenizers library). What you want to do is tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)  # The actual tokenizer from `tokenizers` is tokenizer._tokenizer as we still need a small python wrapper currently  tokenizer._tokenizer.normalizer = normalizers.NFKC() tokenizer.tokenize(""Am I working?"") Keep in mind though that this works on master. On transformers 3.3.1 it would be tokenizer._tokenizer._tokenizer.normalizer. If you want to add a new normalizer, you would have to recreate the actual sequence (bear in mind that normalizers order has importance, so you want to carefully add it at the correct place for your use case)","Oct 12, 2020 3:44 pm","morgan","Ok, so one would have to go to the source code and find the normalizers used and then replicate those? I guess this is a good starting point: tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=True)  tokenizer._tokenizer.normalizer  Output: <tokenizers.normalizers.BertNormalizer at 0x7fc7e939b2f0> Would it be worth exposing more information about the normalizer used with the pretrained tokenizers (e.g. with a .normalizer or .config attribute)? tokenizer.add_normalizers would be ideal, but I guess its not going to be built for an audience of one Just trying to think how to leverage the parallization power of the tokenizer with easy, off-the-shelf processing functions, without folks having to go digging into the source code (cc @anthony)","Oct 12, 2020 4:04 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Use a pretrained ByteLevelBPETokenizer on text","https://discuss.huggingface.co/t/use-a-pretrained-bytelevelbpetokenizer-on-text/348","1","3.2k","Created: Jul 17, 2020 7:52 am Latest: Jul 17, 2020 2:20 pm","abdallah197","Hi I am asking whether there‚Äôs a simple way to tokenize a piece of text ‚ÄúI will go to the bedroom‚Äù to BPE "" I will go to the bed ##room"" without training a tokenizer from scratch.","Jul 17, 2020 7:52 am","anthony","Hi @abdallah197! There is a bunch of pre-trained tokenizers in the huggingface/transformers library that you can use directly, without having to train anything. You won‚Äôt have any control over how the tokens are split though, as this is based on what the tokenizer learned during training, and the size of its vocabulary. bedroom isn‚Äôt really a rare word, so often it will have its own token in the vocabulary. Your example looks like a WordPiece (instead of BPE), given the ## in ##room which is very specific to this kind of tokenizers. You can try to use those from BERT in the library to see if anything fits your needs, for example with: from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")  tokenizer.tokenize(""I will go to the bedroom"") # ['I', 'will', 'go', 'to', 'the', 'bedroom']  tokenizer.tokenize(""I will go to the Bedroom"") # ['I', 'will', 'go', 'to', 'the', 'Bed', '##room']","Jul 17, 2020 2:20 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using a BertWordPieceTokenizer trained from scratch from transformers","https://discuss.huggingface.co/t/using-a-bertwordpiecetokenizer-trained-from-scratch-from-transformers/4391","2","4.6k","Created: Mar 14, 2021 5:04 pm Latest: Mar 26, 2021 2:30 pm","yngtodd","Hey everyone, I‚Äôd like to load a BertWordPieceTokenizer I trained from scratch using the interface built in transformers, either with BertTokenizer or BertTokenizerFast. It looks like those two tokenizers in transformers expect different ways of loading in the saved data from BertWordPieceTokenizer, and I am wondering what is the best way to go about things. Example I am training on a couple test files, saving the tokenizer, and the reloading it in tokenizers.BertTokenizer (there is a bit of ceremony here creating the test data, but this is everything you need to reproduce the behavior I am seeing): from pathlib import Path  from tokenizers import BertWordPieceTokenizer from transformers import BertTokenizer   def test_text():     text = [         ""This is a test, just a test"",         ""nothing more, nothing less""     ]      return text   def create_test_files():     test_path = Path(""tmp"")     test_path.mkdir()      test_data = test_text()      for idx, text in enumerate(test_data):         file = test_path.joinpath(f""file{idx}.txt"")         with open(file, ""w"") as f:             f.write(text)      return test_path   def cleanup_test(path):     path = Path(path)      for child in path.iterdir():         if child.is_file():             child.unlink()         else:             rm_tree(child)      path.rmdir()   def create_tokenizer_savepath():     savepath = Path(""./bert"")     savepath.mkdir()     return str(savepath)   def main():     # Saving two text files to train the tokenizer     test_path = create_test_files()      files = test_path.glob(""**/*.txt"")     files = [str(f) for f in files]      tokenizer = BertWordPieceTokenizer(         clean_text=True,         strip_accents=True,         lowercase=True,     )      tokenizer.train(         files,         vocab_size=15,         min_frequency=1,         show_progress=True,         special_tokens=[""[PAD]"", ""[UNK]"", ""[CLS]"", ""[SEP]"", ""[MASK]""],         limit_alphabet=1000,         wordpieces_prefix=""##"",     )      savepath = create_tokenizer_savepath()     tokenizer.save_model(savepath, ""pubmed_bert"")      tokenizer = BertTokenizer.from_pretrained(         f""{savepath}/pubmed_bert-vocab.txt"",         max_len=512     )      print(tokenizer)      cleanup_test(test_path)     cleanup_test(savepath)   if __name__ == ""__main__"":     main() Loading the Trained Tokenizer Specifying the path to the pubmed_bert-vocab.txt is deprecated: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated PreTrainedTokenizer(name_or_path='bert/pubmed_bert-vocab.txt', vocab_size=30, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}) But, if I just specify the path to the directory containing pubmed_bert-vocab.txt: Traceback (most recent call last):   File ""minimal_tokenizer.py"", line 86, in <module>     main()   File ""minimal_tokenizer.py"", line 76, in main     max_len=512   File ""/home/ygx/opt/local/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 1777, in from_pretrained     raise EnvironmentError(msg) OSError: Can't load tokenizer for 'bert'. Make sure that:  - 'bert' is a correct model identifier listed on 'https://huggingface.co/models'  - or 'bert' is the correct path to a directory containing relevant tokenizer files The directory I am saving to only contains pubmed_bert-vocab.txt. If specifying the full path to that vocab is deprecated, what is the best way to load that tokenizer? Using BertTokenizerFast If I swap out BertTokenizer for BertTokenizerFast, and pass in the path to the directory where I have saved my tokenizer trained from scratch, I get the same error: Traceback (most recent call last):   File ""minimal_tokenizer.py"", line 86, in <module>     main()   File ""minimal_tokenizer.py"", line 76, in main     max_len=512   File ""/home/ygx/opt/local/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 1777, in from_pretrained     raise EnvironmentError(msg) OSError: Can't load tokenizer for 'bert'. Make sure that:  - 'bert' is a correct model identifier listed on 'https://huggingface.co/models'  - or 'bert' is the correct path to a directory containing relevant tokenizer files And if I specify the path to file saved by my tokenizer (pubmed_bert-vocab.txt), I get a ValueError (vs the deprecation warning I was getting using BertTokenizer): Traceback (most recent call last):   File ""minimal_tokenizer.py"", line 86, in <module>     main()   File ""minimal_tokenizer.py"", line 76, in main     max_len=512   File ""/home/ygx/opt/local/anaconda3/lib/python3.6/site-packages/transformers/tokenization_utils_base.py"", line 1696, in from_pretrained     ""Use a model identifier or the path to a directory instead."".format(cls.__name__) ValueError: Calling BertTokenizerFast.from_pretrained() with the path to a single file or url is not supported.Use a model identifier or the path to a directory instead. Current Approach I am currently using BertTokenizer, specifying the full path the the pubmed_bert-vocab.txt and am ignoring the deprecation warning, but ideally I would like use BertTokenizerFast but don‚Äôt know how to load in my saved tokenizer. What is the best way to go forward on this?","Mar 14, 2021 5:04 pm","valhalla","pinging @anthony","Mar 15, 2021 8:21 am","yngtodd","Should I open an issue on the tokenizers repository for this?","Mar 26, 2021 2:30 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Bug with tokernizer‚Äôs offset mapping for NER problems?","https://discuss.huggingface.co/t/bug-with-tokernizers-offset-mapping-for-ner-problems/2928","3","7.0k","Created: Dec 22, 2020 11:36 pm Latest: Dec 23, 2020 5:38 pm","facehugger2020","I‚Äôm working on NER and am following the tutorial from Token Classification with W-NUT Emerging Entities. I‚Äôm relying on the code in that tutorial to identify which tokens are valid and which tokens have been added by the Tokenizer, such as subword tokens and special tokens like [CLS]. The tutorial says the following: Now we arrive at a common obstacle with using pre-trained models for token-level classification: many of the tokens in the W-NUT corpus are not in DistilBert‚Äôs vocabulary. Bert and many models like it use a method called WordPiece Tokenization, meaning that single words are split into multiple tokens such that each token is likely to be in the vocabulary. Let‚Äôs write a function to do this. This is where we will use the offset_mapping from the tokenizer as mentioned above. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token‚Äôs start position and end position relative to the original token it was split from. That means that if the first position in the tuple is anything other than 0, we will set its corresponding label to -100. While we‚Äôre at it, we can also set labels to -100 if the second position of the offset mapping is 0, since this means it must be a special token like [PAD] or [CLS]. I get different results for the offset mapping from the tokenizer depending on when whether the input text is a complete sentence or a list of tokens. batch_sentences = ['The quick brown fox jumped over the lazy dog.',                    'That dog is really lazy.']  encoded_dict = tokenizer(text=batch_sentences,                                 add_special_tokens=True,                                 max_length=64,                                 padding=True,                                 truncation=True,                                 return_token_type_ids=True,                                 return_attention_mask=True,                                 return_offsets_mapping=True,                                 return_tensors='pt'                                 )  print(encoded_dict.offset_mapping) That prints: tensor([[[ 0,  0],          [ 0,  3],          [ 4,  9],          [10, 15],          [16, 19],          [20, 26],          [27, 31],          [32, 35],          [36, 40],          [41, 44],          [44, 45],          [ 0,  0]],          [[ 0,  0],          [ 0,  4],          [ 5,  8],          [ 9, 11],          [12, 18],          [19, 23],          [23, 24],          [ 0,  0],          [ 0,  0],          [ 0,  0],          [ 0,  0],          [ 0,  0]]]) On the other hand, if the sentences are already split, I get different results: batch_sentences = [['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog.'],                    ['That', 'dog', 'is', 'really', 'lazy.']]  encoded_dict = tokenizer(text=batch_sentences,                             is_split_into_words=True, # <--- different                             add_special_tokens=True,                             max_length=64,                             padding=True,                             truncation=True,                             return_token_type_ids=True,                             return_attention_mask=True,                             return_offsets_mapping=True,                             return_tensors='pt'                             )  print(encoded_dict.offset_mapping) That prints: tensor([[[0, 0],          [0, 3],          [0, 5],          [0, 5],          [0, 3],          [0, 6],          [0, 4],          [0, 3],          [0, 4],          [0, 3],          [3, 4],          [0, 0]],          [[0, 0],          [0, 4],          [0, 3],          [0, 2],          [0, 6],          [0, 4],          [4, 5],          [0, 0],          [0, 0],          [0, 0],          [0, 0],          [0, 0]]]) Here‚Äôs a Colab notebook with a full working example. If this is a bug, I‚Äôll open a ticket in Github.","Dec 22, 2020 11:36 pm","sgugger","I‚Äôm unsure what you think the bug is: the offset_mappings are maps from tokens to the original texts. If you provide the original texts in different formats, you are going to get different results. Each time you come back to 0 in the second results corresponds to one of your new words, and you get (0, 0) for special tokens, which is what the tutorial you mention detects. For non-split texts, you get the spans in the original text (though I‚Äôm not sure how you get your labels in that case?) Note that if you only want to detect the special tokens, you can use the special_tokens_mask the tokenizer can return if you add the flag return_special_tokens_mask=True. Also, for another approach using the word_ids method the fast tokenizer provide, you should check out the token classification example script.","Dec 23, 2020 3:04 pm","facehugger2020","Thank you for the explanation. I see the source of my misunderstanding. As I mentioned, the tutorial has this passage: This is where we will use the offset_mapping from the tokenizer as mentioned above. For each sub-token returned by the tokenizer, the offset mapping gives us a tuple indicating the sub-token‚Äôs start position and end position relative to the original token it was split from. What I didn‚Äôt fully understand is that the sentences in that NER example were already pre-split into tokens. I thought that if you pass in non-split sentences into the tokenizer, then it would return offset_mapping with the same values as the pre-split sentence since the tokenizer would still do tokenization. Note that if you only want to detect the special tokens, you can use the special_tokens_mask the tokenizer can return if you add the flag return_special_tokens_mask=True . I also want to mask out the sub-token pieces that were split off of tokens. I tried out the special_tokens_mask, and it only marks the added tokens for things like [CLS] but not sub-token pieces.","Dec 23, 2020 5:31 pm","sgugger","If you want to maks subtokens and special tokens, look at the script I mentioned in my earlier since it does just that with the word_ids method.","Dec 23, 2020 5:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How padding in huggingface tokenizer works?","https://discuss.huggingface.co/t/how-padding-in-huggingface-tokenizer-works/12161","4","5.9k","Created: Nov 22, 2021 5:10 pm Latest: Nov 22, 2021 10:30 pm","RajSingh333","I tried following tokenization example: tokenizer = BertTokenizer.from_pretrained(MODEL_TYPE, do_lower_case=True) sent = ""I hate this. Not that."",         _tokenized = tokenizer(sent, padding=True, max_length=20, truncation=True) print(_tknzr.decode(_tokenized['input_ids'][0])) print(len(_tokenized['input_ids'][0])) The output was: [CLS] i hate this. not that. [SEP] 9 Notice the parameter to tokenizer: max_length=20. How can I make Bert tokenizer to append 11 [PAD] tokens to this sentence to make it total 20?","Nov 22, 2021 5:10 pm","adorkin","You need to change padding to ""max_length"". The default behavior (with padding=True) is to pad to the length of the longest sentence in the batch, meanwhile sentences longer than specified length are getting truncated to the specified max_length. In your example you have only one sentence, thus there‚Äôs no padding (the only sentence is the longest one). Your sentence is shorter than max length, so there‚Äôs no truncation either.","Nov 22, 2021 6:38 pm","RajSingh333","adorkin: ngth, so there‚Äôs no truncation either. Great thanks!!! It worked. But how one can know that padding does indeed accept string value max_length? I tried to go through both of the tokenizer pages: tokenizer and BertTokenizer. But none of these pages state that padding does indeed accept string values like max_length. Now I am guessing what else it might be accepting and where can I find the whole list. Am I not reading carefully? Can you please tell how can I navigate the docs to obtain this information / link to doc stating this?","Nov 22, 2021 7:28 pm","adorkin","Well, you can call help on an object or a specific method to see more info. For instance, help(tokenizer.__call__) will display the documentation on the method that you‚Äôre using in your example. It‚Äôs the safest bet, in my opinion. However, the implementation of the method is inherited from PreTrainedTokenizerBase and accordingly the related docs can be found here. Although, I do agree with you that not seeing this info on the child classes‚Äô pages may be quite confusing.","Nov 22, 2021 8:58 pm","RajSingh333","Yes / No. Keeping inherited stuff out of child class‚Äô docs might reduce clutter. But I guess it also serves some purpose like having a glance on what all things available on class without navigating whole class hierarchy. Then, a simple switch to show/hide inheritted stuff would be useful‚Ä¶ I guess javadoc follow this approach.","Nov 22, 2021 10:30 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Token Classification: How to tokenize and align labels with overflow and stride?","https://discuss.huggingface.co/t/token-classification-how-to-tokenize-and-align-labels-with-overflow-and-stride/4353","4","5.9k","Created: Mar 12, 2021 1:23 pm Latest: Jul 22, 2024 12:04 pm","oliverguhr","Hello Huggingface, I try to solve a token classification task where the documents are longer than the model‚Äôs max length. I modified the tokenize_and_align_labels function from example token classification notebook. I set the tokenizer option return_overflowing_tokens=True and rewrote the function to map labels for the overflowing tokens: tokenizer_settings = {'is_split_into_words':True,'return_offsets_mapping':True,                          'padding':True, 'truncation':True, 'stride':0,                          'max_length':tokenizer.model_max_length, 'return_overflowing_tokens':True}  def tokenize_and_align_labels(examples):     tokenized_inputs = tokenizer(examples[""tokens""], **tokenizer_settings)      labels = []     for i,document in enumerate(tokenized_inputs.encodings):        doc_encoded_labels = []        last_word_id = None        for word_id  in document.word_ids:                        if word_id == None: #or last_word_id == word_id:                doc_encoded_labels.append(-100)                    else:                document_id = tokenized_inputs.overflow_to_sample_mapping[i]                label = examples[task][document_id][word_id]                               doc_encoded_labels.append(int(label))            last_word_id = word_id        labels.append(doc_encoded_labels)          tokenized_inputs[""labels""] = labels         return tokenized_inputs Executing this code will result in an error: exception has occurred: ArrowInvalid Column 5 named task1 expected length 820 but got length 30 It looks like the input 30 examples can‚Äôt be mapped to the 820 examples after the slicing. How can I solve this issue? Environment info Google Colab running this notbook To reproduce Steps to reproduce the behaviour: Replace the tokenize_and_align_labels function with the function given above. Add examples longer than max_length run tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True) cell.","Mar 12, 2021 1:23 pm","nielsr","cc @sgugger","Mar 15, 2021 11:25 am","jstremme","@oliverguhr, were you able to resolve this issue? I‚Äôm about to try something similar and was hoping to snatch some existing code. Thanks for anything you can share!","Mar 24, 2022 6:01 pm","oliverguhr","Well I avoided the problem. I only tokenize 280 words, which will lead to sequences of <512 tokens.","Sep 13, 2022 9:59 am","A415Hz","I had the same question, so I landed on this thread, and there is a new feature since March 2023 where people can use stride for long text input to be chunked and connect results from each chunk: github.com/huggingface/transformers Chunkable token classification pipeline huggingface:main ‚Üê luccailliau:patch-1 opened Feb 23, 2023 luccailliau +234 -43 This PR improve the TokenClassificationPipeline by extending its usage to tokeni‚Ä¶ It is possible to use stride in TokenClassification pipeline. In case the input token length exceeds the specified model_max_length, this allows continuation of token classification to the next chunk with overlapping tokens between each chunk with the number of overlapping tokens specified by stride parameter. This, however, wouldn‚Äôt solve the original question, and that was fine for me. I trained with model_max_length of 256 and perform inference also with model_max_length of 256. During the training, it is capped at the max length, so for sentences with 257 or more tokens, only the first 256 tokens are part of training. Because in my case sentences with 257 or more tokens were very rare, I could ignore and still achieve the same result. But during inference/prediction time, stride enables processing of all tokens for sentence with more than 256 tokens. If there is a need to train all sentences with overflowing tokens, I suggest chunking the training sentences with the same stride number, and creating new Dataset object and avoid .map(). I think .map() expects the same number of sentences before and after tokenize_and_align_labels at pyarrow level. This alternative approach has disadvantage because by not using .map(), tokenization isn‚Äôt parallelized so it gets slower. In my finding, this alternative approach may not be needed at all, hopefully like my case.","Jul 22, 2024 12:04 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenization in a NER context","https://discuss.huggingface.co/t/tokenization-in-a-ner-context/5635","5","5.3k","Created: Apr 20, 2021 1:13 pm Latest: Aug 11, 2021 7:07 am","Thrix","Hello everyone, I am trying to understand how to use the tokenizers in a NER context. Basically, I have a text corpus with entities annotations, usually in IOB format [1], which can be seen as a mapping f: word ‚Üí tag (annotators are working on a non-tokenized text and we ask them to annotate entire words). When I am using any modern tokenizer, basically, I will get several tokens for a single word (for instance ‚Äúhuggingface‚Äù might produce something like [‚Äúhugging#‚Äù, ‚Äúface‚Äù]). I need to transfer the original annotations to each token in order to have a new labelling fonction g: token ‚Üí tag E.g. what I have in input text  = ""Huggingface is amazing"" labels = [B_org, O, O]"" what I need to produce if the tokenizer output is [""Hugging#"", ""face"", ""is"", ""amazin""] is labels_per_tokens = [B_org, I_org, O, O] ```  To do so I need to backtrack for every token produced by the tokenizer what is the original word / annotation that I got in input but it seems not so easy to do (especially with [UNK] tokens). Am I missing something obvious ? Are there some good practice or solution to my problem ?  Thanks a lot for your help !  [1] https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging)","Apr 20, 2021 1:13 pm","lewtun","hey @Thrix you can see how to align the NER tags and tokens in the tokenize_and_align_labels function in this tutorial: https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/token_classification.ipynb#scrollTo=n9qywopnIrJH","Apr 20, 2021 1:46 pm","Thrix","Thanks a lot for the link ! Sounds exactly what I‚Äôm looking for ; I will check the tutorial !","Apr 20, 2021 2:04 pm","sbmaruf","For a word that is divided into multiple tokens by bpe or sentencepiece like model, you use the first token as your reference token that you want to predict. Since all the tokens are connected via self-attention you won‚Äôt have the problem not predicting the rest of the bpe tokens of a word. In PyTorch, you can ignore computing loss (see ignore_index argument) of those tokens by providing -100 as a label to those tokens (life is so easy with pytorch ).","Apr 29, 2021 4:43 pm","Thrix","Thanks for the trick ! Indeed, that‚Äôs also a very reasonable way to go","May 7, 2021 12:13 pm","s4sarath","@lewtun - Just wanted to know, how to align multiple sub words into individual word in case of SentencePiece tokenizer like Albert, for NER? For example, My name is Abraham George and I am living in Philadelphia"" . Now this will be split into sub words, and predictions happens per sub word. Now how we align it back to the individual words. @sgugger","Aug 11, 2021 7:07 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error with <|endoftext|> in Tokenizer GPT2","https://discuss.huggingface.co/t/error-with-endoftext-in-tokenizer-gpt2/2838","2","7.4k","Created: Dec 15, 2020 9:34 pm Latest: Dec 16, 2020 9:30 am","pivasil","Hi! I work with the sberbank-ai/rugpt3large_based_on_gpt2 . It is the model for the russian language corpus. I need to implement the function: def score(sentence):     tokenize_input = tokenizer.tokenize(sentence)     tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])     loss=model(tensor_input, lm_labels=tensor_input)     return math.exp(loss) from ( #473) For this function to work correctly with a single token, I need to add a special token <|endoftext|>. But when I pass the string ‚Äú<|endoftext|>token‚Äù to Toktokenizer, an error is returned ‚ÄúValueError: type of None unknown: <class ‚ÄòNoneType‚Äô>. Should be one of a python, numpy, pytorch or tensorflow object‚Äù. But even if there are multiple tokens at the input of tokenizer, there will be this error too. Token <|endoftext|> is absend in the tokenizer dictionary. But in the dictionary of special tokens, this token is present. My questions: What am I doing wrong? How can I solve this problem? Why is the <|endoftext|> absent in the dictionary? This is short test shows my problem:     #!pip install transformers     from transformers import GPT2Tokenizer     from transformers import GPT2LMHeadModel     with torch.no_grad():         #view of special tokens in dicronary       print('#' * 20, ' view of special tokens in dictonary ', '#' * 20)       items = tokenizer.get_vocab().items()       for item in items:         if (item[0].startswith('<')==True) and (item[0].endswith('>')==True):           print(item)       #view of special tokens       print('#' * 20, 'map of special_tokens ', '#' * 20)       print(tokenizer.special_tokens_map)       #try to get the id's with <|endoftext|>       print('#' * 20, "" try to get the id's with <|endoftext|> "", '#' * 20,)       single_token = '–≤–æ—Ç'       single_token_with_eos = '<|endoftext|>' + single_token     #error here!       id = tokenizer.encode(single_token_with_eos, return_tensors='pt')  print('single_token_with_eos_id',tokenizer.encode(single_token_with_eos))  Output: ####################  view of special tokens in dictonary  #################### ('<pad>', 0) ('<s>', 1) ('</s>', 2) ('<unk>', 3) ('<mask>', 4) > ################# map of special_tokens  #################### > {'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'} > ####################  try to get the id's with <|endoftext|>  #################### Error here! ...","Dec 15, 2020 9:34 pm","pivasil","Sorry, I realized my mistake. It should have been like this: tokenizer = AutoTokenizer.from_pretrained‚Ä¶ model = AutoModelWithLMHead.from_pretraine‚Ä¶  ‚ÄúIf nothing works, then read the instructions‚Äù","Dec 16, 2020 6:59 am","pivasil","No, this is also not correct. < | endoftext / > is missing in this model. See https://github.com/sberbank-ai/ru-gpts/issues/28","Dec 16, 2020 9:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Combine multiple sentences together during tokenization","https://discuss.huggingface.co/t/combine-multiple-sentences-together-during-tokenization/3430","3","5.5k","Created: Jan 29, 2021 2:52 am Latest: Feb 4, 2022 11:32 pm","prajjwal1","I want my output to be like [CLS] [SEP] text1 [SEP] text2 [SEP] text3 [SEP] eos token. As per the default behaviour, tokenizer expects either a string or a pair of string. tokenizer(sentence1, sentence2) # returns a single vector value for input_ids. I want this but for three sentences  I want the pair of string behavior for three sentences. I can pass a list of sentences, but that creates 3 lists of input_ids. tokenizer([sentence1, sentence2, sentence3]) # returns three tensors for input_ids  I want a single tensor representing the output I wrote above. Is there any good way of doing it ?","Jan 29, 2021 2:52 am","valhalla","I don‚Äôt think tokenizer handles this case directly. You could directly join the sentences using [SEP] and then encode it as one single text. tok = BertTokenizer.from_pretrained(""bert-base-cased"") text = ""sent1 [SEP] sent2 [SEP] sent3"" ids = tok(text, add_special_tokens=True).input_ids tok.decode(ids) => '[CLS] sent1 [SEP] sent2 [SEP] sent3 [SEP]'","Jan 29, 2021 9:12 am","prajjwal1","Okay. Thanks. May have to manually add then.","Jan 29, 2021 2:54 pm","bilalghanem","@prajjwal1 ‚Ä¶ I want to highlight a point here: using more than 2 [SEP] tokens with bert is not a g scientific solution. Bert wasn‚Äôt trained for that.","Feb 4, 2022 11:32 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Chat_template is not set & throwing error","https://discuss.huggingface.co/t/chat-template-is-not-set-throwing-error/104095","3","5.5k","Created: Aug 26, 2024 8:02 pm Latest: Aug 31, 2024 4:01 pm","flxflx","When I am running the sample code from here: tokenizer = AutoTokenizer.from_pretrained(""facebook/blenderbot-400M-distill"")  chat = [    {""role"": ""user"", ""content"": ""Hello, how are you?""},    {""role"": ""assistant"", ""content"": ""I'm doing great. How can I help you today?""},    {""role"": ""user"", ""content"": ""I'd like to show off how chat templating works!""}, ]  tokenizer.apply_chat_template(chat, tokenize=False) I get the error message: ValueError: Cannot use apply_chat_template() because tokenizer.chat_template is not set and no template argument was passed! For information about writing templates and setting the tokenizer.chat_template attribute, please see the documentation at https://huggingface.co/docs/transformers/main/en/chat_templating I am using tokenizers-0.19.1 & transformers-4.44.2 on a ubuntu on windows system. Any clue what the problem could be? I already uninstalled and re-installed these two packages. Thank you for your help!!","Aug 26, 2024 8:02 pm","GIJoe","I have the same issue. Can somebody help?","Aug 30, 2024 11:56 am","nielsr","Thanks for notifying, this is because initially we had default chat templates for models which didn‚Äôt have one set. As this led to various issues, one now needs to explicitly add a chat template (PR for that is here). I‚Äôll open an issue on the Transformers library to replace the code snippet by a model which supports chat templates (like Mistral-7B, LLaMa 3.1, Qwen2, etc.). Check whether the chat_template attribute is present in the tokenizer_config.json file.","Aug 31, 2024 4:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Convert huggingface tokenizer into sentencepiece format","https://discuss.huggingface.co/t/convert-huggingface-tokenizer-into-sentencepiece-format/85682","1","432","Created: May 7, 2024 5:31 pm Latest: Nov 27, 2024 2:06 pm","RaphaelKalandadze","I have a huggingface tokenizer for the BERT model (google-bert/bert-base-cased) which includes three files: tokenizer.json, tokenizer_config.json, and vocab.txt. I would like to convert this tokenizer into the SentencePiece tokenizer format, which uses a single .model file. How can I perform this conversion?","May 7, 2024 5:31 pm","bh4","Similar problem here. I would like to convert smollm2-360m hugging face tokenizer to sentencepiece format but couldn‚Äôt find any way of doing so. Can anyone guide?","Nov 27, 2024 2:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Fastest way to tokenize millions of examples?","https://discuss.huggingface.co/t/fastest-way-to-tokenize-millions-of-examples/18741","4","2.7k","Created: Jun 6, 2022 1:18 am Latest: Mar 8, 2024 12:41 am","SantoshGupta","Currently I am using a pandas column of strings and tokenizing it by defining a function with the tokenization operation, and using that with pandas map to transform my column of texts. It‚Äôs a slow process when I have millions of rows of texts, and I am wondering if there‚Äôs a faster way to tokenize all my training examples.","Jun 6, 2022 1:18 am","SantoshGupta","bump, still looking for a solution","Jul 5, 2022 3:44 am","Jaidev","I have a similar issue, using a pretrained WordPiece tokenizer on a large corpus of text takes several hours. I‚Äôm doing: tokenizer = AutoTokenizer.from_pretrained(‚Äúdistilbert-base-uncased‚Äù) train_tokenized_encodings = tokenizer(df[df.split==‚Äòtrain‚Äô].text.tolist(), truncation=True, padding=True, max_length=MAX_LENGTH) Any suggestions for speed up?","Sep 26, 2022 5:38 am","michaelwechner","Two options come to my mind: parallelization of the process itself detect similarities between the various rows, such that maybe previous tokenizations can be re-used but these are suggestions for which I have not practical experience myself, but maybe it helps somehow","Sep 27, 2022 8:52 pm","JBalabo","Can you share code please?","Mar 8, 2024 12:41 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How can I make sure Tokenizer pads to a fixed length?","https://discuss.huggingface.co/t/how-can-i-make-sure-tokenizer-pads-to-a-fixed-length/16116","2","1.9k","Created: Mar 26, 2022 7:34 pm Latest: Mar 29, 2022 3:05 pm","ollibolli","Hi, I‚Äôm trying to use Distilbert as a layer in keras, however the tokenizer doesn‚Äôt pad to a fixed length but rather just some minimum depending on the batch. I guess that is expected reading up on it. However that doesn‚Äôt work since the input layer (because I‚Äôm combining) needs a fixed length. Can I somehow make sure the Tokenizer always pads to max_length ? thanks for any help and insights","Mar 26, 2022 7:34 pm","merve","Hello and welcome to our forum When you‚Äôre passing your sequences, can you set padding to ‚Äúmax_length‚Äù and pass a value to the max_length argument like so: tokenizer(sequence, return_tensors=""tf"", padding=""max_length"", max_length=15) Let me know if it works","Mar 28, 2022 11:28 am","ollibolli","Hi merve, thanks a lot, this is something I tried, and many others, it didn‚Äôt work. I tried with padding = True, max_length = 512 (or just fixed length) as well. Weirdly a deprecated command does pad_to_max_length = True and the outputs are full length. Very strange. But I‚Äôm happy this works now. I‚Äôm thinking the issue is maybe that none of the text in the dataset are very long, if I make it some small length it works with just the padding/max_length args. Edit: I‚Äôm also thinking, maybe this is wanted behaviour and I don‚Äôt quite understand it, lol.","Mar 29, 2022 3:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Decode token IDs into a list (not a single string)","https://discuss.huggingface.co/t/decode-token-ids-into-a-list-not-a-single-string/42991","3","2.7k","Created: Jun 12, 2023 10:58 pm Latest: Sep 18, 2023 9:17 pm","steventrouble","tokenizer.convert_ids_to_tokens returns: ['ƒ†Drive', 'ƒ†was', 'ƒ†had', 'ƒ†walked', ""'s"", ',', 'ƒ†looked', ...] I need the tokens without the special characters. decode does not work, because it only returns a single string. Is there a function that outputs the plain tokens as a list?","Jun 12, 2023 10:58 pm","ArthurZ","Hey! Not sure I completely understand, but the tokens that you have here are the plain tokens, as they are in the vocab / merge. You should modify the tokenizer if you do not want it to add the spiece token at the beginning. Which tokenizer are you using?","Jun 22, 2023 7:11 am","steventrouble","Thanks for the ping! I was using the GPT byte level tokenizer. I‚Äôm not sure if this is a hack, but to get the behavior I wanted, I just passed the token ids into decode_batch instead, and that returned each token without the odd encoding.","Jun 23, 2023 3:40 am","ArthurZ","It‚Äôs not a hack, but something I wish to improve! IMO batch_decode and decode should be merged into one as we only have encode","Sep 18, 2023 9:17 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to save a fast tokenizer using the transformer library and then load it using Tokenizers?","https://discuss.huggingface.co/t/how-to-save-a-fast-tokenizer-using-the-transformer-library-and-then-load-it-using-tokenizers/9567","7","3.3k","Created: Aug 29, 2021 9:07 am Latest: Dec 14, 2022 11:57 pm","maroxtn","I want to avoid importing the transformer library during inference with my model, for that reason I want to export the fast tokenizer and later import it using the Tokenizers library. On Transformers side, this is as easy as tokenizer.save_pretrained(‚Äútok‚Äù), however when loading it from Tokenizers, I am not sure what to do. from tokenizers import Tokenizer Tokenizer.from_file(‚Äútok/tokenizer.json‚Äù) Seems to work, but it is ignoring the two other files in the directory: tokenizer_config.json and special_tokens_map.json, for that reason I believe it won‚Äôt give me the same tokens. Is there a way to import a tokenizer using the whole directory files ? Or better, can we import a pretrained fast tokenizer from the hub ? Thanks","Aug 29, 2021 9:07 am","zirui3","you can load tokenizer from directory with from_pretrained method: tokenizer = Tokenizer.from_pretrained(""your_tok_directory"")","Aug 30, 2021 6:55 am","maroxtn","Thanks for your reply, but I am trying to do is load it using the Tokenizers library rather than transformers","Aug 31, 2021 5:17 pm","zirui3","maroxtn: tokenizers it seems that the Tokenizer library does not directly support loading from directory, an alternative way is to write a wrapper method that looks for tokenize.json in the directory first, then load it via the from_file method of Tokenizer : from os.path import isfile, join from tokenizers import Tokenizer  def load_tokenizer_from_dir(Tokenizer, your_dir):     # check the existance of tokenizer file     if isfile(join(your_dir, ""tokenizer.json"")):         return Tokenizer.from_file(""tok/tokenizer.json"")     else:         raise ValueError(""tokenier.json not exist in dir {}"".format(your_dir)) the transformers library implement the from_pretrained  method in similar way","Sep 1, 2021 3:12 am","maroxtn","Thank you for the response again! And both tokenizers, on the transformer library and on the tokenizers library, would give exactly the same output ? (if yes, what‚Äôs the point then of tokenizer_config and special_tokens_map)","Sep 1, 2021 8:07 am","sgugger","The tokenizer_config contains information that are specific to the Transformers library (like which class to use to load this tokenizer when using AutoTokenizer). As for the other files, they are generated for compatibility with the slow tokenizers. Everything you need to load a tokenizer from the Tokenizers library is in the tokenizer.json.","Sep 1, 2021 12:27 pm","maroxtn","Crystal clear. Thank you all for the help and assistance","Sep 1, 2021 1:17 pm","danielcabal","Can you save a tokenizer from transformers into the tokenizer.json format? For example, GPT2Tokenizer.save_pretrained returns vocab.json, merges.txt, etc. Can I make it save a tokenizer.json so it can be loaded from the tokenizers library instead without transformers?","Dec 14, 2022 11:57 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issues with offset_mapping values","https://discuss.huggingface.co/t/issues-with-offset-mapping-values/4237","4","4.1k","Created: Mar 8, 2021 12:44 pm Latest: Feb 15, 2022 1:30 pm","matus","Hi guys, I am trying to work with a FairSeq model converted to but I have some issues with tokenizer. I am trying to fine-tune it for POS tagging so I have the text split to words already and I want to use the offset_mapping to detect first token for each word. I do it like this: tokenizer = RobertaTokenizerFast.from_pretrained('path', add_prefix_space=True) ids = tokenizer([['drieme', 'drieme'], ['drieme']],     is_split_into_words=True,     padding=True,     return_offsets_mapping=True) The tokenization looks like this: ['<s>', 'ƒ†d', 'rieme', 'ƒ†d', 'rieme', '</s>'] But the output from the command looks like this: {   'input_ids': [[0, 543, 24209, 543, 24209, 2], [0, 543, 24209, 2, 1, 1]],   'attention_mask': [[1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 0, 0]],   'offset_mapping': [     [(0, 0), (0, 1), (1, 6), (1, 1), (1, 6), (0, 0)],     [(0, 0), (0, 1), (1, 6), (0, 0), (0, 0), (0, 0)]   ] } Notice the offset mapping for the word drieme in the first case. First word has mappings (0, 1) and (1, 6). This looks reasonable, however the second drieme is (1, 1) and (1, 6). Suddenly, there is 1 at the first position. This 1 is there for all but first word for any sentence I try to parse. I feel like it might have something to do with handling the start of the sentence vs all the other words, but I am not sure how to solve this issue so that I have proper offset mappings. What am I doing wrong?","Mar 8, 2021 12:44 pm","sgugger","Thanks for reporting, it‚Äôs definitely a bug. Could you open an issue on tokenizers with your snippet?","Mar 9, 2021 11:11 pm","proxyht","Having similar issue as well. Is there a proper fix yet?","Apr 10, 2021 8:11 am","matus","Don‚Äôt know about the fix, but I solved it by detecting the special character that is used at the start of each word. In my case this was ƒ†. I used this instead of offset mapping to detect the start of the word.","Apr 10, 2021 8:26 am","mspinaci","Hello, Is there any update on this topic? I am encountering the same issue with any tokenizer that has the add_prefix_space=True flag, which seems to be needed for example for RoBERTa tokenizers. A few more examples to exemplify the issue, using words = ['hello', 'world', 'foo', 'bar', '32', '31290485']: Using a non-RoBERTa tokenizer doesn‚Äôt show the issue and works as expected: tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') tokenizer(words, is_split_into_words=True, return_offsets_mapping=True) # This returns: # 'input_ids': [101, 7592, 2088, 29379, 3347, 3590, 21036, 21057, 18139, 2629, 102] # 'offset_mapping': [(0, 0), (0, 5), (0, 5), (0, 3), (0, 3), (0, 2), (0, 3), (3, 5), (5, 7), (7, 8), (0, 0)] # Running tokenizer.add_prefix_space would raise an AttributeError in the above, as expected, special tokens have offsets (0, 0), beginning of words are (0, n), with n being the word/chunk length, continuations of words are of the form (n, m) with n > 0 being the start of the chunk, and m - n the chunk length. However, for RoBERTa gives the following: tokenizer = AutoTokenizer.from_pretrained('distilroberta-base', add_prefix_space=True) tokenizer(words, is_split_into_words=True, return_offsets_mapping=True) # This returns # 'input_ids': [0, 20760, 232, 48769, 2003, 2107, 32490, 3248, 33467, 2] # 'offset_mapping': [(0, 0), (0, 5), (1, 5), (1, 3), (1, 3), (1, 2), (1, 3), (3, 5), (5, 8), (0, 0) tokenizer.add_prefix_space # This returns True The special tokens are still marked as (0, 0) offset, the first word is still of the form (0, n), the chunks from second on are still of the form (n, m), but each word beginning after the second is now of the form (1, n), with n still being the word/chunk length. So this looks to be the only exception to the rule that the length of the chunk is m - n, and that‚Äôs not good. I suppose the beginning 1 is because, as the flag name suggests, a space is added before the words and not taken into account when running the offsets_mapping, but as suggested above that looks like a bug. For my purpose (running NER) I can probably workaround the above by checking explicitly if the word is not the first word in the sentence, but the situation gets weirder when the chunks should be just 1 character long (so that, with the above logic, the offsets_mapping should then be something like (1, 1), (1, 2)). For example, using words = ['hello', 'world', 'foo', 'bar', '32', '31290485', '√º√π'], I am getting: {'input_ids': [0, 20760, 232, 48769, 2003, 2107, 32490, 3248, 33467, 952, 4394, 3849, 9253, 2], 'offset_mapping': [(0, 0), (0, 5), (1, 5), (1, 3), (1, 3), (1, 2), (1, 3), (3, 5), (5, 8), (1, 1), (0, 1), (1, 2), (1, 2), (0, 0)]} contradicting what I was expecting from above; I guess in this case, first the space that is implicitly added before √º√π gets the offset (1, 1), and then the letter √º (which doesn‚Äôt have any space before it anymore) correctly gets the offset (0, 1), but then somehow the offset for √π (which should indeed be (1, 2)`, being the second half of a two-character word) gets‚Ä¶ doubled? Edit: the weirdness in the last part seems to be explained by how the tokenizer works on Unicode characters, encoding them by piece. So √º is actually encoded as √É¬º and √π is encoded as √É¬π, as explained by running tokenizer.convert_ids_to_tokens on the output. It is made all a bit cleaner by running tokenizer(' '.join(words)) and looking at its result, where indeed the tokens are duplicated, but also the offsets are correctly duplicated: {'input_ids': [0, 20760, 232, 48769, 2003, 2107, 32490, 3248, 33467, 952, 4394, 3849, 9253, 2], 'offset_mapping': [(0, 0), (0, 5), (6, 11), (12, 15), (16, 19), (20, 22), (23, 26), (26, 28), (28, 31), (32, 33), (32, 33), (33, 34), (33, 34), (0, 0)]} I guess that for now I‚Äôll stick to using tokenizer(' '.join(words)) instead of tokenizer(words, is_split_into_words=True)‚Ä¶","Feb 15, 2022 1:13 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error loading tokenizer from local checkpoint directory","https://discuss.huggingface.co/t/error-loading-tokenizer-from-local-checkpoint-directory/44428","3","1.4k","Created: Jun 25, 2023 12:43 am Latest: May 13, 2024 1:47 am","dmatekenya","I‚Äôm fine-tuning Whisper for a low-resource language (Chichewa) and following this tutorial. One change I have made is to provide a local directory to save the model instead of pushing to Hub. When its time to use the fine-tuned model using the pipeline module, I‚Äôm getting this error: Can't load tokenizer for '/content/drive/My  Drive/Chichewa-ASR/models/whisper-small-chich/checkpoint-1000. If you were trying to load it from  'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make  sure '/content/drive/My Drive/Chichewa-ASR/models/whisper-small-chich/checkpoint-1000' is the correct path to a directory containing all relevant files for a WhisperTokenizer tokenizer. When I check the model repository used by the tutorial author, I see that it has several files I don‚Äôt have in my model checkpoint directory such as tokenizer_config.json. I‚Äôm just wondering how do I ensure these files are saved in the local model checkpoint directories?","Jun 25, 2023 12:43 am","www007","I had the same problem, have you solved it?","Nov 1, 2023 2:45 am","Unspoiled-Egg","@sanchit-gandhi can you help with this, I have similar problem","Apr 4, 2024 7:12 am","Mutturaj","I had same problem, In tried to copy the tokenizer_config.json file from whisper model output directory to checkpoint directory Then i load the model from checkpoint directory It worked form me","May 13, 2024 1:47 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Run Mistral model only on CPU","https://discuss.huggingface.co/t/run-mistral-model-only-on-cpu/76136","0","1.5k","Created: Mar 6, 2024 6:48 am","mLacht","Hey there, unfortunately I do not have a GPU. I want to run the mistral code just on my CPU. I have a RAG project and all works just fine except the following mistral part: from transformers import DPRContextEncoder, DPRContextEncoderTokenizer import torch import faiss   # used for indexing pip install faiss-cpu from transformers import (RagRetriever,                           RagSequenceForGeneration,                           RagTokenizer) from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline ..... print(""Mistral Models"") model_name_or_path = ""TheBloke/Mistral-7B-Instruct-v0.1-GPTQ"" # To use a different branch, change revision # For example: revision=""gptq-4bit-32g-actorder_True"" tokenizer_2 = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)  print(""HUUUHUUUU"") # # # removed cude here:                                              device_map=""cuda:0"", model_2 = AutoModelForCausalLM.from_pretrained(model_name_or_path,                                              trust_remote_code=False,                                              revision=""gptq-4bit-32g-actorder_True"") # Save the Mistral model and tokenizer print(""saving mistral now...."") model_2.save_pretrained(""mistralModel"") tokenizer_2.save_pretrained(""mistralTokenizer"") The error I get is: ..... Mistral Models HUUUHUUUU CUDA extension not installed. CUDA extension not installed. Traceback (most recent call last):   File "".....\test.py"", line 263, in <module>     model_2 = AutoModelForCausalLM.from_pretrained(model_name_or_path,               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".....\Python311\Lib\site-packages\transformers\models\auto\auto_factory.py"", line 566, in from_pretrained     return model_class.from_pretrained(            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".....\Python311\Lib\site-packages\transformers\modeling_utils.py"", line 3928, in from_pretrained     model = quantizer.post_init_model(model)             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   File "".....Python311\Lib\site-packages\optimum\gptq\quantizer.py"", line 587, in post_init_model     raise ValueError( ValueError: Found modules on cpu/disk. Using Exllama or Exllamav2 backend requires all the modules to be on GPU.You can deactivate exllama backend by setting `disable_exllama=True` in the quantization  config object I tried already many diffrent things but I could not fix this issue I would be pleased by any kind of help. Thx Markus","Mar 6, 2024 6:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Fine-tuned BERT tokenizer taking too long to load","https://discuss.huggingface.co/t/fine-tuned-bert-tokenizer-taking-too-long-to-load/9747","1","3.3k","Created: Sep 6, 2021 4:48 pm Latest: Aug 23, 2022 5:12 am","Vageesh","I finetuned a pre-trained BERT model on my custom dataset for the LM task, to introduce new vocabularies (around 40k new tokens) from my dataset. Now that I am trying to further finetune the trained model on another classification task, I have been unable to load the pre-trained tokenizer with added vocabulary properly. I tried loading it up using BERTTokenizer, encoding/tokenizing each sentence using encode_plus takes me 1m 23sec. That‚Äôs too much considering I have over 200k sentences for classification just in my training data. I know that I can also use batch_encode_plus with parallelization, but even then, it will take forever to encode just my training data. I also tried loading it up using BertTokenizerFast and AutoTokenizer, but they take forever to load up. I tried running the same script with the pre-trained BERT tokenizers without my added tokens, and it takes a fraction of seconds (994 us) to encode the entire batch. So the problem is definitely with my own pre-trained tokenizer, which has the newly added tokens. Has anyone encountered a similar problem before? While pertaining, I used AutoTokenizer save_pretrained function. When I check the tokenizer after loading it up using BERTTokenizer, I can see all the newly added tokens using the get_vocab() function. So it‚Äôs unlikely that something went wrong while saving it.","Sep 6, 2021 4:48 pm","rvphil","Hi, the base classes PreTrainedTokenizer and PreTrainedTokenizerFast implement the common methods for encoding string inputs in model inputs (see below) and instantiating/saving python and ‚ÄúFast‚Äù tokenizers either from a local file or directory or from a pretrained tokenizer provided by the library (downloaded from HuggingFace‚Äôs AWS S3 repository). They both rely on PreTrainedTokenizerBase that contains the common methods, and SpecialTokensMixin. PreTrainedTokenizer and PreTrainedTokenizerFast thus implement the main methods for using all the tokenizers: Tokenizing (splitting strings in sub-word token strings), converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers). Adding new tokens to the vocabulary in a way that is independent of the underlying structure (BPE, SentencePiece‚Ä¶). Managing special tokens (like mask, beginning-of-sentence, etc.): adding them, assigning them to attributes in the tokenizer for easy access and making sure they are not split during tokenization. BatchEncoding holds the output of the PreTrainedTokenizerBase‚Äôs encoding methods (call, encode_plus and batch_encode_plus) and is derived from a Python dictionary. When the tokenizer is a pure python tokenizer, this class behaves just like a standard python dictionary and holds the various model inputs computed by these methods (input_ids, attention_mask‚Ä¶). When the tokenizer is a ‚ÄúFast‚Äù tokenizer (i.e., backed by HuggingFace tokenizers library), this class provides in addition several advanced alignment methods which can be used to map between the original string (character and words) and the token space (e.g., getting the index of the token comprising a given character or the span of characters corresponding to a given token). If you still haven‚Äôt got the answers to what you are looking for, you may get in touch with chatbot development services company. They can provide you with a free consultation.","Aug 23, 2022 5:12 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Get vocabulary tokens in order to exclude them from generate function","https://discuss.huggingface.co/t/get-vocabulary-tokens-in-order-to-exclude-them-from-generate-function/5192","2","2.6k","Created: Mar 29, 2021 6:22 pm Latest: Aug 1, 2022 2:13 pm","hfnlpmb","I want to get the vocabulary ids of some phrases in order to exclude these ids from text generation with GPT-2. I use AutoConfig and AutoTokenizer and when I am trying to get the ids that I want to exclude with tokenizer(bad_word, add_prefix_space=True).input_ids as it says in the bad_words_ids argument of [generate](Models ‚Äî transformers 4.4.2 documentation) function I get the error: _batch_encode_plus() got an unexpected keyword argument 'add_prefix_space' Do I have to use this argument and why is this error thrown?","Mar 29, 2021 6:22 pm","materialbeing","I‚Äôm running into a similar situation. It seems that the pre-trained GPT2 models use PreTrainedTokenizerFast rather than the regular GPT2Tokenizer. PreTrainedTokenizerFast does not accept the ‚Äòadd_prefix_space‚Äô argument. Does this mean that it‚Äôs not possible to tokenize a bad_words_id list with the pretrained GPT2 models? I‚Äôm a little lost myself!","Feb 22, 2022 11:52 am","felfri","Hey, instead of defining it when you execute the tokenizer, you can try to define it when you define the tokenizer itself. See AutoTokenizer _batch_encode_plus method don't have add_prefix_space argument ¬∑ Issue #17391 ¬∑ huggingface/transformers ¬∑ GitHub","Aug 1, 2022 2:13 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Remove only certain special token id during tokenizer decode","https://discuss.huggingface.co/t/remove-only-certain-special-token-id-during-tokenizer-decode/20436","3","2.2k","Created: Jul 15, 2022 7:42 pm Latest: Oct 26, 2022 5:24 am","adibm","I am using a GPT2 based language model to generate some text. My training data has special tokens in them, so I want my model to generate those special tokens as well. The models generated text has a lot of padding token and I was wondering if there is a way to remove them during decoding. One way to solve it would be to pass it through a regular expression/filter and remove all the padding tokens. Here is an example of the generated text i got after i decoded it. I removed a lot of the pad tokens for this forum post. '<|begincontext|><|user|>What can I go do I am bored sitting here.<|system|>What do you like to do and where would you like to do it?<|user|>I want something to do in or around NYC and Musical shows are one of my favorites.<|system|>I pulled up a list of 10 so far I think this one would interest you its on March 11th at 7:30 pm at the Kaufmann Concert Hall - Abbi Jacobson will be there.<|user|>That does interest me. Is there a direct bus I can take to get there?<|endcontext|> \n\n<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>FindAttractions<|endintent|>\n\n<|beginbelief|>Travel_goodForKids: True Travel_location: NYC<|endaction|>\n\n<|beginaction|>OFFER Travel_attractionName, OFFER Travel_category<|endintent|>\n\n<|beginresponse|>The Ac 9 Hotel By Marriott Brooklyn Bridge is a fine tourist spot.<|endintent|>\n\n<|pad|><|pad|>OFFER_attractionName, OFFER Travel_category<|endresponse|><|pad|><|pad|><|pad|><|pad|>'","Jul 15, 2022 7:42 pm","berkayberabi","Hi, did you manage to find a proper solution for this except removing other special tokens manually in an iterative way?","Oct 25, 2022 8:54 am","adibm","unfortunately I was not able to find a proper solution to this. Initially I had used a regular expressions to remove certain strings but later I decided to tokenize the text and use a list comprehension to remove by token ids.","Oct 25, 2022 5:13 pm","lianghsun","Could you paste your code? Maybe it would be more details there.","Oct 26, 2022 5:24 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Build a RoBERTa tokenizer from scratch","https://discuss.huggingface.co/t/build-a-roberta-tokenizer-from-scratch/2758","5","3.2k","Created: Dec 9, 2020 10:46 pm Latest: Dec 12, 2020 4:44 pm","flyaway","Hi, there, I try to train a RoBERTa model from scratch in the Chinese language. The first step is to build a new tokenizer. First, I followed the steps in the quicktour . After the tokenizer training is done, I use run_mlm.py to train the new model. However, the RoBERTa model training fails and I found two observations: The output of tokenzier(text) is <s> </s>. No matter what the text is, the output is always <s> </s>. There is nothing encoded. There is no ƒ† symbol in the generated merges.txt file. The merges.txt contains: #version: 0.2 - Trained by huggingface/tokenizers ‰ªÄ ‰πà ÊÄé ‰πà ÂèØ ‰ª• Êâã Êú∫ ... The code I used to train tokenizer: def build_BPE_tokenizer(         train_files: List[TextIO],         output_dir: TextIO,         # name: str,         vocab_size: int,         min_frequency: int):      tokenizer = Tokenizer(BPE())     tokenizer.pre_tokenizer = Whitespace()      trainer = BpeTrainer(         vocab_size=vocab_size, min_frequency=min_frequency,         special_tokens=[""<s>"", ""<pad>"", ""</s>"",                         ""<unk>"", ""<mask>""]     )      tokenizer.train(trainer, train_files)     tokenizer.model.save(output_dir) And examples of training data: ÂñúÊ¨¢ ÊâìÁØÆÁêÉ ÁöÑ Áî∑Áîü ÂñúÊ¨¢ ‰ªÄ‰πàÊ†∑ ÁöÑ Â•≥Áîü  Áà± ÊâìÁØÆÁêÉ ÁöÑ Áî∑Áîü ÂñúÊ¨¢ ‰ªÄ‰πàÊ†∑ ÁöÑ Â•≥Áîü Êàë ÊâãÊú∫ ‰∏¢ ‰∫Ü Ôºå ÊàëÊÉ≥ Êç¢‰∏™ ÊâãÊú∫  ÊàëÊÉ≥ ‰π∞‰∏™ Êñ∞ÊâãÊú∫ Ôºå Ê±Ç Êé®Ëçê How can I fix the problem? Any help is appreciated! Thanks for the help!","Dec 9, 2020 10:46 pm","valhalla","Pinging @Narsil","Dec 11, 2020 10:05 am","Narsil","Hi @flyaway, I can‚Äôt reproduce your problem, What version of tokenizers are you using ?  from tokenizers import Tokenizer, models, pre_tokenizers, trainers import tokenizers   print(tokenizers.__version__) # 0.9.4   def build_BPE_tokenizer(     train_files,     output_dir,     # name: str,     vocab_size: int,     min_frequency: int, ):      tokenizer = Tokenizer(models.BPE())     tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()      trainer = trainers.BpeTrainer(         vocab_size=vocab_size,         min_frequency=min_frequency,         special_tokens=[""<s>"", ""<pad>"", ""</s>"", ""<unk>"", ""<mask>""],     )      tokenizer.train(trainer, train_files)     tokenizer.model.save(output_dir)     return tokenizer  # Test.txt contains the examples you gave tokenizer = build_BPE_tokenizer([""test.txt""], ""out"", 100, 1)  print(tokenizer.encode(""ÂñúÊ¨¢ ÊâìÁØÆÁêÉ ÁöÑ Áî∑Áîü ÂñúÊ¨¢ ‰ªÄ‰πàÊ†∑ ÁöÑ Â•≥Áîü "").tokens) # Output is ['ÂñúÊ¨¢', 'ÊâìÁØÆÁêÉ', 'ÁöÑ', 'Áî∑Áîü', 'ÂñúÊ¨¢', '‰ªÄ‰πàÊ†∑', 'ÁöÑ', 'Â•≥Áîü'] Are you sure you were using the correct tokenizer ?","Dec 11, 2020 10:37 am","flyaway","@Narsil Thanks for the reply. I think I may have misled you. The problems happened AFTER I used run_mlm.py. The script I used is like this: output_dir=""../embeddings/roberta-chinese/"" CUDA_VISIBLE_DEVICES=0 python ../run_mlm.py \     --output_dir=$output_dir \     --model_type=roberta \     --do_train \     --tokenizer_name=$output_dir \     --config_name=$output_dir \     --train_file=$train_file \     --do_eval \     --validation_file=$validation_file  \     --line_by_line \     --overwrite_output_dir \     --max_steps=$max_steps In the ../embeddings/roberta-chinese/, there is a config file which looks like this:   ""_name_or_path"": ""roberta-base"",   ""architectures"": [     ""RobertaModel""   ],   ""attention_probs_dropout_prob"": 0.1,   ""bos_token_id"": 0,   ""eos_token_id"": 2,   ""gradient_checkpointing"": false,   ""hidden_act"": ""gelu"",   ""hidden_dropout_prob"": 0.1,   ""hidden_size"": 768,   ""initializer_range"": 0.02,   ""intermediate_size"": 3072,   ""layer_norm_eps"": 1e-05,   ""max_position_embeddings"": 514,   ""model_type"": ""roberta"",   ""num_attention_heads"": 12,   ""num_hidden_layers"": 12,   ""pad_token_id"": 1,   ""type_vocab_size"": 1,   ""vocab_size"": 30000 } After training, I used the huggingface AutoTokenizer to load the trained model and tokenizer: from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained('../embeddings/roberta-chinese')  encode_dict = tokenizer('ËøôÊòØ ‰∏Ä‰∏™ ÊµãËØï') print(encode_dict)  #{'input_ids': [0, 2], 'attention_mask': [1, 1]} I do not understand the problem because I used the BERT (and WordPiece class in Tokenizer) model exactly the same way and it works fine.","Dec 11, 2020 5:26 pm","cedpsam","if you consider the number of characters/ideogram in chinese does it makes any sense to have multiple character tokens? you should at least ensure that all CJKV characters are encoded to avoid any unencoded ones, I wonder what would be a reasonable number of tokens then","Dec 11, 2020 6:00 pm","Narsil","Ohhh I see. You‚Äôre saving only tokenizer.model.save(..) in your script, but AutoTokenizer.from_pretrained() needs the full tokenizer (tokenizer.save('tokenizer.json') and put tokenizer.json in the correct directory. To get a bit of background by what we call model vs full-tokenizer in the tokenizers library you can check : https://huggingface.co/docs/tokenizers/python/latest/pipeline.html. Bonus: As small doctor script to get back your tokenizer from AutoTokenizer.from_pretrained: from tokenizers import models, Tokenizer, pre_tokenizers                                                                                           # Those are the two files exported by the previous script and present in ../embeddings/roberta-chinese                        tokenizer = Tokenizer(models.BPE.from_file(""out/vocab.json"", ""out/merges.txt""))      # Notice how we need to respecify this as it was not present in vocab.json or merges.txt # Careful your special tokens are still missing at this point you should probably should readd them # but you need to check your files to see what was done with them at learning time tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()                                                                                                                     tokenizer.save(""out/tokenizer.json"")                                                                                                                                                                                 # Now let's check it works, tested on 4.0                                 from transformers import AutoTokenizer                                                                                                                                    tokenizer = AutoTokenizer.from_pretrained(""out"")                                     print(tokenizer.encode(""ÂñúÊ¨¢ ÊâìÁØÆÁêÉ ÁöÑ Áî∑Áîü ÂñúÊ¨¢ ‰ªÄ‰πàÊ†∑ ÁöÑ Â•≥Áîü "")) # [32, 40, 28, 39, 32, 35, 28, 36] # Notice we don't need to specify `.tokens` compared to previous script ? # ` transformers.tokenizer` and `tokenizers.tokenizer` are slightly different #  in that regard (because of backward compatibility) Is that clearer ?","Dec 12, 2020 4:44 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Extracting embedding values of NLP pertained models from tokenized strings","https://discuss.huggingface.co/t/extracting-embedding-values-of-nlp-pertained-models-from-tokenized-strings/9287","3","2.2k","Created: Aug 17, 2021 6:58 am Latest: Aug 18, 2021 6:28 am","kadaj13","I am using huggingface‚Äôs pipeline to extract embeddings of words in a sentence. As far as I know, first a sentence will be turned into a tokenized strings. I think the length of the tokenized string might not be equal to the number of words in the original sentence. I need to retrieve word embedding of a particular sentence. For example, here is my code: #https://discuss.huggingface.co/t/extracting-token-embeddings-from-pretrained-language-models/6834/6  from transformers import pipeline, AutoTokenizer, AutoModel import numpy as np import re  model_name = ""xlnet-base-cased"" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModel.from_pretrained(model_name)  tokenizer.add_special_tokens({'pad_token': '[PAD]'}) model.resize_token_embeddings(len(tokenizer))  model_pipeline = pipeline('feature-extraction', model=model_name, tokenizer=tokenizer)  def find_wordNo_sentence(word, sentence):          print(sentence)     splitted_sen = sentence.split("" "")     print(splitted_sen)     index = splitted_sen.index(word)       for i,w in enumerate(splitted_sen):         if(word == w):             return i      print(""not found"") #0 base def return_xlnet_embedding(word, sentence):              word = re.sub(r'[^\w]', "" "", word)     word = "" "".join(word.split())          sentence = re.sub(r'[^\w]', ' ', sentence)     sentence = "" "".join(sentence.split())          id_word = find_wordNo_sentence(word, sentence)                       try:         data = model_pipeline(sentence)                  n_words = len(sentence.split("" ""))         #print(sentence_emb.shape)         n_embs  = len(data[0])         print(n_embs, n_words)         print(len(data[0]))              if (n_words != n_embs):             ""There is extra tokenized word""                                   results = data[0][id_word]           return np.array(results)          except:         return ""word not found""  return_xlnet_embedding('your', ""what is your name?"") Then the output is: what is your name [‚Äòwhat‚Äô, ‚Äòis‚Äô, ‚Äòyour‚Äô, ‚Äòname‚Äô] 6 4 6 So the length of tokenized string that is fed to the pipeline is two more than number of my words. How can I find which one (among these 6 values) are the embedding of my word?","Aug 17, 2021 6:58 am","kadaj13","More especifically, I want to know when I call the model_pipeline(sentence), how should I understand how the sentence was tokenized? Because I think some words in the sentence might be tokenized into several parts, so I need to understand them.","Aug 17, 2021 7:07 am","lvwerra","Hi @kadaj13 You can checkout which words correspond to which token with the tokenizer and the word_ids function: inputs = tokenizer('This is a loooong word') print(f""Word IDs: {inputs.word_ids()}"") print(f""Tokens: {inputs.tokens()}"")  >>> Word IDs: [0, 1, 2, 3, 3, 3, 3, 4, None, None] >>> Tokens: ['‚ñÅThis', '‚ñÅis', '‚ñÅa', '‚ñÅ', 'loo', 'o', 'ong', '‚ñÅword', '<sep>', '<cls>'] You can see that tokens __, loo, o, and ong all belong the word with ID 3 (in other words the 4th word). This also helps you spot the position of the special input characters that you probably don‚Äôt want to embed which are indicated with None. Hope this helps!","Aug 17, 2021 8:37 am","kadaj13","Thank you very much","Aug 18, 2021 6:28 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer dataset is very slow","https://discuss.huggingface.co/t/tokenizer-dataset-is-very-slow/19722","3","3.8k","Created: Jun 28, 2022 12:35 pm Latest: Mar 2, 2024 3:47 pm","ccfeidao","This is my tokenizer method. I found that no matter how much batch_size is set, the speed is the same. Tokenizer Spend time even longer than training. How cloud I do. Thanks very much. def tokenize_function(example):     return tokenizer(example[""sentence1""], truncation=True, max_length = 512) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, batch_size = 8) tokenized_datasets = tokenized_datasets.remove_columns([""sentence1""])","Jun 28, 2022 12:35 pm","mariosasko","Hi! What tokenizer are you using? What does tokenizer.is_fast return? If the returned value is False, you can set num_proc > 1 to leverage multiprocessing in map. Fast tokenizers use multithreading to process a batch in parallel on a single process by default, so it doesn‚Äôt make sense to use num_proc there.","Jun 28, 2022 6:14 pm","msinghy","I‚Äôve checked mine, and I have a fast tokenizer. However, it‚Äôs still taking about 20 seconds per example for tokenization, which is too slow. Here‚Äôs the code, base_model_id = ""google/gemma-7b"" tokenizer = AutoTokenizer.from_pretrained(     base_model_id,     padding_side=""left"",     add_eos_token=True,     add_bos_token=True, ) tokenizer.pad_token = tokenizer.eos_token max_length = 1026  def generate_and_tokenize_prompt(prompt):     result = tokenizer(         formatting_func(prompt),         truncation=True,         max_length=max_length,         padding=""max_length"",     )     result[""labels""] = result[""input_ids""].copy()     return result  train_dataset = dataset_split['train'] eval_dataset = dataset_split['test'] tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt) tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt) Can someone please help me figure out what I‚Äôm missing? Thanks.","Mar 2, 2024 6:19 am","mariosasko","Feel free to report this issue in the tokenizers repo.","Mar 2, 2024 3:47 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What does this warning mean? -overflowing tokens are not returned for the setting you have chosen","https://discuss.huggingface.co/t/what-does-this-warning-mean-overflowing-tokens-are-not-returned-for-the-setting-you-have-chosen/11594","1","5.2k","Created: Nov 11, 2021 10:51 am Latest: Mar 30, 2022 6:26 am","NDugar","Running tokenizer on dataset: 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 111/393 [00:28<01:12, 3.88ba/s][WARNING|tokenization_utils_base.py:3048] 2021-11-11 10:46:54,553 >> Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the ‚Äòlongest_first‚Äô truncation strategy. So the returned list will always be empty even if some tokens have been removed. Not sure if this is the right category for it.","Nov 11, 2021 10:51 am","Bibhabasu","have you found the solution ? @NDugar may be its related to truncation=True in tokenizer","Mar 30, 2022 6:26 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Regular tokens vs special tokens","https://discuss.huggingface.co/t/regular-tokens-vs-special-tokens/6187","5","2.8k","Created: May 14, 2021 7:40 pm Latest: Jan 8, 2024 5:30 am","Felipehonorato","Based on the CTRL approach on GPT2, i‚Äôm trying to add tokens in order to control my text generation style. Is there a difference between adding a token as a regular one and adding it as a special token?","May 14, 2021 7:40 pm","lewtun","hey @Felipehonorato, as far as i know, special tokens won‚Äôt be split by the tokenizer which might be handy in your case where you‚Äôre trying to incorporate control tokens. you can find more information in the docs here.","May 17, 2021 10:09 am","Felipehonorato","Really thank you for the reply. Just to make sure i understood: When adding a word as a regular token, the tokenizer can maybe split this word into subwords, and when passing it as a special token this shouldn‚Äôt happen, am I right? I‚Äôve made a couple of tests and by passing as special tokens worked pretty well","May 24, 2021 10:51 am","lewtun","yes, you‚Äôre totally right regarding the difference between regular vs special tokens! in case you‚Äôre interested, this is all being handled by the tokenizers library, which has some extra details here: Input sequences ‚Äî tokenizers documentation","May 26, 2021 12:20 pm","jaideepcs","@lewtun if i have taxonomy of text T1 T2 T3 to establish a context between T1 to T3 , would I need special token or normal tokens T1 [L2] T2 [L3] T3","Feb 17, 2023 5:47 pm","Mahdip72","@lewtun Could you help us in this? Tokenizer.add_tokens automatically convert ESM2 new token to special We can not extend the embedding layer by adding new tokens using ESM-2 models.","Jan 8, 2024 5:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer not recognising words in vocabulary","https://discuss.huggingface.co/t/tokenizer-not-recognising-words-in-vocabulary/20140","4","1.7k","Created: Jul 8, 2022 11:27 am Latest: Mar 5, 2024 7:15 pm","Aron","I have an issue where a tokenizer doesn‚Äôt recognise tokens in its own vocabulary. A minimal example is: from transformers import AutoTokenizer model_checkpoint = ‚ÄòDeepChem/ChemBERTa-77M-MTR‚Äô tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) test_smiles = ‚ÄòCCC1=[O+]‚Äô print(tokenizer.vocab[‚Äò[O+]‚Äô]) print(tokenizer.tokenize(test_smiles)) this outputs: 73 [‚ÄòC‚Äô, ‚ÄòC‚Äô, ‚ÄòC‚Äô, ‚Äò1‚Äô, ‚Äò=‚Äô, ‚ÄòO‚Äô] Notice that the '[O+]' expression is encoded simply as 'O', even though it is in the vocabulary. This loses important information. (also posted here as I‚Äôm not sure where exactly the issue is)","Jul 8, 2022 11:27 am","lianghsun","Hi @Aron, the reason why [O+] is encoded as O may be the BPE encoding, but I haven‚Äôt found the way to correct it. However, I find an alternative way to solve this problem. # Step1: Save the vocab.json from ChemBERTa pretrained model from transformers import AutoTokenizer model_checkpoint = 'DeepChem/ChemBERTa-77M-MTR' tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) tokenizer.save_pretrained('/path/to/deepchem')  # Step2: Use WordLevel Model from tokenizers.models import WordLevel new_tokenizer = Tokenizer(     WordLevel.from_file(         '/path/to/deepchem/vocab.json',          unk_token='[UNK]' ))  # Step3: (important) Set pretokenizer to split the SMILES character from tokenizers import Regex pre_tokenizer = Split(     pattern=Regex(""\[(.*?)\]|.*?""),     behavior='isolated' ) pre_tokenizer.pre_tokenize_str('CCC1=[O+]') # You can test it with this line new_tokenizer.pre_tokenizer = pre_tokenizer  # Step4: Check if tokenizer work correctly! test_smi = 'CCC1=[O+]' for idx in new_tokenizer.encode(test_smi).ids:     print(f""{idx} --> {new_tokenizer.id_to_token(idx)}"")  # > 16 --> C # 16 --> C # 16 --> C # 20 --> 1 # 22 --> = # 73 --> [O+]","Oct 27, 2022 6:19 am","Aron","Thanks @lianghsun! Did you test the performance with this change? What I‚Äôm worried about is if it was trained with for instance [O+] encoded the same as O, changing it now will only decrease its performance as it will never have seen a [O+] token.","Oct 27, 2022 12:14 pm","lianghsun","Totally agree with you @Aron , if this pre-trained transformer model haven‚Äôt seen any [O+] in training phase, this may led the model less prone to predict [+O] at output. However, you can use the modified tokenizer to re-train the model to get better performance.","Oct 27, 2022 3:08 pm","kosonocky","@Aron @lianghsun that answer works for most cases but still has a few edge cases slipping through. Br and Cl still wouldn‚Äôt tokenize right, along with a few connector tokens. Here is the regex that worked for me in the end: ‚ÄúCl|Br|%[0-9]{2}|>>|[(.*?)]|.‚Äù","Mar 5, 2024 7:15 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Implementing custom tokenizer components (normalizers, processors)","https://discuss.huggingface.co/t/implementing-custom-tokenizer-components-normalizers-processors/12371","1","2.6k","Created: Nov 29, 2021 6:06 pm Latest: Nov 30, 2021 12:41 pm","saattrupdan","I‚Äôm wondering if there is an easy way to tweak the individual components of a tokenizer. Specifically, I‚Äôd like to implement a custom normalizer and post-processor. Just to provide some context, I‚Äôm trying to train a Danish tokenizer. Danish has a lot of compound nouns (e.g., the Danish translation of ‚Äúhouse owner‚Äù is ‚Äúhusejer‚Äù, with ‚Äúhus‚Äù being ‚Äúhouse‚Äù and ‚Äúejer‚Äù being ‚Äúowner‚Äù), so a tokenizer should split these accordingly. A standard BPE or WordPiece can deal with this just fine. The issue is that for some compound nouns, we impose an ‚Äús‚Äù in between the two words. For instance, ‚Äúbirthday greeting‚Äù is ‚Äúf√∏dselsdagshilsen‚Äù, with ‚Äúf√∏dselsdag‚Äù being ‚Äúbirthday‚Äù and ‚Äúhilsen‚Äù being ‚Äúgreeting‚Äù. This messes up the tokenizer completely, tokenizing it as [‚Äòf√∏dselsdag‚Äô, ‚Äòshi‚Äô, ‚Äòl‚Äô, ‚Äòsen‚Äô] rather than the ideal [‚Äòf√∏dselsdag‚Äô, ‚Äòs‚Äô, ‚Äòhilsen‚Äô]. I think I can solve it by imposing a new special token, <conn>, and at the normaliser stage I check if the word is of the form <word1>s<word2> where <word1> and <word2> are known words, and if so, replaces the ‚Äús‚Äù by <conn>. At the post-processing stage, I then replace the <conn> instances with ‚Äús‚Äù again. Long story short, is there a way to simply subclass the normaliser/processor classes to implement such behaviours?","Nov 29, 2021 6:06 pm","saattrupdan","For anyone else looking, this can be done, and it‚Äôs answered in this question: How to add additional custom pre-tokenization processing? ü§óTokenizers I would like to add a few custom functions for pre-tokenization. For example, I would like to split numerical text from any non-numerical test. Eg ‚Äò1000mg‚Äô would become [‚Äò1000‚Äô, ‚Äòmg‚Äô]. I am trying to figure out the proper way to do this for the python binding; I think it may be a bit tricky since its a binding for the original rust version. I am looking at the pretokenizer function /huggingface/tokenizers/blob/2ccd16bf5c3dd97759d7bdf5229e2feeba314b4a/bindings/python/py_src/tokenizers/pre_to‚Ä¶","Nov 30, 2021 12:41 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer ignores repeated whitespaces","https://discuss.huggingface.co/t/tokenizer-ignores-repeated-whitespaces/17864","3","3.1k","Created: May 12, 2022 10:44 am Latest: May 19, 2022 2:12 pm","berkayberabi","I am using T5 model and tokenizer for a downstream task. I want to add certain whitesapces to the tokenizer like line ending (\t) and tab (\t). Adding these tokens work but somehow the tokenizer always ignores the second whitespace. So, it tokenizes the sequence ‚Äú\n\n‚Äù as a single line ending and the sequence""\n\n\n\n"" is tokenized as two line endings and so on. See below to reproduce. from transformers import T5Tokenizer tokenizer = T5Tokenizer.from_pretrained(""t5-large"") tokenizer.add_tokens([""\n""])  tokenizer.encode(""\n"") # returns [32100, 1] as expected tokenizer.encode(""\n\n"") # returns [32100, 1] but expected would be [32100, 32100, 1] tokenizer.encode(""\n\n\n\n"") # returns [32100, 32100, 1] but expected would be [32100, 32100, 32100, 32100, 1] what is the reasoning behind this behaviour? Is it a bug or something related to how tokenizer works? I noticed that this only happens for added whitespaces but not for other characters. Is there way to prevent tokenizer from ignoring the repeated whitespaces?","May 12, 2022 10:44 am","courtneysprouse131","It looks like you have to add it as a special token instead tokenizer.add_special_tokens({""pad_token"": AddedToken(""\n"")}) Then it should tokenize the '\n' the way you want to. This is a little hacky because the reason your newline char is getting stripped has to do with how the tokenize method for the t5 model default strips tokens. If you encode it as a special token it has different stripping behavior that allows you to keep the new line chars","May 16, 2022 5:14 pm","berkayberabi","Hi @courtneysprouse131 Thank you very much for your answer!! I dont want to add it as a pad token because then all of my padded sequences will contain EOL at the end. In HuggingFace, there are reserved special tokens. I did the following. from tokenizers import AddedToken tokenizer.add_special_tokens({""additional_special_tokens"": [AddedToken(""\n"")]}) print(tokenizer.special_tokens_map) print(tokenizer.encode(""\n\n"")) # [32100, 32100, 1] as expected!! Thanks a lot for pointing into the right direction!","May 19, 2022 1:54 pm","courtneysprouse131","Ahh makes sense! Awesome glad I could help!","May 19, 2022 2:12 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with Decoding in HuggingFace","https://discuss.huggingface.co/t/issue-with-decoding-in-huggingface/15699","2","3.5k","Created: Mar 14, 2022 5:17 pm Latest: Mar 24, 2022 11:22 am","ashutoshsaboo","Hello! Is there a way to batch_decode on a minibatch of tokenized text samples to get the actual input text, but with sentence1 and sentence2 as separated? What I mean is that: currently batch_decode returns the required text but with a whole lot of special tokens by default (PAD, CLS, SEP etc etc). I know there is the skip_special_tokens param (Utilities for Tokenizers) which can help remove these unwanted tokens, but unfortunately a by-product of that is also that the special SEP token is also removed - which means in the returned special token free text there‚Äôs no way to split and get decoded sentence1 and sentence2 as separate sentences and both are concatenated. Is there some way to clear these other unwanted tokens (PAD, CLS etc) but leave SEP in the batch_decode (or if there‚Äôs any alternative method already available for this use-case?) - so we can get the decoded sentence1 and sentence2 separately back? Can someone please help if possible? @lewtun: I came across many of your insightful posts/answers in the community. If you could please help out with the above if possible, that‚Äôd be so helpful and awesome!","Mar 14, 2022 5:17 pm","lewtun","Hey @ashutoshsaboo I am not aware of a built in method to achieve what you want, but can‚Äôt you just slide out the first and last tokens from the batch_decode() function? Here‚Äôs a simple example to show what I mean: from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"") inputs = tokenizer(""My name is Alice"", ""My name is Bob"") outputs = tokenizer.batch_decode(inputs[""input_ids""]) # Returns ['My', 'name', 'is', 'Alice', '[SEP]', 'My', 'name', 'is', 'Bob'] outputs[1:-1] If that‚Äôs not what you‚Äôre after, perhaps you can share an example of the inputs and the desired outputs?","Mar 18, 2022 2:56 pm","ashutoshsaboo","First and Last tokens take care of only CLS and last SEP (if it exists). What about PAD token which can be dynamically padded in a batch? Any easy native way to strip those too? @lewtun Of-course, you could do with python string operations (stripping of PAD tokens might cause redundant spaces too which you might have to strip again) - I‚Äôm currently using re for the same. But I initially thought this would be a quite common use-case and would ideally have a native method to do this. Alas, not unfortunately! If a filter of tokens to strip can be passed to batch_decode that should have done the job ideally.","Mar 24, 2022 11:22 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How does `tokenizer().input_ids` work and how different it is from tokenizer.encode() before `model.generate()` and decoding step?","https://discuss.huggingface.co/t/how-does-tokenizer-input-ids-work-and-how-different-it-is-from-tokenizer-encode-before-model-generate-and-decoding-step/30476","1","2.4k","Created: Jan 26, 2023 12:10 pm Latest: Feb 22, 2023 11:38 pm","copper","How does tokenizer().input_ids work before it gets decoded? I was reading this example 1 and 2 and I see it uses # encode input context input_ids = tokenizer(input_context, return_tensors=""pt"").input_ids # generate sequences without allowing bad_words to be generated outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids) print(""Generated:"", tokenizer.decode(outputs[""sequences""][0], skip_special_tokens=True)) In the above example, is there any encoding takes place? if yes, where? I saw in some other example (I cant find the link now) which used tokenizer.encode() something like this # encode input context input_ids = tokenizer.encode(input_context, return_tensors=""pt"") # generate sequences without allowing bad_words to be generated outputs = model.generate(input_ids=input_ids, max_length=20, do_sample=True, bad_words_ids=bad_words_ids) print(""Generated:"", tokenizer.decode(outputs[0], skip_special_tokens=True)) What is the difference between the two methods?","Jan 26, 2023 12:10 pm","aolney","You can execute the code and see they are the same. tokenizer() will also return the attention mask, which is why selecting input_ids is necessary to get equivalence.","Feb 22, 2023 11:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Maximum recursion depth exceeded when using DataCollator","https://discuss.huggingface.co/t/maximum-recursion-depth-exceeded-when-using-datacollator/25937","2","3.4k","Created: Nov 11, 2022 3:30 pm Latest: Nov 14, 2022 9:48 am","clawdelu","Following this course and got stuck on padding the data using the Data Collator. The error says it‚Äôs reached maximum recursion depth. !!!NOTE: I have loaded my own dataset, but this doesn‚Äôt seem to be the issue. # source https://huggingface.co/course/chapter7/2?fw=tf import datasets from datasets import load_dataset  classes = [""O"", ""Quantity"", ""UnitPriceAmount"", ""GoodsDescription"",             ""Incoterms"", ""GoodsOrigin"", ""Tolerance"", ""HSCode""]  dataset = load_dataset(""json"", data_files='data/dataset_bert.json', features=datasets.Features(                 {                     ""id"": datasets.Value(""string""),                     ""tokens"": datasets.Sequence(datasets.Value(""string"")),                     ""tags"": datasets.Sequence(datasets.features.ClassLabel(names=classes))         }))  # LOAD TOKENIZER from transformers import PreTrainedTokenizerFast, BertTokenizerFast  tokenizer = BertTokenizerFast(     tokenizer_file=""tokenizer/tokenizer.json"",     bos_token=""<S>"",     eos_token=""</S>"",     unk_token=""<UNK>"",     pad_token=""<PAD>"",     cls_token=""<CLS>"",     sep_token=""<SEP>"",     mask_token=""<MASK>"",     padding_side=""right"",     max_length=300, )  inputs = tokenizer(dataset[""train""][0][""tokens""], is_split_into_words=True) print(inputs.tokens())  def align_labels_with_tokens(labels, word_ids):     new_labels = []     current_word = None     for word_id in word_ids:         if word_id != current_word:             # Start of a new word!             current_word = word_id             label = -100 if word_id is None else labels[word_id]             new_labels.append(label)         elif word_id is None:             # Special token             new_labels.append(-100)         else:             # Same word as previous token             label = labels[word_id]             # If the label is B-XXX we change it to I-XXX             if label % 2 == 1:                 label += 1             new_labels.append(label)      return new_labels  def tokenize_and_align_labels(examples):     tokenized_inputs = tokenizer(         examples[""tokens""], truncation=True, is_split_into_words=True     )     all_labels = examples[""tags""]     new_labels = []     for i, labels in enumerate(all_labels):         word_ids = tokenized_inputs.word_ids(i)         new_labels.append(align_labels_with_tokens(labels, word_ids))      tokenized_inputs[""labels""] = new_labels     return tokenized_inputs   tokenized_datasets = dataset.map(     tokenize_and_align_labels,     batched=True,     remove_columns=dataset[""train""].column_names, )  from transformers import DataCollatorForTokenClassification  data_collator = DataCollatorForTokenClassification(     tokenizer=tokenizer, return_tensors=""tf"" )  batch = data_collator([tokenized_datasets[""train""][i] for i in range(2)]) batch[""labels""] The compiler suggests using __call__. Can anyone explain to me how to do that? You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding. Also, a short explanation of why this call goes into recursivity is much appreciated.","Nov 11, 2022 3:30 pm","clawdelu","After some more digging I‚Äôve discovered that the issue is related to this one. The padding strategies are [‚Äòlongest‚Äô, ‚Äòmax_length‚Äô, ‚Äòdo_not_pad‚Äô]. The issue seems to be improper padding. If you set the strategy to do_not_pad it will work (for one sentence)  tokenizer(example). But even when I set the max_length of the tokenizer to a number, the issue still persists.","Nov 14, 2022 9:19 am","clawdelu","I‚Äôve fixed the issue. As with most issues in programming, the solution was simple. The tokenizer was the issue. The special tokens used by BERT were in the format [XX], and I put the format . Feel stupid and extatic right now. Hope this helps someone","Nov 14, 2022 9:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Convert_tokens_to_ids produces <unk>","https://discuss.huggingface.co/t/convert-tokens-to-ids-produces-unk/24771","1","4.1k","Created: Oct 21, 2022 10:45 am Latest: Oct 25, 2022 8:01 pm","AfonsoSousa","Hi. I am trying to tokenize single words with a Roberta BPE Sub-word tokenizer. I was expecting to have some words with multiple ids, but when that is supposed to be the case, the method convert_tokens_to_ids just returns <unk>. However, __call__ from tokenizer produces the multiple ids. To reproduce the problem, run: from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""roberta-base"") token_id = tokenizer.convert_tokens_to_ids(""exam"") print(f""{token_id} => {tokenizer.decode([token_id])}"") token_ids = tokenizer(""exam"").input_ids[1:3] print(f""{token_ids} => {tokenizer.decode(token_ids)}"") Is there a way to make convert_tokens_to_ids have the same behaviour as tokenizer(token).input_ids[1:3]? THanks in advance for any help you can provide.","Oct 21, 2022 10:45 am","lianghsun","I think you misunderstand tokenizer.convert_tokens_to_ids(). Please note this function is to map token to id, however exam is not a token, it is a word instead. You can check by the following code: tokenizer.convert_ids_to_token([3463, 424]) > ['ex', 'am'] # exam is tokenized to two token! So, it‚Äôs obivously that there is no token exam in vocab, but [UNK] instead.","Oct 25, 2022 8:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizing Domain Specific Text","https://discuss.huggingface.co/t/tokenizing-domain-specific-text/1978","5","1.3k","Created: Nov 11, 2020 5:29 am Latest: Nov 20, 2020 4:57 am","jodiak","Hello everyone, I‚Äôve been referencing this paper on training transformer based models using metadata enhanced MIDI and was thinking about implementing this using the huggingface transformers and tokenizer libraries as an introduction to these libraries beyond the basic language modeling examples. As I‚Äôve been researching and referencing this tutorial I‚Äôve ran into issues with tokenization and was wondering when training a tokenizer, how can I set up ‚Äúword level‚Äù semantics? Technically each ‚Äúword‚Äù in this case will be the data within a string like so ‚ÄòEvent(name=Position, time=360, value=4/16, text=360)‚Äô rather than just words and characters delimited on spaces like its doing now as listed below  #version: 0.2 - Trained by huggingface/tokenizers m e     a l     u e     al ue     i me     n ame     v alue     ƒ† value     ƒ†t ex     ƒ†t ime     Ev en     ƒ†tex t     Even t     ) ,     ƒ† Event     N o Apologies on if these questions are noobish I‚Äôm grokking a lot of this as I go along. Any help is greatly appreciated.","Nov 11, 2020 5:29 am","thomwolf","pinging @anthony and @Narsil here","Nov 12, 2020 12:48 pm","Narsil","What is your original data like ? The ‚ÄúWord level‚Äù semantics is usually dealt with the Pretokenizer logic (that basically splits up the data where it‚Äôs relevant). In your case, it would depend on your original data. There is more info in the docs: huggingface.co The tokenization pipeline ‚Äî tokenizers documentation If you have a specific example of what data comes in, and what you expect as a return, it‚Äôs probably going to be easier to give you pointers. Cheers, Nicolas","Nov 12, 2020 1:18 pm","jodiak","Hello Narsil, my data looks something similar to this: Note On_XX, Tempo Value_XX, Chord_Note:type, Position_X/X, where XX can be a number from 0 to 127 and X can be a number from 1 to 16. I also have a dictionary with every possible values for the above text types numericalized. Will I have to write a custom tokenizer in order to accomplish this tokenization? Is there a way to use my existing numericalized dictionary in a tokenizer? Thank you for your help and the documentation I‚Äôve begun going over it.","Nov 17, 2020 7:31 am","Narsil","Yes it seems you vocabulary is well defined and sufficiently small (127 * 16 * 16) to be in a ‚ÄúWordLevel‚Äù tokenizer. huggingface.co Components ‚Äî tokenizers documentation You can even create your vocabulary manually which makes it easier to run. vocab = {} i=0 for value in range(0, 128):     for position in range(0, 16):       word = ""Note on {value},Position_{position}""       vocab[word] = i       i += 1  import tokenizers from tokenizers import pre_tokenizers  tokenizer = Tokenizer(BPE(vocab=vocab)) tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  encoded = tokenizer.encode(""Note On_12,Position_16/16  Note on 45,Position_8/8"") encoded.ids # [3, 5] The vocabulary creation I wrote seems a bit off compared to what you described but it should be close enough. I assumed the possible values are relatively small (< 50 000) because of 128 notes + 16 tempos + 16 values which makes 32k possible values being conservative. If there‚Äôs actually an order of magnitude more, then this approach won‚Äôt work anymore. Does that help ?","Nov 17, 2020 9:05 am","jodiak","Thank you this is incredibly helpful.","Nov 20, 2020 4:57 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Added Tokens Not Decoding with Spaces","https://discuss.huggingface.co/t/added-tokens-not-decoding-with-spaces/10883","3","2.6k","Created: Oct 19, 2021 3:21 am Latest: Jan 19, 2024 5:49 am","mcrchopra","Hi All, My goal is to add a set of starting tokens to a pre-trained AlbertTokenizerFast. In the Albert Pre-Trained Vocab (SentencePiece Model), all start tokens are preceded with the meta-symbol: ‚ñÅ (e.g. ‚ñÅhamburger). I tried adding tokens, prefixed with the meta symbol: new_tokens = [AddedToken(""‚ñÅhamburger"",), AddedToken(""‚ñÅpizza"")] num_added_tokens = tokenizer.add_tokens(new_tokens) However, as this forum post shows, input text to AddedToken is treated literally; so manually adding the meta-symbol prefixes doesn‚Äôt achieve the desired effect. Instead, I tried using the single_word parameter: new_tokens = [AddedToken(""hamburger"", single_word=True, lstrip=True), AddedToken(""pizza"", single_word=True, lstrip=True)] num_added_tokens = tokenizer.add_tokens(new_tokens) This solution successfully encodes the new tokens where hamburger is being encoded by token 30001: tokenizer('This hamburger tastes great')  >> [2, 15, 30001, 53, 8, 345,3] However, when I try to decode these ids, no space appears between ‚Äúthis‚Äù and ‚Äúhamburger‚Äù: tokenizer.decode([2, 15, 30001, 53, 8, 345,3])  >> ('Thishamburger tastes great') I was wondering if anybody had any thoughts about how to fix this.","Oct 19, 2021 3:21 am","nielsr","Does the same occur when setting lstrip=False when defining the new tokens?","Oct 19, 2021 12:07 pm","mcrchopra","Thank you for the response! Yup, if I set lstrip=False, I see the same behavior: tokenizer('This hamburger tastes great')  >> [2, 15, 30001, 53, 8, 345,3] tokenizer.decode([2, 15, 30001, 53, 8, 345,3])  >> ('Thishamburger tastes great') Digging through the code, my hypothesis is: The pre-tokenizer replaces the meta-symbol with a space character, when applied to raw text. The decoder reverses the effects of the pre-tokenizer. When the decoder sees a token with a meta-symbol; it inserts a space (for correct viewing) Since the AddedTokens don‚Äôt go through the same pipeline (i.e. no metasymbol is added); I‚Äôm not sure if the pre-tokenizer is applied / if decoding works as expected. Any thoughts on what could be going wrong? Or how one might approach this?","Oct 19, 2021 4:19 pm","louisowen6","Hi @mcrchopra , I‚Äôm also facing the same issue. Are you able to solve this issue last time?","Jan 19, 2024 5:49 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RuntimeError: Cannot re-initialize CUDA in forked subprocess","https://discuss.huggingface.co/t/runtimeerror-cannot-re-initialize-cuda-in-forked-subprocess/28392","2","3.0k","Created: Dec 25, 2022 5:12 am Latest: Apr 3, 2023 1:17 am","Edenzzz","I‚Äôm trying to launch distributed training with Stable Diffusion, but CUDA is initialized even before any model/dataset is loaded, which probably causes this bug. In order words, if you go to the last fews line of the following code (print(‚Äúbefore loading tokenizer: cuda:‚Äù,torch.cuda.is_available())), it prints True. Any help is much appreciated!","Dec 25, 2022 5:12 am","Edenzzz","Part of my code as addition information: from huggingface_hub import login login(""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"") # notebook_login() # from google.colab import drive # drive.mount(""/content/drive"",force_remount=True)  # + id=""1_h0kO-VnQog"" outputId=""530fc822-cdad-4642-9e8b-37f9651dbd9d"" #@title Import required libraries # # %pip install protobuf==3.20.* #For deepspe  import argparse import itertools import math import os import random import numpy as np import torch,torch.nn as nn import torch.nn.functional as F import torch.utils.checkpoint from torch.utils.data import Dataset import torchvision import PIL from accelerate import Accelerator from accelerate.logging import get_logger from accelerate.utils import set_seed from diffusers import AutoencoderKL, DDPMScheduler, PNDMScheduler, StableDiffusionPipeline, UNet2DConditionModel from diffusers.hub_utils import init_git_repo, push_to_hub from diffusers.optimization import get_scheduler from diffusers.pipelines.stable_diffusion import StableDiffusionSafetyChecker from PIL import Image from tqdm.auto import tqdm from transformers import CLIPFeatureExtractor, CLIPTextModel, CLIPTokenizer,TrainingArguments import kornia.augmentation as K#augmentaiton import pandas as pd import wandb import subprocess parser = argparse.ArgumentParser() parser.add_argument(""--lr"",help=""learning rate"",default=5e-6,type=int) parser.add_argument(""--epochs"",default=12,type=int) parser.add_argument(""--train_unet"",help=""whether to train Unet or not"",default=False,type=bool) parser.add_argument(""--decay"",help=""weight_decay"",default=1e-2,type=int) parser.add_argument(""--train_text_encoder"",default=True,type=bool) parser.add_argument(""--data_root"",default=""../book dataset"",type=str) parser.add_argument(""--num_examples"",default=6000,type=int,help=""number of training examples"") parser.add_argument(""--num_devices"",default=3) parser.add_argument(""--gradient_acc_steps"",default=8,type=int) args = parser.parse_args() def image_grid(imgs, rows, cols):     assert len(imgs) == rows*cols      w, h = imgs[0].size     grid = Image.new('RGB', size=(cols*w, rows*h))     grid_w, grid_h = grid.size          for i, img in enumerate(imgs):         grid.paste(img, box=(i%cols*w, i//cols*h))     return grid      #For reproducibility def set_seed(seed: int = 42) -> None:     np.random.seed(seed)     random.seed(seed)     torch.manual_seed(seed)     torch.cuda.manual_seed(seed)     # When running on the CuDNN backend, two further options must be set     torch.backends.cudnn.deterministic = True     torch.backends.cudnn.benchmark = False     # Set a fixed value for the hash seed     os.environ[""PYTHONHASHSEED""] = str(seed)     print(f""Random seed set as {seed}"") global_seed = 42   # + id=""If5Jswe526QP"" #@markdown `pretrained_model_name_or_path` which Stable Diffusion checkpoint you want to use #pretrained_model_name_or_path = ""runwayml/stable-diffusion-v1-5"" #@param {type:""string""} pretrained_model_name_or_path =""CompVis/stable-diffusion-v1-4"" # data_root=""/kaggle/input/goodreads-best-books"" # label_root=""/kaggle/input/goodreads-best-book-cleaned-version"" data_root=args.data_root label_root=args.data_root  book_cover_templates=[#the first entry is for ""highly legible text""     ""A {} book cover with author {}, book title {} "",     #repeat some prompts to give model prior knowledge about book cover styles     ""A {} book cover written by author {} with book title {} "", #     ""A {} simple book cover with author {}, book title {} "", #     ""A plain {} book cover with author {}. The book title is{} "", #     ""A {} vivid book cover with author {}, book title {} "",     ""A  {} book cover with author name:{}, book title: {}"", # #     ""We are going to create a clear, {}, highly detailed book cover with author named {}, and book title is '{}'"", #     ""An intricate {}, book cover including book author:{}, book title: '{}'"", #     ""A detailed, {}, book cover with {} ,written by author {}"", #     ""A creative, colorful {}, book cover written by {}. The book title is {}, "", #     ""A {} old-fashioned, plain book cover written by {}. The book title is {}"", #     ""A simple, {}, old-fashioned book cover with author name {}, book title {} "", #     ""A simple, {}, plain book cover with author name {}, book title {} "",     ""A detailed {} book cover with author {} and book title {} ""      ] #TODO: add more to match the number of templates summary_placeholders=[     "", and summary: {}"",     ', and abstract: {}',     "",summary: {}"",     "", the book describes that {}"",     "", book discription: {}"",     "", main story: {}"",     "", the book is mainly about {}"",     "", and main story: {}"",     ""and book abstract: {}"",     "", and book description: {}"" ] test_templates=[#the first entry is for ""highly legible text""     ""A {} book cover with author {}, book title {} "",     #repeat some prompts to give model prior knowledge about book cover styles     ""A {} book cover written by author {} with book title {} "",     ""A {} simple book cover with author {}, book title {} "",     ""A plain {} book cover with author {}. The book title is{} "",     ""A {} vivid book cover with author {}, book title {} "",     ""A  {} book cover with author name:{}, book title: {}"", #     ""We are going to create a clear, {}, highly detailed book cover with author named {}, and book title is '{}'"",     ""An intricate {}, book cover including book author:{}, book title: '{}'"",     ""A detailed, {}, book cover with {} ,written by author {}"",     ""A creative, colorful {}, book cover written by {}. The book title is {}, "",     ""A {} old-fashioned, plain book cover written by {}. The book title is {}"",     ""A simple, {}, old-fashioned book cover with author name {}, book title {} "",     ""A simple, {}, plain book cover with author name {}, book title {} "",     ""A detailed {} book cover with author {} and book title {} ""      ] #pad to the same length  for i in range(len(summary_placeholders),len(test_templates)):   summary_placeholders+=[random.choice(summary_placeholders)] summary_placeholders=summary_placeholders[:len(test_templates)]  # imagenet_templates_small = [ #     ""a photo of a {}"", #     ""a rendering of a {}"", #     ""a cropped photo of the {}"", #     ""the photo of a {}"", #     ""a photo of a clean {}"", #     ""a photo of a dirty {}"", #     ""a dark photo of the {}"", #     ""a photo of my {}"", #     ""a photo of the cool {}"", #     ""a close-up photo of a {}"", #     ""a bright photo of the {}"", #     ""a cropped photo of a {}"", #     ""a photo of the {}"", #     ""a good photo of the {}"", #     ""a photo of one {}"", #     ""a close-up photo of the {}"", #     ""a rendition of the {}"", #     ""a photo of the clean {}"", #     ""a rendition of a {}"", #     ""a photo of a nice {}"", #     ""a good photo of a {}"", #     ""a photo of the nice {}"", #     ""a photo of the small {}"", #     ""a photo of the weird {}"", #     ""a photo of the large {}"", #     ""a photo of a cool {}"", #     ""a photo of a small {}"", # ]  # imagenet_style_templates_small = [ #     ""a painting in the style of {}"", #     ""a rendering in the style of {}"", #     ""a cropped painting in the style of {}"", #     ""the painting in the style of {}"", #     ""a clean painting in the style of {}"", #     ""a dirty painting in the style of {}"", #     ""a dark painting in the style of {}"", #     ""a picture in the style of {}"", #     ""a cool painting in the style of {}"", #     ""a close-up painting in the style of {}"", #     ""a bright painting in the style of {}"", #     ""a cropped painting in the style of {}"", #     ""a good painting in the style of {}"", #     ""a close-up painting in the style of {}"", #     ""a rendition in the style of {}"", #     ""a nice painting in the style of {}"", #     ""a small painting in the style of {}"", #     ""a weird painting in the style of {}"", #     ""a large painting in the style of {}"", # ]  # + id=""fcA-kMQblqUe"" #@title Training hyperparameters  hyperparam = {     ""learning_rate"": args.lr, #original: 5e-4     ""scale_lr"": False,     ""epochs"": args.epochs,     ""train_batch_size"": 1,     ""gradient_accumulation_steps"": args.gradient_acc_steps,     ""seed"": global_seed,     ""weight_decay"": args.decay,     # ""noise_scheduler"": ""DDIM"",     ""pretrained_model_name_or_path"": pretrained_model_name_or_path,     ""output_dir"": ""./model"",     ""training_dataset_size"":args.num_examples,     ""train_unet"": args.train_unet,     ""train_text_encoder"": args.train_text_encoder,     ""num_templates"": len(book_cover_templates),     ""include_summary"": False,#True to add book summary to prompts     ""templates"" : book_cover_templates }  # + id=""xp2InXqXW8aY"" outputId=""32997303-8897-4243-9a83-e4b75a03272e"" #@title Load the Stable Diffusion model  print(""before loading tokenizer: cuda:"",torch.cuda.is_available()) tokenizer = CLIPTokenizer.from_pretrained(     pretrained_model_name_or_path,     subfolder=""tokenizer"",     use_auth_token=True, )","Dec 25, 2022 5:28 am","zzzj","move all stuff about diffusers under a process including the sentence of import packages.","Apr 3, 2023 1:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How do you use SentencePiece for BPE of sequences with no whitespace","https://discuss.huggingface.co/t/how-do-you-use-sentencepiece-for-bpe-of-sequences-with-no-whitespace/1895","1","2.0k","Created: Nov 4, 2020 5:17 pm Latest: Apr 29, 2021 4:50 pm","wjs20","I am trying to use byte pair encoding on amino acid sequences which have no spaces: ADNRRPIWNLGHMVNALKQIPTFLXDGANA the tokenizers summary section of the docs states suggests SentencePiece could be useful, as it treats the input as a raw stream, includes the space in the set of characters to use, then uses BPE or unigram to construct the appropriate vocabulary. How would I train a tokenizer from scratch using SentencePiece? The tokenizer library seems to only support WordPiece.","Nov 4, 2020 5:17 pm","sbmaruf","In original sentencepiece model, white space is considered as a regular character. Please read the description here. GitHub GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural... Unsupervised text tokenizer for Neural Network-based text generation. - GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation. I am not totally familier with the huggingface implementation of the sentencepiece. But you can use the original sentencepiece library for that and then try loading that sentencepiece model by huggingface wrapper if needed.","Apr 29, 2021 4:50 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076","https://discuss.huggingface.co/t/arrowinvalid-column-3-named-attention-mask-expected-length-1000-but-got-length-1076/6904","3","2.5k","Created: Jun 16, 2021 9:08 pm Latest: Jul 26, 2023 6:33 pm","Peppe95","I‚Äôm trying to evaluate a QA model on a custom dataset. This is how I prepared the velidation features: def prepare_validation_features(examples):     # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results     # in one example possible giving several features when a context is long, each of those features having a     # context that overlaps a bit the context of the previous feature.     tokenized_examples = tokenizer(         examples[""question"" if pad_on_right else ""context""],         examples[""context"" if pad_on_right else ""question""],         truncation=""only_second"" if pad_on_right else ""only_first"",         max_length=max_length,         stride=doc_stride,         return_overflowing_tokens=True,         return_offsets_mapping=True,         padding=""max_length"",     )      # Since one example might give us several features if it has a long context, we need a map from a feature to     # its corresponding example. This key gives us just that.     sample_mapping = tokenized_examples.pop(""overflow_to_sample_mapping"")      # We keep the example_id that gave us this feature and we will store the offset mappings.     tokenized_examples[""example_id""] = []      for i in range(len(tokenized_examples[""input_ids""])):         # Grab the sequence corresponding to that example (to know what is the context and what is the question).         sequence_ids = tokenized_examples.sequence_ids(i)         context_index = 1 if pad_on_right else 0          # One example can give several spans, this is the index of the example containing this span of text.         sample_index = sample_mapping[i]         tokenized_examples[""example_id""].append(examples[""id""][sample_index])          # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token         # position is part of the context or not.         tokenized_examples[""offset_mapping""][i] = [             (o if sequence_ids[k] == context_index else None)             for k, o in enumerate(tokenized_examples[""offset_mapping""][i])         ]      return tokenized_examples But as I try to apply the function to my dataset: test_features = test_dataset.map(     prepare_validation_features,     batched=True, ) at a certain moment (more or less 23% of the process) it return me this error: 23% 5/22 [00:19<00:51, 3.04s/ba] --------------------------------------------------------------------------- ArrowInvalid                              Traceback (most recent call last) <ipython-input-156-6658d0dc57be> in <module>()       1 test_features = test_dataset.map(       2     prepare_validation_features, ----> 3     batched=True,       4 )  8 frames /usr/local/lib/python3.7/dist-packages/pyarrow/error.pxi in pyarrow.lib.check_status()  ArrowInvalid: Column 3 named attention_mask expected length 1000 but got length 1076 How can I fix it?","Jun 16, 2021 9:08 pm","alexandra","Hello! I have the same issue. Did you fix it?","Oct 20, 2021 3:56 pm","mariosasko","Hi @Peppe95 @alexandra, are you sure that each key/column in the returned batch has the same number of elements? You can check this by inserting the line: assert set(len(column_values) for column_values in returned_batch.values()) == 1, ""Mismatch in the number of elements"" before the return statement. Let me know if this helps.","Nov 5, 2021 12:49 pm","pols0","From Python docs I see that set() makes a ‚Äúsequence of iterable elements‚Äù. Why are you iterating through returned_batch.values(), creating a set of the lengths of the columns, and then checking whether the set == 1? How does this solve the shape error?","Jul 26, 2023 6:33 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to use tokenizer.tokenize in Chinese data properly?","https://discuss.huggingface.co/t/how-to-use-tokenizer-tokenize-in-chinese-data-properly/11491","0","887","Created: Nov 9, 2021 6:14 am Latest: Nov 9, 2021 6:14 am","victor181","When I add a Chinese token to the tokenizer, it can‚Äôt tokenize properly. How should I fix it? tokenizer = AutoTokenizer.from_pretrained(‚Äòbert-base-chinese‚Äô) text = [‚Äòcov19‚Äô, ‚ÄòÁóÖÊØí‚Äô] tokenizer.tokenize(text) # [‚Äòco‚Äô, ‚Äò##v‚Äô, ‚Äò##19‚Äô, ‚ÄòÁóÖ‚Äô, ‚ÄòÊØí‚Äô] tokenizer.add_tokens(text) tokenizer.tokenize(text) # [‚Äòcov19‚Äô, ‚ÄòÁóÖ‚Äô, ‚ÄòÊØí‚Äô] I will be grateful for any help you can provide.","Nov 9, 2021 6:14 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SentencePiece tokenizer encodes to unknown token","https://discuss.huggingface.co/t/sentencepiece-tokenizer-encodes-to-unknown-token/49113","0","839","Created: Aug 2, 2023 9:30 am","sytelus","I am using HuggigFace implementation of SentencePiece tokenizer, i.e., SentencePieceBPETokenizer and SentencePieceUnigramTokenizer classes. I train these tokenizers on dataset which has no unicode characters and then try to encode the string that does have unicode characters. My understanding is that SentencePiece is lossless and reversible and therefore it should always encode out-of-vocabulary tokens such that it can be decoded to same string, just like ByteLevelBPETokenizer tokenizer. So, theoretically, SentencePiece shouldn‚Äôt even need <unk> as special token. However, HuggingFace implementation does have parameter to specify unknown token as special token and it always encodes unseen unicode characters in input string as <unk>. My questions are, Is this expected with SentencePiece in general and therefore its claim being lossless not really true? Is this specific to HuggingFace implementation (but not to Google‚Äôs)? Is there anyway to make HuggingFace implementation perfectly lossless just like ByteLevelBPETokenizer? Thanks.","Aug 2, 2023 9:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Automatic sentence segmentation and encoding","https://discuss.huggingface.co/t/automatic-sentence-segmentation-and-encoding/1479","0","839","Created: Oct 12, 2020 9:43 am","vblagoje","Hello everyone, Is there a way to segment sentences and encode them using the tokenizers library automatically? I am currently using spacy and/or nltk for the sentence segmentation part and then tokenizers to encode the sentences. Many thanks.","Oct 12, 2020 9:43 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizing my novel for GPT model","https://discuss.huggingface.co/t/tokenizing-my-novel-for-gpt-model/33532","0","836","Created: Mar 10, 2023 8:36 pm","christinazhou96","Hello, I‚Äôm a fiction author and I wanted to finetune a pretrained GPT model to see what would happen if I asked it to write more chapters of my novel (30k words), in my style. Unfortunately I think I‚Äôm getting stuck on tokenizing my novel and would appreciate any help. I‚Äôve written code to train the novel and it runs up until I call the actual training part: from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments from datasets import Dataset, load_dataset  # Step 1: Import my novel import docx import pandas as pd  # Read each paragraph from a Word file doc = docx.Document(r""C:\Users\chris\Downloads\The Black Squirrel (1).docx"") paras = [p.text for p in doc.paragraphs if p.text]  # Convert list to dataframe df = pd.DataFrame(paras) df.reset_index(drop=False,inplace=True) df.rename(columns={'index':'label',0:'text'},inplace=True)  # Split my novel into train and test from sklearn.model_selection import train_test_split  train, test = train_test_split(df, test_size=0.05)  # Export novel as CSV to be read by Huggingface library train.to_csv(r""C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv"", index=False) test.to_csv(r""C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv"", index=False)  # Tokenize novel datasets = load_dataset('csv',                        data_files={'train':r""C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_train.csv"",                        'test':r""C:\Users\chris\OneDrive\Documents\ML\data\black_squirrel_dataset_test.csv""})  # Instantiate tokenizer tokenizer = GPT2Tokenizer.from_pretrained(""EleutherAI/gpt-neo-1.3B"",                                           pad_token='[PAD]')  # Do I need the below? # tokenizer.enable_padding(pad_id=tokenizer.token_to_id('[PAD]')) paragraphs = df['text'] max_length = max([len(tokenizer.encode(paragraphs)) for paragraphs in paragraphs])  # Tokenize my novel def tokenize_function(examples):     return tokenizer(examples[""text""], padding='max_length', truncation=True)  tokenized_datasets = datasets.map(tokenize_function, batched=True)  # Step 2: Train the model model = GPTNeoForCausalLM.from_pretrained(""EleutherAI/gpt-neo-1.3B"")  model.resize_token_embeddings(len(tokenizer))  training_args = TrainingArguments(     output_dir=r""C:\Users\chris\OneDrive\Documents\ML\models"",     overwrite_output_dir=True,     num_train_epochs=3,     per_device_train_batch_size=32, # batch size for training     per_device_eval_batch_size=64,  # batch size for evaluation     eval_steps = 400, # Number of update steps between two evaluations.     save_steps=800, # after # steps model is saved     warmup_steps=500,# number of warmup steps for learning rate scheduler     )  trainer = Trainer(     model=model,     args=training_args,     train_dataset=tokenized_datasets['train'],     eval_dataset=tokenized_datasets['test'] )  trainer.train() But when I train, I get a memory error: The following columns in the training set don't have a corresponding argument in `GPTNeoForCausalLM.forward` and have been ignored: text. If text are not expected by `GPTNeoForCausalLM.forward`,  you can safely ignore this message. C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning   FutureWarning, ***** Running training *****   Num examples = 779   Num Epochs = 3   Instantaneous batch size per device = 32   Total train batch size (w. parallel, distributed & accumulation) = 32   Gradient Accumulation steps = 1   Total optimization steps = 75   Number of trainable parameters = 1315577856   0%|          | 0/75 [19:12<?, ?it/s] Traceback (most recent call last):   File ""C:\Users\chris\AppData\Local\Programs\Python\Python37\lib\code.py"", line 90, in runcode     exec(code, self.locals)   File ""<input>"", line 9, in <module>   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py"", line 1547, in train     ignore_keys_for_eval=ignore_keys_for_eval,   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py"", line 1791, in _inner_training_loop     tr_loss_step = self.training_step(model, inputs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py"", line 2539, in training_step     loss = self.compute_loss(model, inputs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\trainer.py"", line 2571, in compute_loss     outputs = model(**inputs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py"", line 752, in forward     return_dict=return_dict,   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py"", line 627, in forward     output_attentions=output_attentions,   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py"", line 342, in forward     feed_forward_hidden_states = self.mlp(hidden_states)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\models\gpt_neo\modeling_gpt_neo.py"", line 300, in forward     hidden_states = self.act(hidden_states)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\torch\nn\modules\module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""C:\Users\chris\PycharmProjects\37venv\lib\site-packages\transformers\activations.py"", line 35, in forward     return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0)))) RuntimeError: [enforce fail at C:\actions-runner\_work\pytorch\pytorch\builder\windows\pytorch\c10\core\impl\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 405798912 bytes. I successfully fine tuned a pretrained BERT model last night, so I know I should be able to run this. Also, I have 16GB RAM so 405 MB shouldn‚Äôt actually be a problem. I am running on CPU, so I know that impacts performance but I feel like here that shouldn‚Äôt lead to a memory error. Since I‚Äôm new to tokenization, I feel like maybe my tokens are too big, or I messed up with padding, or other arguments that GPT Neo required. (I can change to any GPT model, but wanted to see GPT Neo since I‚Äôve read it‚Äôs newer.) I‚Äôm specifically wondering if the fact that I‚Äôm entering in paragraphs is an issue. I write long paragraphs, so I think that the token length (which seemed to be at 387 for my longest paragraphs, unless I did that wrong) could be impacting my memory. I can try to read in sentences, but the code I‚Äôve found to break my novel up in sentences will be a little inaccurate as it breaks up by . which won‚Äôt work if the sentence ends with a !, dialogue mark, ellipses, etc. I would appreciate any help. For anyone interested in replicating this specifically, here‚Äôs a link to the Google Doc of my novel, The Black Squirrel, where you can download as docx. Thank you! Christina","Mar 10, 2023 8:36 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenized sequence lengths","https://discuss.huggingface.co/t/tokenized-sequence-lengths/15117","6","1.8k","Created: Feb 24, 2022 7:29 pm Latest: Mar 10, 2022 7:21 pm","jbmaxwell","I have a strange situation where I‚Äôm trying to build a custom tokenizer for a custom ‚Äúlanguage‚Äù (encoded music). The language is designed to represent the data in a way that is compact and also reasonably human-readable. In most cases, the ideal tokenization would be essentially word-level, just separating by whitespace, but because I don‚Äôt necessarily know in advance all the possibilities, I want to avoid just forcing word-level tokenization. What I‚Äôm finding is that, if I train a tokenizer using the basic ‚Äúintro‚Äù approach on the Tokenizers doc page, e.g.: tokenizer = Tokenizer(BPE()) corpus_root = './content' paths = [str(x) for x in Path(corpus_root).glob(""**/*.txt"")]  tokenizer.pre_tokenizer = Whitespace() from tokenizers.trainers import BpeTrainer  trainer = BpeTrainer(special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""], vocab_size=4000) tokenizer.train(files=paths, trainer=trainer) then I get pretty much the ideal vocab, and the encoding works as expected. However, this tokenizer seems to work differently to those in the Transformers library (e.g., it doesn‚Äôt recognize tokenizer.vocab, and so on), so using a more ‚Äústock‚Äù Transformers tokenizer seems like the safer/easier option going forward. As a workaround, I tried saving the tokenizer files using: tokenizer.model.save('./tokenizer/roberta_tokenizer') tokenizer.save('./tokenizer/roberta_tokenizer/config.json') and then loading this into a RobertaTokenizer, using the saved vocab.json and merges.txt files. Although this does function, what I‚Äôm noticing is that the lengths of the encoded tokenizations are dramatically different‚Äîup to 3x longer for the same input when using RobertaTokenizer over the Tokenizer(BPE.from_file()) version, using the same files, e.g.: count = len(input.split(' ')) output = tokenizer.encode(input) print(f'{output.tokens}, count = {count}, encoded count = {len(output.ids)}') tokenizer.model.save('./tokenizer/roberta_tokenizer') tokenizer.save('./tokenizer/roberta_tokenizer/config.json') roberta_tokenizer = RobertaTokenizer('./tokenizer/roberta_tokenizer/vocab.json', './tokenizer/roberta_tokenizer/merges.txt') roberta_tokenizer.save_pretrained('./tokenizers/roberta-tokenizer')  test_tok = AutoTokenizer.from_pretrained('./tokenizers/roberta-tokenizer') test_tok2 = Tokenizer(BPE.from_file('./tokenizer/roberta_tokenizer/vocab.json', './tokenizer/gpt2_tokenizer/merges.txt'))  test = test_tok.encode(input) print(""Roberta test: "", test)  test2 = test_tok2.encode(input) print(""Tokenizer(BPE()) test: "", test2.ids)  -------   ""Roberta test:  [4000, 192, 357, 52, 5, 194, 661, 11, 33, 32, 9, 39, 9, 111, 51, 58, 6, 130, 53, 46, 51, 58, 6, 146, 6, 260, 58, 6, 1372, 7, 584, 7, 1876, 58, 6, 99, 101, 9, 37, 8, 59, 6, 151, 9, 77, 695, 11, 4001]"" ""Tokenizer(BPE()) test:  [939, 711, 196, 115, 131, 120, 295, 165, 233, 309, 99, 395, 100, 335, 568]"" I tried testing this with a small, natural-language dataset from the Datasets page, and I don‚Äôt see these dramatic differences‚Äîi.e., I only see differences of a few tokens, which could be explained by special/added tokens. So I‚Äôm assuming that it‚Äôs my ‚Äúsynthesized‚Äù language that is the difference. So is there a way to use Tokenizer(BPE.from_file()), but still have all the integration features of using, for example, RobertaTokenizer from the Transformers library? I have been posting some of my saga on the Discord server, but I think this is probably a better place for something so detailed (and mysterious). Any help would be greatly appreciated.","Feb 24, 2022 7:29 pm","jbmaxwell","Or, alternately, does anyone know why: tokenizer = Tokenizer(BPE.from_file('./tokenizer/roberta_tokenizer/vocab.json', './tokenizer/roberta_tokenizer/merges.txt')) print(""vocab_size: "", tokenizer.model.vocab) Fails with an error that 'tokenizers.models.BPE' object has no attribute 'vocab'. According to the docs, it should have: Input sequences ‚Äî tokenizers documentation According to tokenizers.__version__ I‚Äôm running 0.11.0. These docs are for 0.10.0‚Äîis vocab removed in 0.11.0? Or is something just borked in my install? UPDATE: I gave 0.10.1 a try, just for kicks, but same error.","Feb 25, 2022 12:07 am","jbmaxwell","Digging in further, it looks like the difference must be between BPE and ByteLevelBPETokenizer (i.e., RoBERTa‚Äôs tokenizer). With the former, I get the 4000 item vocab I want, but the latter only gives me a 1300 item vocab (despite indicating 4000 in the vocab_size). So to get what I‚Äôm after, I have to either; figure out how to get the BPE version into a tokenizer that plays nice with transformers OR figure out how to get the ByteLevelBPETokenizer to learn a 4000 item vocab","Feb 25, 2022 2:05 am","jbmaxwell","Okay, I‚Äôve made some progress with this approach: from tokenizers import Tokenizer from tokenizers.models import BPE from tokenizers.trainers import BpeTrainer from tokenizers.pre_tokenizers import Whitespace from transformers import PreTrainedTokenizerFast  train = True  if train:     tokenizer = Tokenizer(BPE(unk_token=""[UNK]""))     trainer = BpeTrainer(special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""])      tokenizer.pre_tokenizer = Whitespace()     tokenizer.train(files, trainer)      tokenizer.save('./tokenizer/bpe_tokenizer/tokenizer.json')   fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=""./tokenizer/bpe_tokenizer/tokenizer.json"")  test = fast_tokenizer.encode(input) print(""test tokenization: "", test)  decode_test = fast_tokenizer.decode(test) print(""test decode: "", decode_test) Then I modified run_clm.py to use tokenizer = PreTrainedTokenizerFast(tokenizer_file=model_args.model_name_or_path) instead of the original AutoTokenizer version. This now works as expected. Worth noting is that you can‚Äôt use PreTrainedTokenizer (i.e., the slow version) or you‚Äôll hit a NotImplemented error when trying to call tokenizer.encode(input).","Feb 25, 2022 10:56 pm","mmalandro","That‚Äôs right. The methods of the tokenizers from the tokenizers library are different from the methods of the tokenizers from the transformers library. You want to use PreTrainedTokenizerFast from the transformers library to access the functionality of the transformers tokenizers. I suspect you want to be working with a character-level BPE rather than a byte-level BPE tokenizer. I‚Äôm also working on encoded music. We should talk","Mar 1, 2022 2:01 am","jbmaxwell","Yes, that‚Äôs what I finally realized, in a kind of roundabout way. And yes again, we should talk!","Mar 1, 2022 4:10 pm","jbmaxwell","btw, Huggingface people, I‚Äôm still wondering if there‚Äôs any way to force a larger vocabulary during training? Presumably this would just be more ‚Äúmerging‚Äù, no? Shouldn‚Äôt there be a parameter to force a larger vocab if you want it? EDIT: I notice I was apparently getting the 4000 word vocab when I posted this, but that‚Äôs not the case now‚Ä¶ I request vocab_size=4000 and I get 2026. Hmm‚Ä¶","Mar 10, 2022 7:12 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Sliding window for Long Documents","https://discuss.huggingface.co/t/sliding-window-for-long-documents/19367","1","1.8k","Created: Jun 20, 2022 7:51 am Latest: Feb 9, 2023 10:34 pm","shreyans92dhankhar","Hi, Is there any way to chunk a large document with left and right context? The default param in tokenizer provide only left context, is there a way we can provide right context also and predict for the central part not for the context. Similar to the approach mention in this paper: https://arxiv.org/pdf/2011.06993.pdf","Jun 20, 2022 7:51 am","melhoushi","return_over_flowing_tokens might help: github.com huggingface/notebooks/blob/main/examples/question_answering.ipynb {  ""cells"": [   {    ""cell_type"": ""markdown"",    ""metadata"": {     ""id"": ""X4cRE8IbIrIV""    },    ""source"": [     ""If you're opening this Notebook on colab, you will probably need to install ü§ó Transformers and ü§ó Datasets. Uncomment the following cell and run it.""    ]   },   {    ""cell_type"": ""code"",    ""execution_count"": null,    ""metadata"": {     ""colab"": {      ""base_uri"": ""https://localhost:8080/"",      ""height"": 1000     },     ""id"": ""MOsHUjgdIrIW"", This file has been truncated. show original","Feb 9, 2023 10:34 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Further pre-training the tokenizer?","https://discuss.huggingface.co/t/further-pre-training-the-tokenizer/17360","0","808","Created: Apr 30, 2022 2:31 pm","petarulev","Hello! Currently, I am pre-training roberta with MLM from scratch by : 1. Training a tokenizer on my domain (all the texts that I have) 2. Masking 15% of the words (which are not special tokens) 3. Passing attention mask, labels and input_ids to a RobertaForMaskedLM. It has been training for a couple of hours and it seems OK. My questions are: because I have around 2GB of data (which is not much), my idea is to ‚Äúfurther pre-train‚Äù the roberta model (just use RobertaForMaskedLM.from_pretrained(‚Ä¶)) , like a transfer-learning task. This seems smart, but catastrophic forgetting may occur, and what mostly concerns me is: I have created a entirely new tokenizer on my own data, but roberta originally has another tokenizer. If I want to further pre-train with a different tokenizer, this will totally confuse the model, since the old weights would be for different tokenizer, right? How can I approach the problem with further pre-training then? My tokenizer code: from pathlib import Path from tokenizers import ByteLevelBPETokenizer  paths = [str(x) for x in Path('data').glob('*.txt')]  tokenizer = ByteLevelBPETokenizer()  tokenizer.train(files=paths, vocab_size=50_000, min_frequency=2,                 special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])  tokenizer.save_model('CustomBertTokenizer')] Thanks.","Apr 30, 2022 2:31 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I was using huugginfface meta-llama/Llama-2-7b-chat-hf and im facing an error","https://discuss.huggingface.co/t/i-was-using-huugginfface-meta-llama-llama-2-7b-chat-hf-and-im-facing-an-error/57742","2","2.5k","Created: Oct 7, 2023 6:34 pm Latest: Oct 8, 2023 5:40 am","amalsalilanhuggingfa","OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on ‚Äòhttps://huggingface.co/models‚Äô If this is a private repository, make sure to pass a token having permission to this repo with use_auth_token or log in with huggingface-cli login and pass use_auth_token=True.","Oct 7, 2023 6:34 pm","osanseviero","Hi there! Did you follow the error instructions? (either logging in or passing the token)","Oct 7, 2023 8:48 pm","amalsalilanhuggingfa","Yes i have followed the error and i got access to the model through meta,hugginface also","Oct 8, 2023 5:40 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using whitespace tokenizer for training models","https://discuss.huggingface.co/t/using-whitespace-tokenizer-for-training-models/6591","1","3.0k","Created: Jun 5, 2021 10:14 pm Latest: Jun 6, 2021 2:03 pm","Neel-Gupta","I have a dataset for which I wanted to use a tokenizer based on whitespace rather than any subword segmentation approach. This snippet I got off github has a way to construct and use the custom tokenizer that operates on whitespaces:- from tokenizers import Tokenizer, trainers from tokenizers.models import BPE from tokenizers.normalizers import Lowercase from tokenizers.pre_tokenizers import CharDelimiterSplit  # We build our custom tokenizer: tokenizer = Tokenizer(BPE())  tokenizer.normalizer = Lowercase() tokenizer.pre_tokenizer = CharDelimiterSplit(' ')  # We can train this tokenizer by giving it a list of path to text files: trainer = trainers.BpeTrainer(special_tokens=[""[UNK]""], show_progress=True) tokenizer.train(files=['/content/dataset.txt'], trainer=trainer) I wanted to use it for pre-training the BigBird attention model, but facing two issues: I can‚Äôt seem to be able to use this snippet with the custom tokenizer above to convert tokenized sentences in model-friendly sequences from tokenizers.processors import BertProcessing  tokenizer._tokenizer.post_processor = tokenizers.processors.BertProcessing(     (""</s>"", tokenizer.token_to_id(""</s>"")),     (""<s>"", tokenizer.token_to_id(""<s>"")), ) tokenizer.enable_truncation(max_length=16000) This returns me an error, and without any preprocessing the output does not contain the sequence start and end tokens (<s>; </s>) as expected. Next problem arises, when I save the tokenizer state in the specified folder, I am unable to use it via: tokenizer = BigBirdTokenizerFast.from_pretrained(""./tok"", max_len=16000) since it yields the error that my directory does not ‚Äòreference‚Äô the tokenizer files, which shouldn‚Äôt be an issue since using RobertaTokenizerFast does work - I assume it has something to do in the tokenization post-processing phase. If anyone wants, I can create a reproducible colab notebook to speed up the issue being solved. Thanks in advance, N","Jun 5, 2021 10:14 pm","Neel-Gupta","I have created a fully reproducible colab notebook, with commented problems and synthetic data. Please find it here. Thanx","Jun 6, 2021 2:03 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizers Wheel Takes Forever to Build","https://discuss.huggingface.co/t/tokenizers-wheel-takes-forever-to-build/17114","1","2.9k","Created: Apr 23, 2022 6:56 pm Latest: May 8, 2023 3:22 am","maxdavish","Hey all - I have a Docker image that deploys a model using transformers on Google Cloud Run. Here‚Äôs what my Dockerfile looks like: FROM python:3.10-slim  ENV PYTHONUNBUFFERED True  #set up environment RUN apt-get update && apt-get install --no-install-recommends --no-install-suggests -y curl RUN apt-get install unzip RUN apt-get -y install python3 RUN apt-get -y install python3-pip  RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y ENV PATH=""/root/.cargo/bin:${PATH}""  ENV APP_HOME /app WORKDIR $APP_HOME COPY . ./  RUN pip3 install torch --extra-index-url https://download.pytorch.org/whl/cpu RUN pip3 install --no-cache-dir -r requirements.txt   CMD exec gunicorn --bind :$PORT --workers 1 --threads 8 --timeout 0 app:app This does build properly, but it takes extremely long to build on Google Cloud Run (like 30+ minutes). It specifically gets stuck on the Building wheel for tokenizers (pyproject.toml). Do you have any idea why this takes so long or if there‚Äôs anything that can be done to speed it up?","Apr 23, 2022 6:56 pm","jayyouth","running to the same problem, how should it be solved?","May 8, 2023 3:21 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HuggingFace BPE Trainer Error - Training Tokenizer","https://discuss.huggingface.co/t/huggingface-bpe-trainer-error-training-tokenizer/10629","1","2.9k","Created: Oct 8, 2021 9:07 pm Latest: Jul 14, 2022 6:35 pm","mgiardinelli","I am trying to train a ByteLevelBPETokenizer using an iterable instead of from files. There must be something I am doing wrong when I instantiate the trainer, but I can‚Äôt tell what it is. When I try to train the tokenizer with my dataset (clothing data from Kaggle) + the BpeTrainer, I get an error. Also posted on Stackoverflow: nlp - HuggingFace BPE Trainer Error - Training Tokenizer - Stack Overflow **TypeError**: 'tokenizers.trainers.BpeTrainer' object cannot be interpreted as an integer I am using Colab Step 1: Install tokenizers & download the Kaggle data !pip install tokenizers  # Download clothing data from Kaggle # https://www.kaggle.com/nicapotato/womens-ecommerce-clothing-reviews/version/1?select=Womens+Clothing+E-Commerce+Reviews.csv Step 2: Upload the file # use colab file upload from google.colab import files uploaded = files.upload() Step 3: Clean the data (remove floats) & run trainer import io import pandas as pd    # convert the csv to a dataframe so it can be parsed data = io.BytesIO(uploaded['clothing_dataset.csv'])  df = pd.read_csv(data)  # convert the review text to a list so it can be passed as iterable to tokenizer clothing_data = df['Review Text'].to_list()  # Remove float values from the data clean_data =  []     for item in clothing_data:   if type(item) != float:     clean_data.append(item)      from tokenizers import ByteLevelBPETokenizer from tokenizers.processors import BertProcessing from tokenizers import trainers, pre_tokenizers from tokenizers.trainers import BpeTrainer from pathlib import Path   # Initialize a tokenizer tokenizer = ByteLevelBPETokenizer(lowercase=True)  # Intantiate BpeTrainer trainer = BpeTrainer(     vocab_size=20000,     min_frequence = 2,     show_progress=True,     special_tokens=[""<s>"",""<pad>"",""</s>"",""<unk>"",""<mask>""],)  # Train the tokenizer tokenizer.train_from_iterator(clean_data, trainer) Error - I can see that the trainer is a BpeTrainer Type. --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) <ipython-input-103-7738a7becb0e> in <module>()      34       35 # Train the tokenizer ---> 36 tokenizer.train_from_iterator(clean_data, trainer)  /usr/local/lib/python3.7/dist-packages/tokenizers/implementations/byte_level_bpe.py in train_from_iterator(self, iterator, vocab_size, min_frequency, show_progress, special_tokens)     119             show_progress=show_progress,     120             special_tokens=special_tokens, --> 121             initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),     122         )     123         self._tokenizer.train_from_iterator(iterator, trainer=trainer)  TypeError: 'tokenizers.trainers.BpeTrainer' object cannot be interpreted as an integer Interesting Note: If I set the input trainer=trainer I get this --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) <ipython-input-104-64737f948e6d> in <module>()      34       35 # Train the tokenizer ---> 36 tokenizer.train_from_iterator(clean_data, trainer=trainer)  TypeError: train_from_iterator() got an unexpected keyword argument 'trainer'","Oct 8, 2021 9:07 pm","drsis","A little late but I think you have to call the train_from_iterator in such a way:  tokenizer.train_from_iterator(clean_data, vocab_size=20000,      min_frequence = 2,      show_progress=True,      special_tokens=[""<s>"",""<pad>"",""</s>"",""<unk>"",""<mask>""]) You can see from the sourcecode (tokenizers/byte_level_bpe.py at main ¬∑ huggingface/tokenizers ¬∑ GitHub) how the method from ByteLevelBPETokenizer has to be called.","Jul 14, 2022 6:35 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does a tokenizer keep the mapping between my labels to their encoding?","https://discuss.huggingface.co/t/does-a-tokenizer-keep-the-mapping-between-my-labels-to-their-encoding/16296","3","2.0k","Created: Mar 31, 2022 7:49 am Latest: Apr 4, 2022 1:53 pm","revuze","And if not, where can I keep it? Do I have to map it myself or can it be done automatically somewhere? I have a multi class use case, and so far I‚Äôve used LabelEncoder/Binarizer class. Is there a better way? Thanks","Mar 31, 2022 7:49 am","lucadini","If you ask this for token classification, you can pass already tokenized text to the tokenizer and set the parameter is_split_into_words=True. The tokenized text will have the parameter word_ids that contains the mapping from your tokenization and the additional tokenization done by the tokenizer. In this script for token classification there is tokenize_and_align_labels function that I think does what you need.","Mar 31, 2022 1:24 pm","revuze","I see, thank you, but actually I was referring more to the labels of prediction. Maybe it‚Äôs unrelated to tokenizers? Say I got 3 labels, the simple mapping from [0,1,2] to [‚Äúlabel1‚Äù, ‚Äúlabel2‚Äù, ‚Äúlabel3‚Äù] - I assume there should be a place the mapping is saved automatically but maybe I‚Äôm wrong?","Apr 3, 2022 8:17 am","lucadini","I‚Äôm sorry I didn‚Äôt get it! In the same script you can find ‚Äúid2label‚Äù and ‚Äúlabel2id‚Äù that are manually set and passed to the model‚Äôs config. These are dictionaries mapping from labels to label ids and vice versa. Try to check if your model config has this parameter set or do the mapping manually and then add the parameter!","Apr 4, 2022 1:53 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Disabling addition of CLS from BERT tokenizer","https://discuss.huggingface.co/t/disabling-addition-of-cls-from-bert-tokenizer/15538","5","1.6k","Created: Mar 9, 2022 2:41 pm Latest: Mar 11, 2022 9:39 am","AfonsoSousa","Hi. I am using the BERT Huggingface tokenizer for a sequence-to-sequence task. I am using an LSTM-based Encoder-Decoder architecture. I want my input to the decoder to start with the sep_token followed by the target sentence shifted one character to the right. I built the sentence as I wished to, but when I tokenized it, the [CLS] token is always added to the beginning of the sentence. How can I disable this addition? Thanks in advance for any help you can provide.","Mar 9, 2022 2:41 pm","christopher","Hello there. The [CLS] token is added by the tokenizer‚Äôs post_processor. I believe you will have to use the HF tokenizers library to define your own. See: Input sequences ‚Äî tokenizers documentation","Mar 11, 2022 7:13 am","BramVanroy","You should be able to add add_special_tokens=False to the tokenizer(your_input, add_special_tokens=False) call.","Mar 11, 2022 7:43 am","christopher","Wouldn‚Äôt that also disable the sep_token OP mentioned?","Mar 11, 2022 8:19 am","BramVanroy","It seems to me that they want to add a SEP token at the start of the string, but that the other parts are already formatted as they want/do not need other special tokens. So they should be able to do something like this (untested): encoded = tokenizer(your_input, add_special_tokens=False) encoded[""input_ids""] = [tokenizer.sep_token_id] + encoded[""input_ids""]","Mar 11, 2022 8:39 am","AfonsoSousa","Thank you. That solved the issue.","Mar 11, 2022 9:39 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Cannot initialize deberta-v3-base tokenizer","https://discuss.huggingface.co/t/cannot-initialize-deberta-v3-base-tokenizer/15322","2","1.3k","Created: Mar 3, 2022 7:14 pm Latest: Oct 9, 2022 4:05 pm","MagicFlute","when calling tokenizer = AutoTokenizer.from_pretrained(""microsoft/deberta-v3-base"") I get a ValueError: This tokenizer cannot be instantiated. Please make sure you have sentencepiece installed in order to use this tokenizer. Tried conda install -c conda-forge transformers[sentencepiece] as well as conda install -c conda-forge sentencepiece to no result","Mar 3, 2022 7:14 pm","simonschoe","Running pip install sentencepiece and restarting the kernel should do the trick.","Sep 21, 2022 12:08 pm","carlosaguayo","simonschoe: pip install sentencepiece Thank you!","Oct 9, 2022 4:05 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Train wordpiece from scratch","https://discuss.huggingface.co/t/train-wordpiece-from-scratch/9843","2","1.3k","Created: Sep 9, 2021 10:06 am Latest: Sep 9, 2021 11:56 am","Kamel","Hi, I am pre training a Bert model from scratch. For that I first need to train a wordpiece tokenizer, I am using BertWordPieceTokenizer for this. My question: Should I train the tokenizer on the whole corpus which is huge, or training it on a sample is enough? Is there a way to tell the tokenizer to take train only on a sample? Thanks.","Sep 9, 2021 10:06 am","nielsr","Kamel: Should I train the tokenizer on the whole corpus which is huge Yes. With HuggingFace Tokenizers, it takes seconds. From the README: ‚ÄúTakes less than 20 seconds to tokenize a GB of text on a server‚Äôs CPU‚Äù.","Sep 9, 2021 11:41 am","Kamel","Thanks again Nielsr","Sep 9, 2021 11:56 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Best way to mask a multi-token word when using `.*ForMaskedLM` models","https://discuss.huggingface.co/t/best-way-to-mask-a-multi-token-word-when-using-formaskedlm-models/6428","2","2.2k","Created: May 27, 2021 4:39 pm Latest: Apr 4, 2022 5:47 pm","emilylearning","For example, in a context where the model is likely to predict the word seaplane (which gets decomposed into two tokens), should I include a single mask or two masks in the contextual sentence? Here is a complete example: Google Colaboratory Below is the predicted top 6 words for a single mask (where the word seaplane should go). Here it seems reasonable to concatenate the top two predicted vocab words, but this doesn‚Äôt seem to extend into the less probable words in the list below. top_vocab_idxes = torch.topk(torch.softmax(single_mask_token_logits[masked_idx], dim=0), 6) for token_id in top_vocab_idxes[1]:     print (tokenizer.decode([token_id])) sea plane hangar helicopter lake river Below is result for using two masks in the contextual sentence, printing out the top 6 most likely combos for first and second masked tokens in each line. top_vocab_idxes = torch.topk(probs, 6) for token_id in torch.transpose(top_vocab_idxes[1], 1, 0):     print (tokenizer.decode(token_id)) sea plane water area mountain hangar land dive landing aircraft flying field In this particular case the top 3 most probable combos above seem like reasonable predictions for the two masked tokens given context: double_mask_sentence = f""""""When taking off in a seaplane, flying in a seaplane, and then landing in a {tokenizer.mask_token} {tokenizer.mask_token}, remember to fashion your seat belt."""""" It seems likely that I should use the second method above for my inference and possible later fine-tuning, however, I doubt this is what is done during pretraining. Thank you for any feedback on what might be best practice here.","May 27, 2021 4:39 pm","pafitis","This is something of interest for me too! This might be of help: [2009.07118] It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners","Jul 13, 2021 2:57 pm","emilylearning","Following up here, sorry didn‚Äôt do sooner. My concern at the time of writing this post was that the masking of a single sub-word token (that is part of a multi-token word), may influence the predictions of the words immediately before or after, that token by ‚Äòleaking‚Äô the information that the predicted tokens should also be a sub-word token (to later be combined with the unmasked subword token). It seems one way to fix this is by using ‚Äòwhole-word-masking‚Äô as shown in this notebook, whole_word_to_mask_multitoken_words_in_*ForMaskedLM.ipynb. However, I wasn‚Äôt so sure if this ‚Äòleaking‚Äô of information was really happening or not, so I just tried a slightly different experiment: explicitly replacing a single-word token_id (for the word plane) with a subword token_id (also for the word plane, but its subword variant), and then masking the token proceeding it. I also changed the sentence so that both subword and single word tokens may be the appropriate top predictions for the masked word. # Step 1: mask `sea` like word before single word `plane` single_mask_mutitoken_sentence = f""""""When taking off in a small seaplane, flying in any small plane, and then landing in a {tokenizer.mask_token} plane, remember to fashion your seat belt.""""""  # Step 2: swap `plane` token for `_plane` token single_mask_mutitoken_swapped_input = torch.where( single_mask_mutitoken_input == plane_id, _plane_id, single_mask_mutitoken_input ) And we do see the top tokens predicted to proceed a masked subword, are all themselves subword appropriate tokens. This is the case, even though the single-token word small would have been a pretty suitable prediction. for token_id in top_vocab_idxes[1]: print (tokenizer.decode([token_id]))  bi sea land float ##ero sail as can be seen in this notebook: possible_leaking_of_subwords_in_multitoken_words_in_*ForMaskedLM.ipynb‚Ä¶ indicating to me that some leaking is in fact going on. Would love other‚Äôs thoughts on this!","Apr 4, 2022 5:47 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Index of wordpieces (subwords) after tokenization by transformers","https://discuss.huggingface.co/t/index-of-wordpieces-subwords-after-tokenization-by-transformers/9552","0","682","Created: Aug 28, 2021 4:08 pm Latest: Aug 28, 2021 4:16 pm","fatihbeyhan","Hello, I am trying to find if there is any way to get wordpiece information after tokenization. I can think of many ways to do it manually but I am curious about whether there is built-in parameter that I can set and make my tokenizer return these. I already checked the parameters but none of them worked. So, I am here to see if anyone know something like that. Example: for BERT: lets say tokens are these >> [""[CLS]"", ‚Äúmy‚Äù, ‚Äútoken‚Äù, ‚Äú##ized‚Äù, ‚Äúwords‚Äù, ‚Äú!‚Äù, ‚Äú[SEP]‚Äù, ‚Äú[PAD]‚Äù] what I want >> [0, 0, 0, 1, 0, 0, 0, 0] for RoBERTa: lets say tokens are these >> [‚Äô<s>‚Äô, ‚Äòmy‚Äô, ‚Äòƒ†token‚Äô, ‚Äòized‚Äô, ‚Äòƒ†words‚Äô, ‚Äò!‚Äô, ‚Äò</s>‚Äô, ‚Äú[PAD]‚Äù] what I want >> [0, 0, 0, 1, 0, 0, 0, 0] If I implement a function to return these, it will be rule based and model dependent since they use different ways to tokenize. **WHY I NEED IT: I am trying to shape my dataset for token classification, hence, I need wordpiece and special token information. There is a parameter for special tokens but not for wordpieces.","Aug 28, 2021 4:08 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can‚Äôt load tokenizer using from_pretrained, Inference API","https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-inference-api/85235","4","1.7k","Created: May 5, 2024 8:59 am Latest: May 6, 2024 12:55 pm","asif00","Can't load tokenizer using from_pretrained, please update its configuration: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 560, column 3 Here are my project files: Files. I could use the model locally from the local checkpoint folder after the finetune; however, when I upload the same checkpoint folder on hugginceface as a model, it doesn‚Äôt seem to work anymore. Could anyone please help me with that! Thank you!","May 5, 2024 8:59 am","nielsr","Thanks for reporting, will forward to the team! Duplicate of Model Inference API error","May 6, 2024 9:00 am","asif00","Thank you! Looking forward to hearing from the team!!!","May 6, 2024 10:09 am","asif00","Could use the same model uploaded on Kaggle. However, when used with AutoTokenizer, it complained, worked with MBart50Tokenizer.","May 6, 2024 12:41 pm","asif00","Update: Works with MBart50Tokenizer from the HuggingFace checkpoint as well. Here is the Space I was testing it.","May 6, 2024 12:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What is Wav2Vec2FeatureExtractor doing?","https://discuss.huggingface.co/t/what-is-wav2vec2featureextractor-doing/16423","0","670","Created: Apr 3, 2022 8:11 pm Latest: Apr 4, 2022 1:57 am","bennicholl","I have a very good understanding of traditional transformer architectures for NLP, but I have recently been given a task which requires raw audio. I understand that tokenizers for BERT map a word to a specific indice, where that indice points to a word vector in a dictionary. But when I feed raw audio data into Wav2Vec2FeatureExtractor function, where the raw audio data looks like tensor([ 3.9514e-06,  1.0558e-04, -4.7315e-06,  ...,  3.9716e-04,          2.4415e-04,  9.1544e-05]) I get back a bunch of float values, which look like tensor([[-0.0020, -0.0001, -0.0021,  ...,  0.0051,  0.0024, -0.0004]]) What are these features that are being generated with Wav2Vec2FeatureExtractor. In NLP, the words are mapped to some vector representation, and that vector is the feature representation. So what are these frequencies from the raw audio being mapped to?","Apr 3, 2022 8:11 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"‚ÄúAdd_tokens‚Äù breaks words when encoding","https://discuss.huggingface.co/t/add-tokens-breaks-words-when-encoding/21154","2","1.2k","Created: Aug 2, 2022 2:42 pm Latest: Aug 22, 2023 4:05 am","sduran","I am using the add_tokens function in order to get a larger vocabulary for the distilgpt2 pre-trained tokenizer. But, when I do it, it changes the tokenizer‚Äôs behaviour when doing the encoding: it breaks words, in order to identify my new token. I don‚Äôt understand why it does it, and whether is possible to force it to work as it was before. I provide an example: from transformers import AutoTokenizer, TFAutoModelForCausalLM  tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"") model = TFAutoModelForCausalLM.from_pretrained(""distilgpt2"") If I use the word ‚Äòcrypt‚Äô, it does not break it, even if there are existing subwords in it: tokenizer.encode('crypt') ## [29609] tokenizer.encode('cry') ## [20470] tokenizer.encode('pt') ## [457] But if I add ‚Äúryp‚Äù, it breaks the words (and I don‚Äôt want them broken! I just want to add the full word ‚Äúryp‚Äù) tokenizer.add_tokens('ryp') model.resize_token_embeddings(len(tokenizer)) tokenizer.encode('crypt') # [66, 50257, 83] Would you know what this happens, and how I can force it to work as before? I read the docs about the BPE encoding, but I don‚Äôt find how to force it to use the largest found token. Thanks !","Aug 2, 2022 2:42 pm","AlexandrosChariton","from transformers import AutoTokenizer, TFAutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")  from transformers.tokenization_utils import AddedToken tokenizer.add_tokens(AddedToken('ryp', single_word = True)) AddedToken accepts single strings. Then as you can see: tokenizer.encode('crypt') #[29609] Huggingface source is here: Added Tokens","Feb 13, 2023 5:09 pm","TunyTrinh","I would like to express my gratitude to @AlexandrosChariton for providing a solution to a problem I was facing. However, I have tried implementing the suggested method, and while it does work, it doesn‚Äôt entirely address my specific case. Here is my situation: I have checked the tokens ‚ÄúHe‚Äù and ‚ÄúHello‚Äù in the vocabulary of the ‚Äòbert-base-cased‚Äô model, and they exist. The results are as follows: tokenizer.tokenize(""Hello"") # Output is ['Hello']  tokenizer.tokenize(""He"") # Output is ['He']  tokenizer.tokenize(""Helloe"") # Output is ['Hello', 'e']  tokenizer.tokenize(""Hee"") # Output is ['He', 'e'] However, when I attempt to use the same method to add a new token, ‚Äúpr,‚Äù using the following code: tokenizer.add_tokens(AddedToken('pr', single_word=True)) The method works in some cases: tokenizer.tokenize(""present"") # Output is ['present']  tokenizer.tokenize(""pr"") # Output is ['pr'] But it doesn‚Äôt work in other cases: tokenizer.tokenize(""prmention"") # Output is ['p', '##rm', '##ent', '##ion'] # My expected output is ['pr', '##ment', '##ion'], similar to the output of tokenizer.tokenize('privatemention') which is ['private', '##ment', '##ion'] Could anyone advise me on how to solve this problem? Thank you in advance for your assistance.","Aug 22, 2023 4:05 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How does `byte_fallback` work and affect vocab size in BPE?","https://discuss.huggingface.co/t/how-does-byte-fallback-work-and-affect-vocab-size-in-bpe/64634","1","1.5k","Created: Dec 5, 2023 9:39 am Latest: Mar 19, 2024 9:06 am","dinhanhx","BPE has byte fallback option to convert unk character to utf-8 bytes. That‚Äôs what I know for now. After reading wikipedia, stackoverflow, google repos, I still don‚Äôt understand how all things work together.","Dec 5, 2023 9:39 am","alvations","It‚Äôs in Rust so it might be a not so convenient to read. tokenizers/tokenizers/src/decoders/byte_fallback.rs at main ¬∑ huggingface/tokenizers ¬∑ GitHub","Mar 19, 2024 9:06 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Import distilbert-base-uncased tokenizer to an android app along with the tflite model","https://discuss.huggingface.co/t/import-distilbert-base-uncased-tokenizer-to-an-android-app-along-with-the-tflite-model/3234","3","1.9k","Created: Jan 16, 2021 6:54 pm Latest: Dec 29, 2021 11:20 pm","akgarg","I have converted the model (.h5) to tflite using: converter = tf.lite.TFLiteConverter.from_keras_model(model) converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS,                                         tf.lite.OpsSet.SELECT_TF_OPS] converter.optimizations = [tf.lite.Optimize.DEFAULT] tflite_model = converter.convert() open(""/models/tflite_models/5th_Jan/distilbert_sms_60_5_jan.tflite"", ""wb"").write(tflite_model) but I also need tokenizer to run the model locally on the android app (independent of internet availability). According to the articles on internet and question answered on stackoverflow How to tokenize input text in android studio to process in NLP model? we need json file of tokenizers to tokenize words in new inputs. When I run the following code: import json  with open( 'android/word_dict.json' , 'w' ) as file:     json.dump( tokenizer.word_index , file ) The following error comes: AttributeError: 'DistilBertTokenizer' object has no attribute 'word_index I am unable to find solution to use tokenizer of distilbert-base-uncased in android app. Any help will be appreciated. Thanks.","Jan 16, 2021 6:54 pm","toyl","excuse me did you solve it ?","Dec 28, 2021 2:45 pm","merve","word_index is an attribute of Keras‚Äô own tokenizer, not Hugging Face. See docs here.","Dec 29, 2021 5:35 am","toyl","excuse me . what is the equivalent manner in hugging face ?","Dec 29, 2021 11:20 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding New Tokens to MarianMT Model","https://discuss.huggingface.co/t/adding-new-tokens-to-marianmt-model/71547","8","684","Created: Feb 2, 2024 2:32 am Latest: Feb 3, 2024 5:01 pm","minkhantycc","Hey guys, I am trying to fine-tune a MarianMT model that translates from Japanese to English. I have some new tokens (around 50). So, I made the following steps: tokenizer.add_tokens(new_tokens) model.resize_token_embeddings(len(tokenizer)) Then I fine-tuned the model on kde4 dataset. Since MarianMT tokenizer doesn‚Äôt have train_from_iterate method, I can‚Äôt fine-tune the tokenizer. My question is ""Do I need to re-train the tokenizer? Or Is just fine-tuning the model enough? Thank you for your time.","Feb 2, 2024 2:32 am","kumarme072","Fine tuning is enough go ahead","Feb 2, 2024 5:34 pm","kumarme072","Also I would like to connect with you. I am also working in the same way on different domain","Feb 2, 2024 5:34 pm","Saugatkafley","Did you train the model? I am also working on extending the vocabulary of seq2seq and LLMs . Figuring out how to effectively train them.","Feb 3, 2024 7:25 am","minkhantycc","@kumarme072 I think so. I fine-tuned the model and it has some improvement. It resulted in 54 BLEU score. It had only 39.1 originally. Now, my problem becomes not having enough corpus that includes added tokens. Anyway thank you for your answer.","Feb 3, 2024 10:54 am","minkhantycc","Yes I fine-tuned the model with a custom dataset (a combination of 100 sentences and kde4 dataset). I think we should use a corpus that has the vocabularies we added. We should have at least more than 30 sentences for a vocabulary. In my experiments, I added 50 vocabluaries and fine-tuned it on around 100 sentences. It was just a waste of time and I got poor result. So, I combined them with kde4 dataset from huggingface. It caused the model improve from 39.1 to 54 SacreBLEU. What do you think? Do you have any other ideas?","Feb 3, 2024 11:03 am","Saugatkafley","Of course , the pretraining corpus and the tokenizer‚Äôs ability to tokenized the words(fertility) matter a lot. I also faced similar problems finding a proper model for task-specific fine-tuning. Found out that the presence of related context in the pre-training corpus is essential for better fine-tuning. The model generalized faster and improved.","Feb 3, 2024 4:57 pm","Saugatkafley","like Fine-tuning a model for a specific language task that contains plenty of data about that language (other than English) was found to be better than fine-tuning a popular model that rarely contained that language.","Feb 3, 2024 5:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Run_seq2seq_qa.py: Column 3 named labels expected length 1007 but got length 1000","https://discuss.huggingface.co/t/run-seq2seq-qa-py-column-3-named-labels-expected-length-1007-but-got-length-1000/13237","1","2.5k","Created: Dec 30, 2021 9:50 am Latest: Feb 17, 2022 2:12 am","brundle","I tried to run t5 training with example code snippet provided on: github.com transformers/examples/pytorch/question-answering at master ¬∑... master/examples/pytorch/question-answering ü§ó Transformers: State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX. - transformers/examples/pytorch/question-answering at master ¬∑ huggingface/transformers Code-Snippet: python run_seq2seq_qa.py \   --model_name_or_path t5-small \   --dataset_name squad_v2 \   --context_column context \   --question_column question \   --answer_column answer \   --do_train \   --do_eval \   --per_device_train_batch_size 12 \   --learning_rate 3e-5 \   --num_train_epochs 2 \   --max_seq_length 384 \   --doc_stride 128 \   --output_dir /tmp/debug_seq2seq_squad/ When I run the example code I got this error: ValueError: --answer_column‚Äô value ‚Äòanswer‚Äô needs to be one of: id, title, context, question, answers I changed ‚Äú‚Äìanswer_column answer‚Äù into ‚Äú‚Äìanswer_column answers‚Äù and run it again and got this error when running tokenizer on validation dataset: Running tokenizer on validation dataset:   0% 0/12 [00:03<?, ?ba/s] Traceback (most recent call last):   File ""run_seq2seq_qa.py"", line 678, in <module>     main()   File ""run_seq2seq_qa.py"", line 522, in main     desc=""Running tokenizer on validation dataset"",   File ""/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py"", line 2110, in map     desc=desc,   File ""/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py"", line 518, in wrapper     out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)   File ""/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py"", line 485, in wrapper     out: Union[""Dataset"", ""DatasetDict""] = func(self, *args, **kwargs)   File ""/usr/local/lib/python3.7/dist-packages/datasets/fingerprint.py"", line 411, in wrapper     out = func(self, *args, **kwargs)   File ""/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py"", line 2486, in _map_single     writer.write_batch(batch)   File ""/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py"", line 458, in write_batch     pa_table = pa.Table.from_pydict(typed_sequence_examples)   File ""pyarrow/table.pxi"", line 1560, in pyarrow.lib.Table.from_pydict   File ""pyarrow/table.pxi"", line 1532, in pyarrow.lib.Table.from_arrays   File ""pyarrow/table.pxi"", line 1181, in pyarrow.lib.Table.validate   File ""pyarrow/error.pxi"", line 84, in pyarrow.lib.check_status pyarrow.lib.ArrowInvalid: Column 3 named labels expected length 1007 but got length 1000 How can I fix it?","Dec 30, 2021 9:50 am","Seohyeong","Hi, I was having the exact same issue and looks like there was an issue posted on HF github few days ago. Check this out: Error with run_seq2seq_qa.py official script (pyarrow.lib.ArrowInvalid: Column 4 named labels expected length 1007 but got length 1000) ¬∑ Issue #15398 ¬∑ huggingface/transformers ¬∑ GitHub","Feb 17, 2022 2:12 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SentencePieceUnigramTokenizer","https://discuss.huggingface.co/t/sentencepieceunigramtokenizer/23509","0","625","Created: Sep 22, 2022 8:55 pm","antoine2323231","Hello! I am trying to use the SentencePieceTokenizer. I tried to install it but it seems to have been replaced? Do you know what is the equivalence now? (from t5_tokenizer_model import SentencePieceUnigramTokenizer) Many thanks","Sep 22, 2022 8:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"No labels column for tokenized data","https://discuss.huggingface.co/t/no-labels-column-for-tokenized-data/19540","2","2.0k","Created: Jun 23, 2022 4:55 pm Latest: Jun 27, 2022 4:33 pm","ablam","I‚Äôm tokenizing to fine-tune a custom dataset with the goal of code generation. My tokenized dataset has the following columns: ['text', 'input_ids', 'attention_mask', ""token_type_ids""], however, post-processing to fine-tune my model implies I have a ['label'] or target column. Since that is not evident here, my backward() call in training keeps failing. Can someone help me clarify if these features (label, target‚Ä¶) are task-dependent? And if so, how one would go about this in tokenization?","Jun 23, 2022 4:55 pm","courtneysprouse131","This depends on whether you want a supervised or unsupervised model. Most models assume supervised e.g. for a given sample of input data you have the correct answer (label column). It sounds like maybe you have an unsupervised dataset. So for your training to work you either need an unsupervised model or you need to supply the labels on your dataset (in your case what the code generated should look like given some set of inputs).","Jun 27, 2022 3:08 pm","ablam","Oh, I see. Yes, I duplicated my input_ids column to create the labels column but I‚Äôm not sure that would create what the generated model needs as target unless I assume the model masks certain tokens from the input and then generate examples to match the labels column I duplicated in its training. I‚Äôll check the kind of model it is and go forth with what I find. Thank you.","Jun 27, 2022 4:33 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Token alignment for word-level tasks","https://discuss.huggingface.co/t/token-alignment-for-word-level-tasks/577","1","2.4k","Created: Aug 3, 2020 4:46 pm Latest: Aug 5, 2020 10:07 am","vblagoje","Hey guys, I want to make a POS tagger on top of BERT. The dataset is in conll-u format [1], input sentences are already tokenized; input word tokens are mapped to labels (POS etc ). Therefore, I have to take special care of input/output alignment as BERT will add additional tokens during tokenization, similar to what is described in the original BERT repo [2]. So I took the alignment algorithm outlined in [2]; I tokenized each input word and map/expanded labels for each BERT tokenized word. I then needed to add padding to each sentence, attach attention_mask, convert to tensors etc. Looking at Tokenizer API I didn‚Äôt see how to do this easily if I already have sentences and labels converted to token ids in the first step. This whole exercise left me wondering if there is a simpler and less verbose approach to these word-level tasks using Tokenizer API? Cheers, Vladimir [1] https://universaldependencies.org/format#conll-u-format [2] https://github.com/google-research/bert#tokenization","Aug 3, 2020 4:46 pm","vblagoje","I found an answer in utils_ner.py See function convert_examples_to_features; it does what I was doing except in a more general model-agnostic approach. Cheers, Vladimir","Aug 5, 2020 10:07 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Save tokenizer with argument","https://discuss.huggingface.co/t/save-tokenizer-with-argument/17389","2","1.9k","Created: May 1, 2022 1:55 pm Latest: Oct 26, 2022 9:43 am","petarulev","I am training my huggingface tokenizer on my own corpora, and I want to save it with a preprocessing step. That is, if I pass some text to it, I want it to apply the preprocessing and then tokenize the text, instead of explicitly preprocessing it before that. A good example is BERTweet: GitHub - VinAIResearch/BERTweet: BERTweet: A pre-trained language model for English Tweets (EMNLP-2020) and their tokenizer = AutoTokenizer.from_pretrained(""vinai/bertweet-base"", normalization=True) (here normalization=True indicates that the input will be preprocessed according to some function). I want the same to apply when I train a tokenizer with a custom preprocessing function. My code is: from pathlib import Path from tokenizers import ByteLevelBPETokenizer  def preprocess(text):     return text  paths = [str(x) for x in Path('data').glob('*.txt')]  tokenizer = ByteLevelBPETokenizer()  tokenizer.train(files=paths, vocab_size=50_000, min_frequency=2,                 special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])  tokenizer.save_model('CustomBertTokenizer') Now, when I load the tokenizer: from transformers import RobertaTokenizerFast sentence = 'Hey' tokenizer = RobertaTokenizerFast.from_pretrained('CustomBertTokenizer') tokenizer(sentence) I want sentence to be preprocessed with the preprocess function, and then tokenized. So I want to pass like an argument : preprocessing=True, or something like that. How can I do it?","May 1, 2022 1:55 pm","petarulev","Any ideas guys?","May 8, 2022 2:35 pm","lianghsun","Hi @petarulev , whatever you want to do is: Preprocess ‚Üí Tokenization ‚Üí Model Then, I‚Äôll suggest you using HF Datasets at first, so the code should be: # Suppose I have a dataset in .txt format, and the context is: #  # C1CCCC1\n # C1NNCC1\n # ...and so on  # Step1: load datasets with HF Datasets from datasets import load_dataset ds = load_dataset(     ""text"", data_files={'train': ['path/to/data_1', ['path/to/data_2']} )  # Step2: use .map() method def preprocess(example):     # You can find out that each line in dataset file contain     # unwanted breaking line symbol '\n', so i decide to remove it     return example['text'] = example['text'].rstrip()  processed_ds = ds.map(preprocess, num_proc=4) # set `num_proc` to speed up!  # Step3: tokenization tokenizer = ... And this should be the way you can try.","Oct 26, 2022 9:43 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issues with Data Collator and Tokenizing with NER Datasets","https://discuss.huggingface.co/t/issues-with-data-collator-and-tokenizing-with-ner-datasets/17489","1","2.3k","Created: May 3, 2022 7:59 pm Latest: May 9, 2022 4:27 pm","courtneysprouse131","I‚Äôm having some strange issues with the Data Collator and Dataloader. I get the following value error ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length.. What is strange is as you can see from the code below, I‚Äôm both truncating and padding. I can see that when the dataset is returned from the tokenizer the input ids are all the same length. However, when I check the input ids length after they are loaded into the dataloader the lengths are variable. If I remove the collator and batch size arguments everything works fine with the same code. I assume I‚Äôm doing something stupid with the data collator? But I‚Äôve tried a couple collators, datasets, models, and tokenizers and I have the same issue. Any thoughts? from transformers import (     DataCollatorWithPadding,     DataCollatorForTokenClassification,     AutoTokenizer,     AutoModelForTokenClassification, )  from datasets import load_dataset  from torch.utils.data import DataLoader  #Load model and tokenizer tokenizer = AutoTokenizer.from_pretrained(""dslim/bert-base-NER"") model = AutoModelForTokenClassification.from_pretrained(""dslim/bert-base-NER"")  # tokenizing function def tokenize(data):     tokenized_samples = orig_tokenizer(         data[""tokens""],         is_split_into_words=True,         truncation=True,         padding=True,     )  #load wikiann dataset dataset = load_dataset(""wikiann"", ""bn"")[""train""]  #tokenize dataset with padding and truncation dataset_tokenized = dataset.map(tokenize, batched=True)  #remove extra columns dataset_tokenized = dataset_tokenized.remove_columns([""langs"", ""spans"", ""tokens""])  #change tag columns to labels dataset_tokenized = dataset_tokenized.rename_column(""ner_tags"", ""labels"")  #instantiate collator - note also tried this with DataCollatorWithPadding collator = DataCollatorForTokenClassification(tokenizer)  #Instantiate Pytorch DataLoader dl = DataLoader(dataset_tokenized, shuffle=True, collate_fn=collator, batch_size=2)","May 3, 2022 7:59 pm","courtneysprouse131","I did figure out the issue. It turns out when you set padding=True that is equivalent to setting padding='longest_sequence'. If you use longest sequence to pad, it actually pads to the longest sequence in the batch, which is not consistent across the dataset, hence the error in the trainer. I‚Äôm not sure if I‚Äôm missing something, but it didnt seem like I could control batch size or use longest sequence across the whole dataset? So instead I used padding='max_length' which sets the padding to a consistent value across the dataset.","May 9, 2022 4:27 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Documentation of SentencePieceBPETokenizer?","https://discuss.huggingface.co/t/documentation-of-sentencepiecebpetokenizer/84777","0","575","Created: May 2, 2024 6:48 am","raptorkwok","I cannot find SentencePieceBPETokenizer in the official documentation, therefore I ask this question. I refer to the source code of SentencePieceBPETokenizer to write my codes to train a custom Tokenizer based on SentencePieceBPETokenizer. Question 1: do the following codes make sense? from tokenizers import SentencePieceBPETokenizer  special_tokens = [""<unk>"", ""<pad>"", ""<cls>"", ""<sep>"", ""<mask>""] tokenizer = SentencePieceBPETokenizer() tokenizer.train_from_iterator(     get_training_corpus(),     vocab_size = 400000,     min_frequency = 10,      show_progress = True,     special_tokens = special_tokens ) where get_training_corpus() is a function to get sentences in batch from a list of strings. I notice that there are Trainer classes, but there is no SentencePiece. Also, from the Tokenizer documentation, it specifies the attributes of train_from_iterator() are iterator and trainer object, but my attributes follow the Trainer class instead. Question 2: No error occurred, can I assume the attributes are correct? Question 3: What is the use of initial_alphabet and limit_alphabet? The documentation does not tell clearly. Any example? p.s. I know show_progress = True does not work in Jupyter notebook. p.s. using tokenizers 0.13.3","May 2, 2024 6:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Programmatic way to Tokenization on Custom Text Columns","https://discuss.huggingface.co/t/programmatic-way-to-tokenization-on-custom-text-columns/19679","0","563","Created: Jun 27, 2022 3:13 pm","courtneysprouse131","I‚Äôm having issues with tokenizing in a more programmatic way. I can‚Äôt seem to figure out a way to pass multiple arguments to the map function in tokenizers. I also can‚Äôt run the map function on a particular column in the dataset. And I can‚Äôt use the generic python map function because I can‚Äôt figure out how to make it return a transformers dataset object (although that could be my own ignorance). Any thoughts?","Jun 27, 2022 3:13 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to configure TokenizerFast for AutoTokenizer","https://discuss.huggingface.co/t/how-to-configure-tokenizerfast-for-autotokenizer/11353","2","1.8k","Created: Nov 4, 2021 12:08 pm Latest: Nov 11, 2021 11:15 am","vblagoje","Hi there, I made a custom model and tokenizer for Retribert architecture. For some reason, when using AutoTokenizer.from_pretrained method, the tokenizer does not initialize model_max_len tokenizer attribute to 512 but to a default of a very large integer. If I invoke AutoTokenizer.from_pretrained with an additional max_len=512 kwarg then the model_max_len gets set to 512 as expected. However, as you might expect I don‚Äôt want users to pass this additional kwarg but would prefer to somehow set this value by default. I figured out that TokenizerFast gets initialized from tokenizer.json and I attempted to add model_max_len attribute to tokenizer.json. However, as soon as I do that AutoTokenizer complains that it can not load the JSON file any longer. Perhaps this property can‚Äôt be set via tokenizer.json or perhaps I am not adding it at the right JSON node. Any ideas on how to set model_max_len tokenizer property so that AutoTokenizer picks it up without additional kwargs? Best, Vladimir","Nov 4, 2021 12:08 pm","vblagoje","Hi again, The model_max_len default seems to be initialized in the modelling class itself. See for example Retribert and how it gets set in the tokenizer for the official HF models, known models and their registered paths. Because the path of my model is not registered with the HF codebase this value does not get set. Which leaves me confused about how to set model_max_len value even more? Vladimir","Nov 4, 2021 12:27 pm","vblagoje","Hi, I figured this one out; leaving a small note if you stumble on this issue yourself. All you need to do is add tokenizer_config.json file with additional configs for the tokenizer. I added a simple tokenizer_config.json with the following contents: {‚Äúmodel_max_length‚Äù: 512} That‚Äôs all. Cheers, Vladimir","Nov 11, 2021 7:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using a fixed vocab.txt with AutoTokenizer?","https://discuss.huggingface.co/t/using-a-fixed-vocab-txt-with-autotokenizer/9919","1","2.2k","Created: Sep 12, 2021 9:56 pm Latest: Sep 13, 2021 2:50 am","jbmaxwell","Hello, I have a special case where I want to use a hand-written vocab with a notebook that‚Äôs using AutoTokenizer but I can‚Äôt find a way to do this (it‚Äôs for a non-language sequence problem, where I‚Äôm pretraining very small models with a vocab designed to optimize sequence length, vocab size, and legibility). If it‚Äôs not possible, what‚Äôs the best way to use my fixed vocab? In the past I used BertWordPieceTokenizer, loaded directly with the vocab.txt path, but I don‚Äôt know how to use this approach with newer Trainer-based approach in the notebook. UPDATE: More specifically, if I try my old method of using BertWordPieceTokenizer(vocab='vocab.txt') it fails later with: TypeError                                 Traceback (most recent call last) /tmp/ipykernel_3379/2783002494.py in <module>       3 # Setup train dataset if `do_train` is set.       4 print('Creating train dataset...') ----> 5 train_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=False) if training_args.do_train else None       6        7 # Setup evaluation dataset if `do_eval` is set.  /tmp/ipykernel_3379/2486475202.py in get_dataset(args, tokenizer, evaluate)      32   if args.line_by_line:      33     # Each example in data file is on each line. ---> 34     return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path,       35                                  block_size=args.block_size)      36   ~/anaconda3/envs/torch_17/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py in __init__(self, tokenizer, file_path, block_size)     133             lines = [line for line in f.read().splitlines() if (len(line) > 0 and not line.isspace())]     134  --> 135         batch_encoding = tokenizer(lines, add_special_tokens=True, truncation=True, max_length=block_size)     136         self.examples = batch_encoding[""input_ids""]     137         self.examples = [{""input_ids"": torch.tensor(e, dtype=torch.long)} for e in self.examples]  TypeError: 'BertWordPieceTokenizer' object is not callable The notebook I‚Äôm trying to use is from: github.com/gmihaila/ml_things.git","Sep 12, 2021 9:56 pm","jbmaxwell","Okay, so obviously I‚Äôm not a Python guy‚Ä¶ I see there‚Äôs some insanity in the language that allows class instances to be callable‚Ä¶ (why, Python‚Ä¶ WHY???) ‚Ä¶so I‚Äôm a bit stumped, but presumably it has to do with the fact that BertWordPieceTokenizer is not a subclass of PreTrainedTokenizer (which has the crazy attribute of being callable). I‚Äôm really stuck. I‚Äôd just like to plug in my custom tokenizer, but it seems that when I hit ‚ÄúLineByLineTextDataset‚Äù, I‚Äôm going to hit the same callable error. I tried running with the default tokenization and although my vocab went down from 1073 to 399 tokens, my sequence length went from 128 to 833 tokens. Hence the desire to load my tokenizer from the hand-written vocab. Aack! UPDATE: Okay, I hadn‚Äôt realized I could do it with BertTokenizerFast. I haven‚Äôt totally verified that this is working, but so far it looks correct.","Sep 13, 2021 12:12 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using HuggingFace Tokenizers Without Special Characters","https://discuss.huggingface.co/t/using-huggingface-tokenizers-without-special-characters/21962","2","1.7k","Created: Aug 23, 2022 7:20 am Latest: Nov 2, 2022 6:50 am","dotan1111","Hey, I would like to use HuggingFace Tokenizers for a unique dataset which doesn‚Äôt require any special characters. Thus, the results vocabulary should consist only characters from the input file / files. For example, if my file contains the sentence: ‚ÄúAAABBBCCC‚Äù The vocabulary should consist words with the letters: ‚ÄúA‚Äù, ‚ÄúB‚Äù and ‚ÄúC‚Äù only. My intentions are to run the following tokenizers: BPE, sentence piece and word piece. I looked at the base code but couldn‚Äôt find the parameter to do so.","Aug 23, 2022 7:20 am","lianghsun","Hey @dotan1111, have you solved this question yet? HF Tokenizers provide several models and trainers to learn how to tokenize the sentence to smaller piece. In modern way, it often split the sentence to subwords, and let tokenizer use certain model to learn how merge them to specific token. So, the question you ask is how to tokenize the sentence ‚ÄúAAABBBCCC‚Äù to the letters: ‚ÄúA‚Äù, ‚ÄúB‚Äù and ‚ÄúC‚Äù only, without any merges. However, the models in HF Tokenizers often learn to build a vocabulary with letters and merged pattern, and I believe you already know this. Here, I‚Äôll provide several ways for you to build up a ‚Äúchar-level‚Äù tokenizer using BPE. BPE with defined minimum merged frequency from tokenizers import Tokenizer from tokenizers.models import BPE from tokenizers.trainers import BpeTrainer  # Data file = ['/path/to/AAABBBCCC.txt'] # AAABBBCCC only  # Use BPE model tokenizer = Tokenizer(BPE(unk_token=""[UNK]""))  # Setup BPE trainer trainer = BpeTrainer(     min_frequency=1_000_000_000 # We set min_freq to large threshold here! ) tokenizer.train(file, trainer)  # Show vocab list tokernizer.get_vocab() # > {'A': 0, 'B': 1, 'C': 2} Pythonic way from tokenizers import Tokenizer from tokenizers.models import BPE from tokenizers.trainers import BpeTrainer  # Context from Char.txt # AAAABBB # CCCCDDDD # EEEFF # GGHHHHH  # Get unique letters letters = set() with open('/path/to/Char.txt', 'r') as f:     for line in f.readlines():         for char in [*line.rstrip()]:             letters.add(char) print(letters)  # Build your own vocab.json import json vocab = dict() with open('/path/to/char_vocab.json', 'w') as f:     for idx, item in enumerate(letters):         vocab[item] = idx     json.dump(vocab, f, indent = 6)  # Build your own mergs.txt: please create an empty file and named it `char_merges.txt`  # Init a new tokenizer new_tokenizer = Tokenizer(BPE.from_file('/path/to/char_vocab.json', '/path/to/char_merges.txt')) new_tokenizer.get_vocab() # > {'C': 7, 'F': 4, 'G': 3, 'E': 2, 'A': 0, 'B': 6, 'H': 5, 'D': 1} I wouldn‚Äôt say which method is the best way to implement char-level tokenizer Hope this solve your question.","Oct 27, 2022 3:35 am","dotan1111","Thanks! I have used a similar approach","Nov 2, 2022 6:50 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Average time to train a SentencePieceBPETokenizer","https://discuss.huggingface.co/t/average-time-to-train-a-sentencepiecebpetokenizer/23081","0","537","Created: Sep 13, 2022 10:13 am","jessicalopez","Hello, I am training my own SentencePieceBPETokenizer tokenizer on java CodeSearchNet dataset. I know sp_tokenizer.train_from_iterator does not show the bar progress in Colab. However, I would like to know the progress my tokenizer is doing in the training. So, I have two questions: Do you a work around to print the progress bar in Colab? How long time a tokenizer finish to train aprox.? Thank you","Sep 13, 2022 10:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Find which tokens are unknown in new data","https://discuss.huggingface.co/t/find-which-tokens-are-unknown-in-new-data/22466","0","531","Created: Sep 2, 2022 7:02 pm","noncomp","I am fine-tuning on new data and using the GPT subword tokenizer. Certain substrings of my input get converted to the UNK token during tokenization. How can I find these substrings so I can add them to my tokenizer‚Äôs vocabulary?","Sep 2, 2022 7:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to ensure that tokenizers never truncate partial words?","https://discuss.huggingface.co/t/how-to-ensure-that-tokenizers-never-truncate-partial-words/14024","2","1.7k","Created: Jan 23, 2022 5:55 pm Latest: Jan 24, 2022 3:37 am","nasheed","Is there a way to ensure tokenizers never partially truncate a word, as illustrated below: tokenizer= SomeTokenizer.from_pretrained('some/path') tokenizer.decode(tokenizer('I am Nasheed and I like xylophones.', truncation=True, max_length=12)['input_ids']) The output is the above sentence truncated like: ‚Äú‚Äô[CLS] I am Nasheed and I like xylo [CLS]‚Äô‚Äù I want it to be truncated as: ‚Äú‚Äô[CLS] I am Nasheed and I like [CLS]‚Äô‚Äù Is there a way to enforce this?","Jan 23, 2022 5:55 pm","marshmellow77","Hi Nasheed, I‚Äôm quite curious about your use case and why you‚Äôre interested in never partially truncating, if you don‚Äôt mind sharing! In any case, here is how I would do it: Increase max_length by 1. Tokenize the text. Decode the tokenized text. Check if the second to last token (the one before the final [CLS] token) starts with ## (the prefix that signifies that a longer token was split). If yes, remove both tokens, the one that starts with ## and the one before that. If not, just remove the one before the [CLS] token. In your example it would be [CLS] I am Nasheed and I like xylo ##phones [CLS] Because the second to last token starts with ## you would remove that token and the token before it. Hope that helps. Cheers Heiko","Jan 23, 2022 11:08 pm","nasheed","Thanks for the quick response @marshmellow77 I am working on a paper that aims to extend all Transformer models and architectures beyond the 512 token limit. A principal part of how I do this is through splitting up (with overlap) my original document/text. For last words that are longer than 3 tokens, I should recursively remove tokens from the end so long as they have the ## prefix and post that remove 1 more token which is the start of the word. I am curious whether the approach you have described would also work with sentencepiece tokenizers? I will update it here when post experimentation. Also, I have a follow-up question about controlling the stride overlap behavior of tokenizers along the lines of the original post. I will post a link to that discussion here as well. Edit 1: This is the follow-up question.","Jan 24, 2022 3:03 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How would you train a sentencepiece BPE tokenizer on this language with 400 ‚Äúcharacters‚Äù?","https://discuss.huggingface.co/t/how-would-you-train-a-sentencepiece-bpe-tokenizer-on-this-language-with-400-characters/14669","0","2.9k","Created: Feb 13, 2022 6:17 am Latest: Feb 13, 2022 6:21 am","mmalandro","I am dealing with a language where each sentence is a sequence of instructions, and each instruction has a character component and a numerical component. The number of possible instructions is known and is finite. There are a few hundred of them. Without getting into the idiosyncrasies of the language I‚Äôm actually dealing with, consider the following language: The instructions are of the form x:y, where x is in ‚ÄòABCD‚Äô and y is in range(100). Here is a sentence consisting of five instructions: A:50 B:2 C:12 A:19 D:12. I think of the instructions as ‚Äúcharacters‚Äù. This language does not have a natural notion of ‚Äúwords‚Äù. I would like to train a BPE tokenizer on a large dataset of instructions. I would think that I would not want the tokenizer to split my ‚Äúcharacters,‚Äù but I cannot get any of the trainers to do that. For instance: import random import tokenizers  alphabet = [] second = ':' for first in 'ABCD':     for third in range(0, 100):         alphabet.append(first + second + str(third)) print('alphabet =', alphabet)  text = '' for i in range(1000):     first = 'ABCD'[random.randint(0, 3)]     second = ':'     third = str(random.randint(0, 99))     text += first + second + third + ' ' text = text.rstrip(' ')  print('text =', text)  tokenizer = tokenizers.SentencePieceBPETokenizer() tokenizer.train_from_iterator([text],                               vocab_size=1000,                               min_frequency=1,                               limit_alphabet=500,                               )  for k in tokenizer.get_vocab():     print(k)  print('vocab size =', len(tokenizer.get_vocab())) Output: alphabet = ['A:0', 'A:1', 'A:2', 'A:3', 'A:4', 'A:5', 'A:6', 'A:7', 'A:8', 'A:9', 'A:10', 'A:11', 'A:12', 'A:13', 'A:14', 'A:15', 'A:16',... text = B:70 D:76 B:82 C:61 A:6 B:73 B:58 C:2 D:60 A:28 C:35 C:85 A:90 A:61 B:84 C:10 C:28 A:36 A:12 A:9 B:48 A:56 B:89 B:44...  ‚ñÅD:50 ‚ñÅB:11 ‚ñÅB:74 ‚ñÅB:44 ‚ñÅA:52 ‚ñÅB:56 ‚ñÅA:44 ‚ñÅC:59 ‚ñÅA:92 ‚ñÅC:68 ‚ñÅD:64 ‚ñÅB:77 ‚ñÅB:85 ‚ñÅD:82 7 ‚ñÅA:51 ‚ñÅA:8 ... ‚ñÅA:45 : vocab size = 400 That won‚Äôt work because it‚Äôs splitting on whitespace before training, so it will never encode more than one instruction per vocabulary token. Let‚Äôs try replacing the whitespaces with semicolons instead. tokenizer = tokenizers.SentencePieceBPETokenizer()  tokenizer.train_from_iterator([text.replace(' ', ';')],                               vocab_size=1000,                               min_frequency=1,                               limit_alphabet=500,                               )  for k in tokenizer.get_vocab():     print(k)  print('vocab size =', len(tokenizer.get_vocab())) Output: ;A:69;B:5 6;C:8 0;B:4 3;B:4 6;D:8 9;A:5 ;D:81;C:64;B:7 6;B:30;C:92;A:95;C:67 ... vocab size = 1000 That‚Äôs better because it‚Äôs generally encoding more than one instruction per vocabulary token now, but see how the vocabulary has tokens in it that encode only parts of an instruction (even if other instructions are included fully), like the second one? I‚Äôm not sure that‚Äôs ideal, so is it possible to tell a tokenizer to never split certain strings while training? I have tried using the ‚Äúinitial_alphabet‚Äù and ‚Äúspecial_tokens‚Äù options in train_from_iterator to accomplish this, with no success. To be clear, I would hope to end up with a vocabulary like: ;D:96;A:3 ;B:2 ;D:94;A:5;B:17 ...","Feb 13, 2022 6:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How much memory is needed for training ByteLevelBPETokenizer?","https://discuss.huggingface.co/t/how-much-memory-is-needed-for-training-bytelevelbpetokenizer/1165","3","1.4k","Created: Sep 16, 2020 1:10 pm Latest: Sep 18, 2020 5:49 am","kouohhashi","Hi, I‚Äôm trying to train LM for Japanese from scratch. To be honest, I copied almost everything from https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb. I just changed datasets from Esperanto datasets to Japanese wiki datasets. But when I tried to train tokenizer, my notebook crashed and restarted probably because of out of memory. My datasets is an entire wikipedia text which is 5.1 G. But my server had 64G memory. How much memory do I need to train tokenizer from scratch? or can I prevent out of memory error with some options? Thanks in advance.","Sep 16, 2020 1:10 pm","valhalla","Pinging @anthony","Sep 16, 2020 1:26 pm","kouohhashi","[UPDATE] I tried subset of entire dataset. 66,101,891 lines, 5.1 G ( entire datasets ) : out of memory 15,000,000 lines: out of memory 3,000,000 lines, 231M: success Can I use subset to train tokenizer and use it to train LM with entire datasets?","Sep 17, 2020 12:15 am","kouohhashi","I could train a tokenizer with train_bert_wordpiece.py without errors. So it could be jupyter notebooks or something else which caused out of memory errors.","Sep 18, 2020 5:49 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Training unigram on long sequences","https://discuss.huggingface.co/t/training-unigram-on-long-sequences/19188","4","1.3k","Created: Jun 15, 2022 11:52 am Latest: Jun 23, 2022 4:12 am","carted-ml","This is a follow-up thread of pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long. Background We‚Äôre training a unigram tokenizer from scratch. This is how it‚Äôs instantiated: tokenizer = Tokenizer(Unigram()) tokenizer.normalizer = normalizers.Sequence(     [normalizers.Replace(""``"", '""'), normalizers.Replace(""''"", '""')] ) tokenizer.pre_tokenizer = pre_tokenizers.Metaspace() This is how the trainer is initialized: trainer = UnigramTrainer(         unk_token=""<unk>"",         special_tokens=[""[CLS]"", ""[SEP]"", ""<unk>"", ""<pad>"", ""[MASK]""],         vocab_size=10000, ) We‚Äôre training on a batch-by-batch basis: tokenizer.train_from_iterator(dataloader, trainer=trainer) We‚Äôre running into the following toward the end: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:52:53] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 50158000 /        0 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:52:57] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 50164000 /        0 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:53:25] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0 [00:01:37] Suffix array seeds                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18201510 / 18201510 [00:00:00] EM training                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18201510 /       32 thread '<unnamed>' panicked at 'likelihood is NAN. Input sentence may be too long.', /__w/tokenizers/tokenizers/tokenizers/src/models/unigram/trainer.rs:413:17 Traceback (most recent call last):   File ""train_unigram_tokenizer.py"", line 106, in <module>     main(**args)   File ""train_unigram_tokenizer.py"", line 92, in main     tokenizer.train_from_iterator(datagen, trainer=trainer) pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long. Here is the version info: transformers: 4.17.0 tokenizers: 0.11.6 Here‚Äôs what tried in the meanwhile: Used the enable_truncation() method that comes with a tokenizer. We set the length to a number for which we‚Äôre able to successfully train the tokenizer of another version of the dataset. But to our wonder, with the same sequence length and updated dataset version, the tokenizer training fails. To better understand the issue, we first sampled all the very long sequences (character sequence lengths having > 300000). Note that we used our updated dataset (which is causing the issue) to sample the sequences. We can successfully train the tokenizer on those sampled entries. We managed to train a BPE tokenizer on the same data on which the Unigram training is failing. Maybe there are some unhandled edge cases in the viterbi algorithm implementation? I‚Äôm also re-iterating the fact that we did manage to train the Unigram tokenizer with the top 20% longest sequences from the data we have. This means that individual sequence lengths do not lead to a training failure. It fails only when we provide the full data. We have also ensured that the data doesn‚Äôt have any non ASCII characters.","Jun 15, 2022 11:52 am","ddeerreekk","Hi, I am facing the same error here and I wonder if you found a way to get around with it?","Jun 22, 2022 7:40 am","carted-ml","Not yet","Jun 22, 2022 1:22 pm","ddeerreekk","So I changed the normalizer and the pretokenizer in their SentencePieceUnigramTokenizer implementation and it worked for me now. I didn‚Äôt try them separately but my guess is that the Metaspace pretokenizer caused the length error. tokenizer.normalizer = normalizers.NFKC() tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()","Jun 22, 2022 2:11 pm","carted-ml","Thanks for sharing. We can give it a try.","Jun 23, 2022 4:12 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NER Label tokenization with overflowing tokens","https://discuss.huggingface.co/t/ner-label-tokenization-with-overflowing-tokens/21561","4","1.2k","Created: Aug 12, 2022 2:16 pm Latest: Nov 9, 2023 5:14 am","Sajan","I am trying to train bert for token classification and I want to use full text and if necessary, split in two samples. For that I am using return_overflowing_tokens with a specific stride. I want to tokenize the labels as well for this. I have seen tokenize_and_align_labels function in the tutorial (Token classification) but this doesn‚Äôt take care of such overflow. Is there something already available for this? Or should I generate the sample without truncation and then split to a specific length later on?","Aug 12, 2022 2:16 pm","Raisa06","@Sajan Hey I‚Äôm also working on the same issue. I tried the same approach and getting the same error where input_ids has different length due to sliding window chunks but the labels still remain the same length as that of dataset. Please let me know if the issue has been resolved and how did you tackled it? ArrowInvalid: Column 2 named input_ids expected length 2 but got length 4","Nov 5, 2023 5:21 pm","Sajan","Hi @Raisa06 The error (ArrowInvalid: Column 2 named input_ids expected length 2 but got length 4) you are getting seems different. For this issue, the bert tokenizer returns word_ids function which gives the original index of pretokenized words and works with return_overflowing_tokens. (eg: tokenized_encoding.word_ids(2) gives original index of input which can be mapped to labels. I then loop over the tokenized word ids and add corresponding label to the final labels list","Nov 6, 2023 1:42 pm","Raisa06","Hi @Sajan Thanks for your reply. I get you. Now I‚Äôve implemented the sliding windows and mapped labels to each window. For example, len(example[‚Äúwords‚Äù]) is 2 - i.e, two list with tokens and size more than maximum sequence length. Introduced sliding windows during tokenization, tokenized_inputs = tokenizer(examples[‚Äúwords‚Äù], is_split_into_words=True, truncation=True, padding=‚Äúmax_length‚Äù, max_length=500, stride=200, return_overflowing_tokens=True, return_offsets_mapping=True) so tokenized_input[‚Äúinput_ids‚Äù] - will have lets say 4 list with respect to sliding window configuration that I‚Äôve set. and I‚Äôve mapped input_ids to word_ids and then to label manually (hugging face doesn‚Äôt have any functions to align labels for sliding windows it is generating) will result in tokenized_input[‚Äúlabels‚Äù] with 4 list tokenized_dataset = datasets.map(tokenize_and_align_labels, batched=True) More info on datasets, DatasetDict({ train: Dataset({ features: [‚Äòwords‚Äô, ‚Äòlabels‚Äô], num_rows: 2 }) }) On calling the function again I‚Äôm getting the below error, ArrowInvalid: Column 1 named labels expected length 2 but got length 4 My doubt here is, Is it an internal function level issue?","Nov 6, 2023 3:15 pm","Sajan","How do you map the input ids to word ids? Can you please share that code? Or better yet, the code snippet that does this processing","Nov 6, 2023 4:36 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to create a Huggingface tokenizer from a non-Huggingface tokenizer?","https://discuss.huggingface.co/t/how-to-create-a-huggingface-tokenizer-from-a-non-huggingface-tokenizer/5983","0","481","Created: May 4, 2021 6:01 pm Latest: May 4, 2021 6:10 pm","ChaosNLP","Hello, The title is self-explanatory. I have access to a published Bert model with its custom tokenizer. I have the vocab file, and python functions that receive a text, tokenize it according to the vocab, and do some post-processing and convert them to IDs acceptable by Bert model. I would like to transform the custom tokenizer, which is a hassle to work with, to a tokenizer so I can use all of amazing functionalities that and other -based libraries provide. I already managed to transform the Bert model to a BertModel, but tokenzier seems to be trickier. Is there a way I can somehow transform this non- tokenizer to a tokenizer? Is PreTrainedTokenizer what I need to use?","May 4, 2021 6:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Use Unicode blocks in regex (in Replace normalizer)","https://discuss.huggingface.co/t/use-unicode-blocks-in-regex-in-replace-normalizer/26904","1","1.1k","Created: Nov 29, 2022 3:28 am Latest: Nov 9, 2023 10:06 pm","astariul","In python, I can use Unicode blocks in regex to filter out characters that I don‚Äôt want. For example : import regex as re  sentence = ""This is √®nglish „Öï„Öõ„Öá„Ñ¥„ÖÅ„Öá „Öá!"" print(re.sub(""[^\p{InBasicLatin}\p{InLatin-1Supplement}]"", """", sentence) # -> 'This is √®nglish !' (See List of Unicode Groups and block ranges) I‚Äôm trying to do the same for my tokenizer. I try to declare a Replace normalizer, but it doesn‚Äôt seem to work : from tokenizers.normalizers import Replace  sentence = ""This is √®nglish „Öï„Öõ„Öá„Ñ¥„ÖÅ„Öá „Öá!"" r = Replace(r""[^\p{InBasicLatin}\p{InLatin-1Supplement}]"", """")  print(r.normalize_str(sentence)) # -> 'This is √®nglish „Öï„Öõ„Öá„Ñ¥„ÖÅ„Öá„Öá!' Therefore my question is : how can I filter out characters that are not in the Unicode blocks, like I did in pure-python ?","Nov 29, 2022 3:28 am","braunagn","From what I can see on the github repo, the Replace normalizer doesn‚Äôt do anything.","Nov 9, 2023 10:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Speed up tokenizer training","https://discuss.huggingface.co/t/speed-up-tokenizer-training/76417","5","604","Created: Mar 7, 2024 4:28 pm Latest: Sep 17, 2024 1:28 pm","astein0","I am training a tokenizer from scratch on a large dataset. Is there a way to take advantage of parallelism in the tokenizer trainer?","Mar 7, 2024 4:28 pm","alisawuffles","I also have this question! I am training several tokenizers from scratch for a research project, and it is quite time-consuming ‚Äî ~15 hours to count the pairs for 4.5G of text. What resources would be most helpful to request for a training job? Is there a way to take advantage of parallelism, as @astein0 asks?","Apr 22, 2024 8:07 pm","maveriq","same question. I have come up with a pyspark implementation of minbpe which will give you merges and vocabulary for a BPE tokenizer. It still needs some more work to optimize the speed but it works decent. Ping me if anyone wants to collaborate on that. Meanwhile it would be nice to have an official implementation, either using tokenizer or the datatrove library. @lhoestq","May 17, 2024 11:47 pm","alisawuffles","It turns out that setting TOKENIZERS_PARALLELISM=true solved my problem.","May 28, 2024 8:01 pm","subatomicseer","If you are using any form of multiprocessing after importing the tokenisers packages, then it will set the parallelism off. So doing something like this will ensure that the tokenizers will use parallelism: dataset = dataset.map(map_fn, batched=True, batch_size=64, num_proc=128)  # set the following environment variable to true and then import tokenizer modules os.environ[""TOKENIZERS_PARALLELISM""] = ""true"" from tokenizers import (   decoders,   models,   pre_tokenizers,   normalizers,   trainers,   Tokenizer,   Regex ) tokenizer = Tokenizer(models.Unigram()) It worked for me.","Sep 17, 2024 1:28 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to skip tokens from translation?","https://discuss.huggingface.co/t/how-to-skip-tokens-from-translation/28171","2","849","Created: Dec 20, 2022 12:30 pm Latest: Oct 15, 2024 12:55 pm","atis06","I am using a MarianMTModel and the corresponding MarianTokenizer for translation. I have specific words in the texts what I don‚Äôt want to translate, but preserve in the original format ([XY]). I have already tried to deactivate attention for those sub-words and add the tokens to additional_special_tokens. How can I skip these tokens during translation? Berlin is a nice city. -> [CITY] is a nice city. -> [CITY] ist eine sch√∂ne Stadt.","Dec 20, 2022 12:30 pm","deepak-llm-art","Did you find any solution for this ?","May 12, 2024 9:12 am","EkmekE","Any solution to that?","Oct 15, 2024 12:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2 tokens for one character in T5","https://discuss.huggingface.co/t/2-tokens-for-one-character-in-t5/14960","2","1.5k","Created: Feb 21, 2022 8:14 am Latest: Aug 10, 2023 5:25 pm","borgr","When tokenizing text, T5(small?) tokenizer adds an eos_token, that‚Äôs expected. A bit weird is that for the character\string\sentence ‚Äú0‚Äù, it tokenizes it to three tokens! One of them is a token that is detokenized to an empty string. Is that an error on my part? Is it a bug? How does that happen? T5 is char based, right, so at the bare minimum, each character should be in the dictionary. tokenizer_name = ""t5-small"" tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True) print(tokenizer.encode(""0"")) # [3, 632, 1] print(tokenizer.encode(""1"")) print(tokenizer.encode(""2"")) print(tokenizer.encode(""3"")) Outputs: [3, 632, 1] [209, 1] [204, 1] [220, 1]","Feb 21, 2022 8:14 am","hyesunyun","I have the same exact question. I can just call something like tokenizer.encode(""0"", add_special_tokens = False) to get rid of the special tokens such as eos_token. However, I am puzzled by the empty string tokens that you are seeing as well.","Aug 10, 2023 3:10 pm","dblakely","The extra token is a SentencePiece underline token, which is a space token to indicate that the proceeding token is either the start of a word or a standalone token. You can check what the actual tokens are like this: ids = tokenizer.encode(""0"") tokens = tokenizer.convert_ids_to_tokens(ids) print(tokens) Which shows: ['‚ñÅ', '0', '</s>'] The '‚ñÅ' is the 3 token you‚Äôre seeing. When you look at the tokens themselves or the token ids, you can see what‚Äôs going on, though when you decode or print, they‚Äôre omitted. In fact, when you use the T5 tokenizer, all words start with a space like this, just most of them have the space built into the token itself. For example, the token for ‚Äú1‚Äù is actually this: '‚ñÅ1' (it‚Äôs just one token, but contains a space and the character for the 1). Meanwhile, for words that contain multiple tokens, the first one in the word will have the space while the others won‚Äôt. For example: ids = tokenizer.encode(""Onomatopoeia"") tokens = tokenizer.convert_ids_to_tokens(ids) print(tokens) You get: ['‚ñÅOn', 'omato', 'p', 'o', 'e', 'i', 'a', '</s>'] Why the tokenizer adds a separate space token in front of ‚Äú0‚Äù, but has it merged into the token for 1, is probably just a statistical quirk of how the SentencePiece algorithm worked when it was run on the corpus the Google people used to create the T5Tokenizer. There‚Äôs no real a priori reason.","Aug 10, 2023 5:25 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ERROR?why encoding [MASK] before ‚Äò.‚Äô would gain a idx 13?","https://discuss.huggingface.co/t/error-why-encoding-mask-before-would-gain-a-idx-13/2897","5","1.0k","Created: Dec 20, 2020 1:27 pm Latest: Dec 27, 2021 9:31 pm","yucuinan","I found that if I use tokenizer for BERT,and decode some sentence with [MASK] before ‚Äò.‚Äô,it would gain an additional idx 13 like this: tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') inputs = tokenizer(""The capital of France is [MASK]."", return_tensors=""pt"") the ‚Äòinput_ids‚Äô would be: tensor([[ 2, 14, 1057, 16, 714, 25, 4, 13, 9, 3]]) 4 refers to [MASK], 9 refers to ‚Äò.‚Äô, but 13 refers to None this would be happen when [MASK] is before ‚Äò.‚Äô is there something wrong?","Dec 20, 2020 1:27 pm","david-waterworth","You don‚Äôt generally mask works by replacing the text of the word with ‚Äú[MASK]‚Äù. You usually encode the text firest ‚ÄúThe capital of France is Paris.‚Äù and then replace the token for Paris with the Mask token. Perhaps that‚Äôs the reason for the addition of toke 13?","Dec 21, 2020 10:11 pm","BramVanroy","I am not sure what the cause is for this. So in sentencepiece you typically have a special character ‚ñÅ before the subword token. In this case, apparently 13 in vocab is the empty token ‚ñÅ. I am not sure why it is needed in this case though. For those who want to try stuff out, here is a MVCE: from transformers import AlbertTokenizer  tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') inputs = tokenizer.encode(""The capital of France is [MASK]."") print(inputs) # [2, 14, 1057, 16, 714, 25, 4, 13, 9, 3] print(tokenizer.convert_ids_to_tokens(inputs)) # ['[CLS]', '‚ñÅthe', '‚ñÅcapital', '‚ñÅof', '‚ñÅfrance', '‚ñÅis', '[MASK]', '‚ñÅ', '.', '[SEP]'] print(tokenizer.decode(inputs)) # [CLS] the capital of france is[MASK].[SEP] Perhaps @lysandre (ALBERT) or @mfuntowicz (tokenizers) have an idea.","Dec 22, 2020 9:46 am","david-waterworth","If anything I‚Äôd expect the ‚Äò_‚Äô token to appear before rather than after ‚Äò[MASK]‚Äô. i.e from transformers import AlbertTokenizer  tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2') inputs = tokenizer.encode(""The capital of France is Paris."") print(inputs) # [2, 14, 1057, 16, 714, 25, 1162, 9, 3] print(tokenizer.convert_ids_to_tokens(inputs)) # ['[CLS]', '‚ñÅthe', '‚ñÅcapital', '‚ñÅof', '‚ñÅfrance', '‚ñÅis', '‚ñÅparis', '.', '[SEP]'] print(tokenizer.decode(inputs)) # [CLS] the capital of france is paris.[SEP] I suspect it‚Äôs something to do with the fact that [MASK] is a special, all other word tokens will start with _. Perhaps there‚Äôs a bug where ‚Äô [MASK]‚Äô maps to ‚Äò[MASK]‚Äô, ‚Äò_‚Äô rather than ‚Äò_‚Äô ,‚Äô[MASK]‚Äô? Or perhaps it‚Äôs just not supported (tokenising text which actually contains literal specials?)","Dec 22, 2020 10:52 pm","yucuinan","this would be a solution. but how to say the tutorial shows in https://huggingface.co/transformers/model_doc/albert.htmlÔºü","Dec 28, 2020 8:02 am","matus","This behavior is also present in other models, e.g. in xlm-roberta-base where a token with id 6 is injected. The token is empty string '', which can be quite confusing. This bug should be addressed or at least it should be indicated that placing mask tokens is not recommended.","Dec 27, 2021 9:31 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3","https://discuss.huggingface.co/t/error-loading-tokenizer-data-did-not-match-any-variant-of-untagged-enum-modelwrapper-at-line-1251003-column-3/111124","3","1.3k","Created: Oct 10, 2024 6:40 am Latest: Oct 10, 2024 10:25 pm","Manju2050","I have quantized the meta-llama/Llama-3.1-8B-Instruct model using BitsAndBytesConfig. However when i try deploying it to sagemaker endpoint, it throws error. I traiged this in a docker container locally and see the same issue reproduced. Sharing the code of my docker file and the working snippets of jupyter notebook for your reference: Docker code: FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0  COPY requirements.txt .  RUN pip install -r requirements.txt  # Set the working directory WORKDIR /opt/ml  # Copy your script into the container COPY inference.py .  # Install your dependencies # RUN pip install --upgrade ""transformers>=4.46.0"" accelerate bitsandbytes peft  # Run the script (if you want it to execute automatically) ENTRYPOINT [""python"", ""inference.py""] requirements.txt : transformers==4.44.2 accelerate==0.34.2 bitsandbytes==0.44.1 peft==0.13.1 Errror: sh-4.2$ docker run -it --gpus all -v /home/ec2-user/SageMaker/saved_models/Llama-3.1-8B-Instruct-test1:/opt/ml/model -e HF_MODEL_ID=/opt/ml/model 8b:v1 --model-id /opt/ml/model --quantize bitsandbytes Unused kwargs: [‚Äòdevice_map‚Äô]. These kwargs are not used in <class ‚Äòtransformers.utils.quantization_config.BitsAndBytesConfig‚Äô>. Unused kwargs: [‚Äò_load_in_4bit‚Äô, ‚Äò_load_in_8bit‚Äô, ‚Äòquant_method‚Äô]. These kwargs are not used in <class ‚Äòtransformers.utils.quantization_config.BitsAndBytesConfig‚Äô>. /opt/conda/lib/python3.9/site-packages/transformers/quantizers/auto.py:174: UserWarning: You passed quantization_config or equivalent parameters to from_pretrained but the model you‚Äôre loading already has a quantization_config attribute. The quantization_config from the model will be used. warnings.warn(warning_msg) low_cpu_mem_usage was None, now set to True since model is quantized. DEBUG:bitsandbytes.cextension:Loading bitsandbytes native library from: /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118.so Loading checkpoint shards: 0%| Loading checkpoint shards: 50%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñàLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:01<00:00, 1.41it/s] ERROR:root:Error loading tokenizer: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 Traceback (most recent call last): File ‚Äú/opt/ml/inference.py‚Äù, line 70, in tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False) File ‚Äú/opt/conda/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py‚Äù, line 897, in from_pretrained return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs) File ‚Äú/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py‚Äù, line 2271, in from_pretrained return cls._from_pretrained( File ‚Äú/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_base.py‚Äù, line 2505, in _from_pretrained tokenizer = cls(*init_inputs, **init_kwargs) File ‚Äú/opt/conda/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py‚Äù, line 115, in init fast_tokenizer = TokenizerFast.from_file(fast_tokenizer_file) Exception: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 Any suggestions or inputs on how to avoid this error? I have the sagemaker endpoint deployment script as below: import json from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri  # sagemaker config instance_type = ""ml.g5.12xlarge"" number_of_gpu = 4 health_check_timeout = 300   # retrieve the llm image uri # llm_image = get_huggingface_llm_image_uri( #   ""huggingface"", #   version=""0.8.2"" # )  # print ecr image uri print(f""llm image uri: {llm_image}"") # this would print: llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.0-tgi0.8.2-gpu-py39-cu118-ubuntu20.04  s3_model_uri = ""s3uri""  # llm_image = ""763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.29.0-lmi11.0.0-cu124"" llm_image = ""763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0""  # Define Model and Endpoint configuration parameter config = {   'HF_MODEL_ID': ""/opt/ml/model"", # path to where sagemaker stores the model   'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica   'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text   'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)   'HF_MODEL_QUANTIZE': ""bitsandbytes"",# Comment in to quantize }  # create HuggingFaceModel with the image uri llm_model = HuggingFaceModel(   role=role,   image_uri=llm_image,   model_data=s3_model_uri,   env=config )  endpoint_name=""model-ptq-test-llama-8b-v1""  llm = llm_model.deploy(     initial_instance_count=1,     instance_type=instance_type,     # volume_size=400, # If using an instance with local SSD storage, volume_size must be None, e.g. p4 but not p3     container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model     endpoint_name=endpoint_name ) It works fine locally in a jupyter notebook: code snippets: !pip install bitsandbytes==0.44.1 !pip install accelerate==0.34.2 !pip install transformers==4.44.2 !pip install peft==0.13.1 import torch from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig from transformers import LlamaForCausalLM, LlamaTokenizer from huggingface_hub import login  token=""hugging Face token""  login(token=token, add_to_git_credential=True)  # model_id = ""meta-llama/Meta-Llama-3.1-70B-Instruct"" model_id = ""meta-llama/Llama-3.1-8B-Instruct"" #model_id = ""meta-llama/Llama-3.1-8B"" #model_id = ""google-t5/t5-small""  cache_dir = ""/home/ec2-user/SageMaker/huggingface_cache""  # model_id = ""facebook/opt-350m"" model_id = ""meta-llama/Llama-3.1-8B-Instruct"" bnb_config = BitsAndBytesConfig(     load_in_4bit=True,     bnb_4bit_use_double_quant=True,     bnb_4bit_quant_type=""nf4"",     bnb_4bit_compute_dtype=torch.bfloat16,     device_map=""auto"" )  model_4bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config) test: tokenizer = AutoTokenizer.from_pretrained(""/home/ec2-user/SageMaker/saved_models/Llama-3.1-8B-Instruct-test1"")  # Prepare your input text # input_text = ""Translate the following English text to French: 'Hello, how are you?'""  input_text = """"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>Translate the following English text to French: 'Hello, how are you?'<|eot_id|><|start_header_id|>assistant<|end_header_id|>""""""   #input_text = ""What are we having for dinner?"" input_ids = tokenizer(input_text, return_tensors=""pt"").to(""cuda"") print(input_ids)  output = model_4bit.generate(**input_ids, max_new_tokens=10)  print(tokenizer.decode(output[0], skip_special_tokens=True)) Output: Setting pad_token_id to eos_token_id:None for open-end generation. {‚Äòinput_ids‚Äô: tensor([[128000, 128000, 128006, 882, 128007, 28573, 279, 2768, 6498, 1495, 311, 8753, 512, 6, 9906, 11, 1268, 527, 499, 20837, 128009, 128006, 78191, 128007]], device=‚Äòcuda:0‚Äô), ‚Äòattention_mask‚Äô: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device=‚Äòcuda:0‚Äô)} userTranslate the following English text to French: 'Hello, how are you?'assistant ‚ÄúBonjour, comment vas-tu?‚Äù","Oct 10, 2024 6:40 am","John6666","I heard that updating the library may fix the problem. github.com/unslothai/unsloth [FIXED] Exception: data did not match any variant of untagged enum ModelWrapper at line 1251003 column 3 opened Sep 25, 2024 djannot fixed - pending confirmation URGENT BUG I get this error:  ``` Traceback (most recent call last):   File ""/home/deni‚Ä¶","Oct 10, 2024 7:42 am","Manju2050","Thank you @John6666 for your response. yes, it works fine after updating the version of transformers on both the ecr images in a local test: huggingface-pytorch-tgi-inference: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0 pytorch-inference 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.4.0-gpu-py311-cu124-ubuntu22.04-sagemaker The issue is, I‚Äôm unable to deploy this model on aws sagemaker . Errors seen from the container logs: i do see container crash logs again for both the ecr images: 2024-10-10T19:22:25,964 [WARN ] W-9003-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died. 2024-10-10T19:22:25,968 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - File ‚Äú/opt/ml/model/code/inference.py‚Äù, line 2, in 2024-10-10T19:22:25,968 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig 2024-10-10T19:22:25,968 [INFO ] W-9003-model_1.0-stdout MODEL_LOG - ModuleNotFoundError: No module named ‚Äòtransformers‚Äô With pytorch-inference i used the following script: Directory Structure: model.tar.gz |- model artifacts  |- code/    |- inference.py         # Your inference script    |- requirements.txt     # Optional, used to install additional dependencies (if supported by your framework version) file: requirements.txt transformers>=4.45 accelerate==0.34.2 bitsandbytes==0.44.1 peft==0.13.1 file: inference.py import torch from transformers import AutoModelForCausalLM, AutoTokenizer import os import logging  # Enable logging logging.basicConfig(level=logging.INFO)  # Model loading function def model_fn(model_dir):     # Load the model     try:         model_4bit = AutoModelForCausalLM.from_pretrained(model_dir)     except Exception as e:         logging.error(f""Error loading model: {e}"")         raise      # Load the tokenizer     try:         tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False)     except Exception as e:         logging.error(f""Error loading tokenizer: {e}"")         raise      return model_4bit, tokenizer   # Prediction function def predict_fn(data, model_and_tokenizer):     model, tokenizer = model_and_tokenizer      # Logging the input     logging.info(f""Received input: {data}"")      input_text = f""""""<|begin_of_text|><|start_header_id|>user<|end_header_id|>{data['inputs']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>""""""      # Tokenize input and move it to the correct device (GPU/CPU)     input_ids = tokenizer(input_text, return_tensors=""pt"").to(model.device)      # Generate text using the model     with torch.no_grad():         output = model.generate(**input_ids, max_new_tokens=50)      # Decode the output tokens back to text     result = tokenizer.decode(output[0], skip_special_tokens=True)      # Logging the generated output     logging.info(f""Generated output: {result}"")      return result Deployment script for PytorchModel: import sagemaker from sagemaker.pytorch import PyTorchModel  # Define the IAM role and model location in S3 model_data = ""s3://compressed-model/Llama-3.1-8B-Instruct-test1-pytorch.tar.gz"" image_uri=""763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:2.4.0-gpu-py311-cu124-ubuntu22.04-sagemaker"" # sagemaker config instance_type = ""ml.g5.12xlarge"" number_of_gpu = 4 health_check_timeout = 300 endpoint_name=""model-ptq-llama-8b-test1""  # Create a PyTorchModel instance pytorch_model = PyTorchModel(     model_data=model_data,     role=role,     entry_point=""inference.py"",     image_uri=image_uri )  # Deploy the model to an endpoint predictor = pytorch_model.deploy(     initial_instance_count=1,     instance_type=instance_type, # Choose an appropriate instance type     endpoint_name=endpoint_name,     container_startup_health_check_timeout=health_check_timeout # 10 minutes to be able to load the model ) Deployment script for HuggingFaceModel import sagemaker from sagemaker.huggingface import HuggingFaceModel  # Define the IAM role and model location in S3 model_data = ""s3://compressed-model/Llama-3.1-8B-Instruct-test1-pytorch.tar.gz"" image_uri=""763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.0.1-tgi0.9.3-gpu-py39-cu118-ubuntu20.04-v1.0"" # sagemaker config instance_type = ""ml.g5.12xlarge"" number_of_gpu = 4 health_check_timeout = 300 endpoint_name=""model-ptq-llama-8b-test3""   llm_model = HuggingFaceModel(     model_data=model_data,     role=role,     entry_point=""inference.py"",     source_dir=""code"",  # Ensure the path to the `code/` directory is specified     image_uri=image_uri ) # Deploy the model to an endpoint predictor = pytorch_model.deploy(     initial_instance_count=1,     instance_type=instance_type, # Choose an appropriate instance type     endpoint_name=endpoint_name,     container_startup_health_check_timeout=health_check_timeout # 10 minutes to be able to load the model )","Oct 10, 2024 9:47 pm","John6666","ModuleNotFoundError: No module named ‚Äòtransformers‚Äô Definitely something wrong in sagemaker side‚Ä¶ stackoverflow.com ModuleNotFoundError: no module named 'transformers' python, artificial-intelligence, google-colaboratory, importerror asked by Khaleel Shazada on 09:54PM - 06 Feb 22 UTC Amazon Web Services, Inc. using transformers module with sagemaker studio project: ModuleNotFoundError:... So as mentioned in my [other recent post](https://repost.aws/questions/QUAL9Vn9abQ6KKCs2ASwwmzg/adjusting-sagemaker-xgboost-project-to-tensorflow-or-even-just-different-folder-name), I'm trying to ...","Oct 10, 2024 10:25 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding new tokens to a BERT tokenizer - Getting ValueError","https://discuss.huggingface.co/t/adding-new-tokens-to-a-bert-tokenizer-getting-valueerror/9253","2","1.5k","Created: Aug 15, 2021 10:49 am Latest: Jan 16, 2022 7:06 pm","kaankork","I have a Python list named unique_list that contains new words that will be added to my tokenizer using tokenizer.add_tokens. However, when I run my code I‚Äôm getting the following error: File ""/home/kaan/anaconda3/envs/env_backup/lib/python3.8/site-packages/transformers/tokenization_utils_base.py"", line 937, in add_tokens     if not new_tokens: ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() When I tested with a test array that contains 10 random words, it worked fine but the larger unique_list is causing a problem. What am I doing wrong here?","Aug 15, 2021 10:49 am","impiyush","Not sure if you already solved this issue, but I stumbled upon it today. Looking closely at the error message, looks like the add_tokens() method expects the new_tokens passed to it as a python list rather than an numpy array. Converting new_tokens from numpy array to a list and then passing it resolved the issue. added_tokens = tokenizer.add_tokens(new_tokens.tolist())","Dec 3, 2021 10:16 pm","toyl","excuse me if I need to solve the problem of not finding the word after tokenizing using BERT ‚Ä¶ can I use your solution","Jan 16, 2022 7:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DeBERTa - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‚Äòpadding=True‚Äô ‚Äòtruncation=True‚Äô to have batched tensors with the same length","https://discuss.huggingface.co/t/deberta-valueerror-unable-to-create-tensor-you-should-probably-activate-truncation-and-or-padding-with-padding-true-truncation-true-to-have-batched-tensors-with-the-same-length/45625","2","1.4k","Created: Jul 4, 2023 3:13 pm Latest: Oct 3, 2023 6:24 pm","paulopirozelli","I‚Äôm trying to train a number of transformer models on a classification task. My dataset has only two columns: text and label. As part of the pre-processing, I tokenize, pad, and truncate the texts. The input for this function is a datasets.dataset_dict.DatasetDict object, where ‚Äòcheckpoint‚Äô refers to the transformer model been trained and ‚Äòdataset_dict‚Äô refers to the pandas dataframe that is been tokenized. The pre-processing is done with the following script. from transformers import AutoTokenizer, DataCollatorWithPadding from torch.utils.data import DataLoader  def tokenizer_padding(dataset_dict, checkpoint, batch_size):     tokenizer = AutoTokenizer.from_pretrained(checkpoint)     data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length')      def tokenize_function(examples):         return tokenizer(examples[""text""], truncation=True)      tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)     tokenized_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""label""])      train_dataloader = DataLoader(         tokenized_dataset[""train""], shuffle=True, batch_size=batch_size, collate_fn=data_collator     )     validation_dataloader = DataLoader(         tokenized_dataset[""validation""], batch_size=batch_size, collate_fn=data_collator     )     test_dataloader = DataLoader(         tokenized_dataset[""test""], batch_size=batch_size, collate_fn=data_collator     )      return train_dataloader, validation_dataloader, test_dataloader When I train for BERT and RoBERTa, the tokenizer_padding function works perfetly. However, when I use deberta (microsoft/deberta-v3-base), I get the following error: ValueError: Couldn‚Äôt instantiate the backend tokenizer from one of: (1) a tokenizers library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one. With sentencepiece installed, I get this second error: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with ‚Äòpadding=True‚Äô ‚Äòtruncation=True‚Äô to have batched tensors with the same length. Perhaps your features (input_ids in this case) have excessive nesting (inputs type list where type int is expected). I have no idea why only the DeBERTa model fails. Can someone help me with that?","Jul 4, 2023 3:13 pm","Sandy1857","Hope your issue is resolved by now. If not, you could use some of these steps: When loading the tokenizer, you could use the use_fast=False parameter to set the slow tokenizer instead of fast one: eg. tokenizer = AutoTokenizer.from_pretrained(DEBERTA_MODEL, use_fast=False) If you want to use the fast tokenizer, install sentencepiece and then restart the kernel for the update to take effect. If the ValueError: Unable to create tensor, arises, you could add padding=‚Äòmax_length‚Äô or ‚Äòlongest‚Äô instead of True.","Oct 3, 2023 5:43 pm","paulopirozelli","Thanks, @Sandy1857. After some head-scratching, the final solution did the trick. It appears that ‚Äòdeberta‚Äô processes inputs differently, requiring me to tweak the padding argument. When I set it to padding=‚Äòlongest‚Äô, it worked fine.","Oct 3, 2023 6:24 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLLB tokenizer multiple target/source languages within a training batch","https://discuss.huggingface.co/t/nllb-tokenizer-multiple-target-source-languages-within-a-training-batch/56307","4","1.1k","Created: Sep 25, 2023 1:48 pm Latest: Jul 17, 2024 11:36 pm","nitindub","From the docs, one way to load the nllb tokenizer is: tokenizer = AutoTokenizer.from_pretrained(""facebook/nllb-200-distilled-1.3B"", use_auth_token=False, src_lang=""eng_Latn"", tgt_lang=""hin_Deva"") But, this way, tgt_lang which gets assigned is hin_Deva for all labels. Is there a way to set the tgt_lang code when calling the tokenizer function, so that one can have multiple languages within a batch while training. e.g. # loading tokenizer tokenizer = AutoTokenizer.from_pretrained(""facebook/nllb-200-distilled-1.3B"", use_auth_token=False, src_lang=""eng_Latn"") model_inputs = tokenizer(inputs, text_target=targets, target_langs=target_langs, max_length=max_length, truncation=True)  # where target_langs = ['hin_Deva', 'tur_Latn', ....]","Sep 25, 2023 1:48 pm","KhaiKit","Hi anyone has a solution to this? To my understanding, one way to prepare the training data for finetuning if we feed the model the same sentence pair twice but flipped so that the model learns to translate in both direction. Eg (LHS is the Input & RHS is the Target): {""eng_Latn"": ""Hi"", ""zho_Hans"": ""‰Ω†Â•Ω""} {""zho_Hans"": ""‰Ω†Â•Ω"", ""eng_Latn"": ""Hi""} But I can‚Äôt seem to find a way to do this with the Trainer API.","Oct 20, 2023 9:55 am","nitindub","Not sure, but found a workaround. Created a preprocess function that appends the tokenizer.lang_code_to_id[ex[""tgt_lang""]] value as part of tokenization process. def preprocess_function_helper(tokenizer, context_length=3):     def preprocess_function(examples, max_length=128):         ......         model_inputs = tokenizer(             inputs, text_target=targets, max_length=max_length, truncation=True         )          # Update first token id in each model_inputs['labels'] with lang_tokens         for i in range(batch_size):             model_inputs['labels'][i][0] = lang_tokens[i]         .....     return preprocess_function  dataset = dataset.map(         preprocess_function_helper(tokenizer=tokenizer, context_length=context_length),         batched=True,         remove_columns=[""translation""],         load_from_cache_file=False,     ) which gets passed into the models. Hope that helps.","Nov 20, 2023 3:49 pm","KhaiKit","Hey thanks for sharing! I think that helps a lot to ensure that the correct lang_id is attached to the correct input sequence. However, when it comes to the training, how will the model know when to switch the output token id (In your case, between eng_Latn & hin_Deva, or in my example, between eng_Latn & zho_Hans) The reason why I ask this is because I know during inference after the model has been trained, we need to specify the forced_bos_token_id to generate output. Thus I am not sure if there is a need/way to specify the forced_bos_token_id while training. Edit: I just realised that multi-lingual finetuning/training is not supported and is unlikely to be supported after reading a related issue posted on huggingface‚Äôs Github","Nov 20, 2023 4:26 pm","natekrasner","I know this a late response, but I have been doing multilingual finetuning without issue (mostly). As long as the first token of the target sentence is the correct output lang_id, the teacher forcing will include it after the first decoding step. The model does not have to correctly predict the output lang_id, it only has to correctly predict the output sequence given the correct lang_id. As far as multiple languages in one batch goes, I‚Äôll update here if my code works. I made a custom torch dataset which prepends the lang_ids and tokenizes the sequences in the __getitem__ function. This way the sequences will be properly tokenized before the dataloader batches them. I am still working on my training loop code for this functionality, so I am unsure of what might go wrong.","Jul 17, 2024 11:31 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Special token printed out as output","https://discuss.huggingface.co/t/special-token-printed-out-as-output/62948","6","941","Created: Nov 21, 2023 3:52 am Latest: Nov 24, 2023 8:16 am","js2jang","Hi i have a question about a llm printing out special token as well when generating an answer. Here is an example: from utils.prompter import Prompter # from utils.util import postprocessing, e2k_model from deeppostagger import tagger from transformers import TextIteratorStreamer, PreTrainedTokenizerFast from threading import Thread from auto_gptq import AutoGPTQForCausalLM import warnings warnings.filterwarnings('ignore') new_line_chr = ['.', '?'] rm_chr = ['<|endoftext|>']  class LLM_qa:     def __init__(self, model_path, max_len):         self.model = AutoGPTQForCausalLM.from_quantized(             model_path,              device_map=""balanced"", max_memory = {0: ""10GB"", 1: ""10GB""},              low_cpu_mem_usage=True             )          self.model.config.use_cache = True         self.model.eval()          self.max_len = max_len          self.tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)                  self.prompter = Prompter(""kullm"")         self.prompter_gen = Prompter(""kullm"")      def qa(self, question, instruction=''):         if instruction:             prompt = self.prompter_gen.generate_prompt(instruction, question)             self.max_len *=2         else:             prompt = self.prompter.generate_prompt(question)          inputs = self.tokenizer(prompt, return_tensors=""pt"")         streamer = TextIteratorStreamer(self.tokenizer, skip_prompt=True)          generation_kwargs = dict(             input_ids=inputs.input_ids[..., :-1],             streamer=streamer, max_new_tokens=self.max_len, no_repeat_ngram_size=3, eos_token_id=2, pad_token_id=self.tokenizer.eos_token_id             )                  thread = Thread(target=self.model.generate, kwargs=generation_kwargs)          return thread, streamer  MODEL_PATH = '/mnt/research/datasets/llm/weights/kullm-polyglot-12.8b-v2/20231115_dupex_quantize' MAX_LEN = 128  llm = LLM_qa(MODEL_PATH, MAX_LEN)  q = 'Hi?' instruction = ''  thread, streamer = llm.qa(q, instruction)  thread.start()  generated_text = '' for new_text in streamer:     flg = [True for i in new_line_chr if i in new_text]     # for c in rm_chr:     #     new_text = new_text.replace(c, '')     if new_text and flg:         print(new_text)     elif new_text:         print(new_text, end='')      # generated_text += new_text     # flg = [True for i in new_line_chr if i in new_text]     # if flg:     #     print(generated_text)     #     # print(engsen2korsen(generated_text))     #     generated_text = ''  print(""\n - done."") OUTPUT = ÏïàÎÖïÌïòÏÑ∏Ïöî! Ïò§ÎäòÏùÄ Î¨¥ÏóáÏùÑ ÎèÑÏôÄÎìúÎ¶¥ÍπåÏöî?<|endoftext|> I dont know why <|endoftext|> is printed‚Ä¶ please help rm_chr = [‚Äò<|endoftext|>‚Äô] is one way that I tried removing the special token and it does work but I wanna know why is this happening and if there are any ways to fix it.","Nov 21, 2023 3:52 am","Yntec","Have you tried using this on line 10 of your code?: rm_chr = [‚Äò‚Äô]","Nov 21, 2023 3:55 am","js2jang","Well thats one way that I tried to erase the token and it does work but I wanna know why is the special token gets printed out any way. Is it because of tokenizer?","Nov 21, 2023 3:56 am","Yntec","Whatever you put inside the single quotes in rm_chr = [‚Äò‚Äô] will get printed, <|endoftext|> was printed because that‚Äôs what you put there.","Nov 21, 2023 8:10 am","js2jang","rm_chr is to erase anything I put in there if u look at this     # for c in rm_chr:     #     new_text = new_text.replace(c, '')","Nov 22, 2023 12:27 am","Yntec","That code is not being run because it has a # at the beginning causing it to be ignored.","Nov 24, 2023 7:59 am","js2jang","Yea I know‚Ä¶","Nov 24, 2023 8:16 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer from own vocab","https://discuss.huggingface.co/t/tokenizer-from-own-vocab/20245","0","435","Created: Jul 11, 2022 6:00 pm Latest: Jul 11, 2022 6:00 pm","ockapuh","Hi everyone. I would like to build a tokenizer that uses as vocabulary a list of pre-defined tokens I have. Would that be possible with the Transformers library and, in that case, how could I do it? Thanks.","Jul 11, 2022 6:00 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Couldn‚Äôt instantiate the backend tokenizer","https://discuss.huggingface.co/t/couldnt-instantiate-the-backend-tokenizer/2662","0","2.3k","Created: Dec 7, 2020 2:19 pm","wei","I have noticed the updates and now I have a problem about loading my sentencepiece tokenizer. When I compute some of my models in the hugging face UI, I got the following error: Couldn‚Äôt instantiate the backend tokenizer from one of: (1) a tokenizers library serialization file, (2) a slow tokenizer instance to convert or (3) an equivalent slow tokenizer class to instantiate and convert. You need to have sentencepiece installed to convert a slow tokenizer to a fast one. I tried to save my tokenizer again after installing transfomers and sentencespiece like: from transformers import T5Tokenizer tok = T5Tokenizer.from_pretrained(‚Äúmy_spm.model‚Äù) tok.save_pretrained(path_to_tf_checkpoint) But this doesn‚Äôt solve my problem. Any idea what should I do?","Dec 7, 2020 2:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPT2Tokenizer.decode maps unicode sequences to the same string ‚ÄòÔøΩ‚Äô","https://discuss.huggingface.co/t/gpt2tokenizer-decode-maps-unicode-sequences-to-the-same-string/32453","3","1.1k","Created: Feb 24, 2023 4:32 pm Latest: Mar 15, 2023 7:36 pm","Cbelem","Hello everyone, I have a naive question about tokenizers, particularly GPT2 Tokenizer. I have this encoded a text sentence, and I‚Äôve obtained the token: 29826, which in GPT2Tokenizer Vocabulary corresponds to the Unicode sequence ‚Äú\u00e6\u0143‚Äù. For some reason, I needed to convert 29826 back to its token, i.e., into text, so I used the following code snippet: from transformers import GPT2Tokenizer, AutoModelForCausalLM import torch import torch.nn.functional as F  tokenizer = GPT2Tokenizer.from_pretrained( ""EleutherAI/gpt-neo-125M"") model = AutoModelForCausalLM.from_pretrained( ""EleutherAI/gpt-neo-125M"") tokenizer.pad_token_id = tokenizer.eos_token_id I found myself comparing the logits distribution for token_id 29826 using the code below: # Version 1. No need to encode because we already have access to token_id token_ids_1 = torch.tensor([[29826]]) logits_1 = model.forward(token_ids_1).logits.squeeze().detach().numpy()  # Version 2. Need to get input_ids token = tokenizer.decode([29826]) token_ids_2 = tokenizer(token, return_tensors=""pt"", add_special_tokens=False).input_ids logits_2 = model.forward(token_ids_2).logits.squeeze().detach().numpy()  ## Visualize the logits distribution import matplotlib.pyplot as plt import seaborn as sns  sns.histplot(logits_1, label=""Logits 1"") sns.histplot(logits_2, label=""Logits 2"") plt.legend() Surprisingly, the logit distributions were different (see attached picture). It appears that having this tokenizer.decode conversion step is actually creating the string ‚ÄòÔøΩ‚Äô, which is being decoded to the index 4210 (a different unicode sequence "" \u00ef\u00bf\u00bd""). In fact, both expressions tokenizer.decode([4210]) and tokenizer.decode([29826]) get decoded to the same ‚ÄòÔøΩ‚Äô character instead of their actual unique expression. But I was hoping that they would decode to their actual unicode string. Is there any way I can deal with this? Is this expected? I‚Äôve tried tweaking the string to decode to its actual unicode string but failed. Environment: transformers version: 4.26.1 tokenizers version: 0.13.2 Platform: Linux-5.4.0-113-generic-x86_64-with-glibc2.31 Python version: 3.9.16 Huggingface_hub version: 0.12.0 PyTorch version (GPU?): 1.12.1 (False) Tensorflow version (GPU?): not installed (NA) Flax version (CPU?/GPU?/TPU?): not installed (NA) Jax version: not installed JaxLib version: not installed Using GPU in script?: False Using distributed or parallel set-up in script?: False I executed the code snippets on a Jupyter Notebook (jupyterlab 3.6.1).","Feb 24, 2023 4:32 pm","Cbelem","I guess the more appropriate way would be to use: tokenizer.convert_ids_to_tokens([4210])","Feb 24, 2023 8:46 pm","Cbelem","On the use of tokenizer.convert_ids_tokens, I‚Äôve found that. the two expressions below lead to two different strings: ts = tokenizer.encode(""Hello!\n I can't do this anymore"") # [15496, 0, 198, 314, 460, 470, 466, 428, 7471] and tokenizer.convert_ids_to_tokens(ts) # ['Hello', '!', 'ƒä', 'ƒ†I', 'ƒ†can', ""'t"", 'ƒ†do', 'ƒ†this', 'ƒ†anymore'] How should I interpret the difference between the two?","Feb 24, 2023 9:57 pm","Cbelem","Based on https://discuss.huggingface.co/t/bpe-tokenizers-and-spaces-before-words/475: In GPT2 and Roberta tokenizers, the space before a word is part of a word, i.e. ""Hello how are you puppetter"" will be tokenized in [""Hello"", ""ƒ†how"", ""ƒ†are"", ""ƒ†you"", ""ƒ†puppet"", ""ter""]. You can notice the spaces included in the words a ƒ† here. Spaces are converted in a special character (the ƒ† ) in the tokenizer prior to BPE splitting mostly to avoid digesting spaces since the standard BPE algorithm used spaces in its process (this can seem a bit hacky but was in the original GPT2 tokenizer implementation by OpenAI).","Mar 15, 2023 7:36 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Dealing with Decimal and Fractions","https://discuss.huggingface.co/t/dealing-with-decimal-and-fractions/23377","1","1.5k","Created: Sep 20, 2022 7:01 am Latest: Apr 14, 2023 7:35 am","amkorba","Hi Team, I‚Äôm currently dealing with dataset which contains decimal and fraction and my task have big impact because of those numbers. When I‚Äôm trying Bert based tokenizer, its tokenizing ‚Äú1-1/2‚Äù as [1,-,1,/,2] but I want it as single token i.e [‚Äú1-1/2‚Äù] Something similar happening with decimals too. Please suggest the possible solutions to tokenize it properly. Thanks Ashish","Sep 20, 2022 7:01 am","lianghsun","Hi @amkorba, you should try retrain a Bert based tokenizer and using customized Split(). Let‚Äôs step by step to demostrate it . Step 1: Suppose you have a dataset (fraction.txt) likes below: Hi 1-1/2 there Lorem oweh 3/4 Step 2: Load pre-trained Bert tokenizer to HF Tokenizers from tokenizers import Tokenizer from tokenizers.pre_tokenizers import Split  tokenizer = Tokenizer.from_pretrained(""bert-base-uncased"") Step 3: Define the patterns to split fraction from tokenizers import Regex  pre_tokenizer = Split(     pattern=Regex(""( ([1-9]\d*\-) | ([1-9]\d*\/[1-9]\d*) | ([1-9]\d*\-[1-9]\d*\/[1-9]\d*) )""),     behavior='isolated' ) pre_tokenizer.pre_tokenize_str('Hi 1-1/2 there') # > [('Hi', (0, 2)), (' 1-1/2 ', (2, 9)), ('there', (9, 14))] <-- it works! Step 4: Re-train Bert tokenizer to learn fraction into your vocab! from tokenizers.trainers import WordPieceTrainer trainer = WordPieceTrainer(     vocab_size=30522, special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""] ) files = ['/path/2/fraction.txt'] tokenizer.train(files, trainer) Step 5: Check the new trained tokenizer! tokenizer.encode(""Hi 1-1/2 there"").tokens # > ['[CLS]', 'hi', ' 1-1/2 ', 'ther', '##e', '[SEP]']","Oct 27, 2022 8:56 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5 model tokenizer","https://discuss.huggingface.co/t/t5-model-tokenizer/23640","2","1.2k","Created: Sep 26, 2022 4:38 pm Latest: Sep 29, 2022 11:54 am","antoine2323231","T5 models are using BPE tokenizers? Is it possible to use another type of tokenizer along a T5 model, or not because they are designed to work with BPE?","Sep 26, 2022 4:38 pm","michaelwechner","AFAIK T5 is using SentencePiece T5 which has BPE implemented GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation. and therefore depends on this. Why would you like to use another tokenizer?","Sep 27, 2022 1:39 pm","michaelwechner","The question got answered on Discord If you‚Äôre training from scratch, then you would typically train a tokenizer on your own data, in which case you can choose which tokenizer training algorithm (BPE, WordPiece or UnigramLM if you‚Äôre using tokenizers) and how to preprocess the data before tokenizing it. I can recommend this chapter of the HF course to learn more about tokenizers: Introduction - Hugging Face Course","Sep 29, 2022 11:54 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SentencePiece user_defined_symbols and fast tokenizers","https://discuss.huggingface.co/t/sentencepiece-user-defined-symbols-and-fast-tokenizers/52208","1","1.4k","Created: Aug 25, 2023 11:01 pm Latest: Jan 3, 2024 4:23 pm","koyfman","I have a trained SentencePiece (BPE) model that I want to be able to load through AutoTokenizer as a fast tokenizer. This works with either T5 or Llama (slow) tokenizer classes, but when I try to load it using one of the fast classes, the encoding does not use the list of user_defined_symbols I specify as a parameter when training. Is there a way around this? Thank you. Example: Train with spm_train adding one user defined symbol: spm_train --input=./botchan.txt --model_prefix=my_test --vocab_size=8000 --character_coverage=1.0 --model_type=bpe --user_defined_symbols=kwyjibo``` Load and test with HF - the slow version encodes the custom word with one token but the fast breaks it up into many tokens. >>> from transformers import AutoTokenizer, T5Tokenizer, T5TokenizerFast >>> slow = T5Tokenizer(vocab_file=""./my_test.model"", use_fast=False) >>> e = slow.encode(""there goes a kwyjibo"") >>> e [178, 1263, 6, 7917, 3, 2] >>> print(slow.convert_ids_to_tokens(e)) ['‚ñÅthere', '‚ñÅgoes', '‚ñÅa', '‚ñÅ', 'kwyjibo', '</s>'] >>> fast = T5TokenizerFast(vocab_file=""./my_test.model"", use_fast=True) >>> f = fast.encode(""there goes a kwyjibo"") >>> f [178, 1263, 6, 152, 7932, 7935, 7953, 1136, 7920, 2] >>> print(fast.convert_ids_to_tokens(f)) ['‚ñÅthere', '‚ñÅgoes', '‚ñÅa', '‚ñÅk', 'w', 'y', 'j', 'ib', 'o', '</s>'] >>>```","Aug 25, 2023 11:01 pm","KhaiKit","Hey I‚Äôm no expert but just a suggestion, I think you can omit the ‚Äúuser_defined_symbols‚Äù when training the SentencePiece (BPE) model and instead just use add_tokens() method to add special tokens. Should be able to achieve your intended outcome?","Jan 3, 2024 4:23 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error training MLM with Roberta Tokenizer","https://discuss.huggingface.co/t/error-training-mlm-with-roberta-tokenizer/14878","1","1.4k","Created: Feb 18, 2022 7:35 pm Latest: Sep 17, 2023 7:30 am","djs","I am currently trying to train a MLM using a ByteLevelBPETokenizer on a custom corpus and am getting the following error: AttributeError: ‚Äòtokenizers.Tokenizer‚Äô object has no attribute ‚Äòmask_token‚Äô Shown below is the code: BOS = ‚Äú‚Äù EOS = ‚Äú‚Äù UNK = ‚Äú‚Äù PAD = ‚Äú‚Äù MASK = ‚Äú‚Äù tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False) tokenizer.decoder = decoders.ByteLevel() tokenizer.enable_truncation(max_length=512) tokenizer.enable_padding() trainer = BpeTrainer( vocab_size=50000, special_tokens=[BOS, PAD, EOS, UNK, MASK], initial_alphabet=pre_tokenizers.ByteLevel.alphabet() ) tokenizer.train_from_iterator(batch_iterator(), trainer=trainer) tokenizer.post_processor = RobertaProcessing( sep=(EOS, tokenizer.token_to_id(EOS)), cls=(BOS, tokenizer.token_to_id(BOS)) ) data_collator = DataCollatorForLanguageModeling( tokenizer, mlm_probability=0.15, return_tensors=‚Äòtf‚Äô) Any ideas? The current environment makes it difficult for me to save the tokenizer and load it back using a load from pretrained. Thanks","Feb 18, 2022 7:35 pm","ArmanAsq","I have the same problem here. Did you find any solution to it?","Sep 17, 2023 7:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to properly clean vocabulary from BBPE tokenizer","https://discuss.huggingface.co/t/how-to-properly-clean-vocabulary-from-bbpe-tokenizer/22827","3","995","Created: Sep 9, 2022 9:36 am Latest: Oct 1, 2022 9:20 pm","mdelas","Hello everyone, I have been building a Byte Level BPE tokenizer from DNA data by using the ByteLevelBPETokenizer method from tokenizers HuggingFace. I already obtained the vocab and the merges but there‚Äôs a lot of ‚Äúgarbage‚Äù vocabulary expected from a human language model (e.g. ‚Äú!‚Äù:3,‚Äú""‚Äù:4,‚Äú#‚Äù:5, ‚Äú$‚Äù:6,‚Äú%‚Äù:7,‚Äú&‚Äù:8,‚Äú'‚Äù:9,‚Äú(‚Äù:10) whereas I know for sure that my data will only be ACGT base DNA format sequences (e.g. ‚ÄúGGT‚Äù:270,‚ÄúAGC‚Äù:271,‚ÄúACC‚Äù:272,‚ÄúGGCC‚Äù:273,‚ÄúTTC‚Äù:274,‚ÄúAA‚Äù:275). Does anyone know if there is a way to clean all these symbols and automatically generated tokens in my vocabulary? Thank you very much in advance!","Sep 9, 2022 9:36 am","mdelas","I could already fix this problem by editing the method train in the class ByteLevelBPETokenizer. I changed the .alphabet from the BaseTokenizer into my choice which was not the 256 characters but [‚ÄúA‚Äù,‚ÄúC‚Äù,‚ÄúG‚Äù,‚ÄúT‚Äù,‚ÄúN‚Äù].","Sep 9, 2022 10:46 am","jessicalopez","mdelas: hod from tokenizers HuggingFace. I already obtained the vocab and the merges but there‚Äôs a lot of ‚Äúgarbage‚Äù vocabulary expected from a human language model (e.g. ‚Äú!‚Äù:3,‚Äú""‚Äù:4,‚Äú#‚Äù:5, Hello, how long time took your tokenizer for the training? Thank you","Sep 13, 2022 9:56 am","mdelas","Sorry for this late answer, Actually, it is going very fast but for 1.4Mb just a few seconds‚Ä¶","Oct 1, 2022 9:20 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Right choice of padding side for Mistral","https://discuss.huggingface.co/t/right-choice-of-padding-side-for-mistral/68401","0","1.9k","Created: Jan 8, 2024 5:06 pm","pmanoj","Mistral with flash attention 2 and right padding ¬∑ Issue #26877 ¬∑ huggingface/transformers (github.com) From the above discussion, I understand that - During model.generate the padding side should be left During model.forward, the padding side could either be left or right. Both are usually supported with HF. My question though is, if model.generate is going to give ‚Äúnot so good‚Äù results using right padding, isn‚Äôt model forward going to affect w/ the same problem?","Jan 8, 2024 5:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Skew between mistral prompt in docs vs. chat template","https://discuss.huggingface.co/t/skew-between-mistral-prompt-in-docs-vs-chat-template/66674","2","1.1k","Created: Dec 22, 2023 12:35 am Latest: Dec 27, 2023 5:42 pm","hamel","See this For mistral, the chat template will apply a space between <s> and [INST], whereas the documentation doesn‚Äôt have this. See these docs vs this code: from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")  chat = [{""role"": ""user"", ""content"": ""abc""},          {""role"": ""assistant"", ""content"": ""ipsum""},          {""role"": ""user"", ""content"": ""123""},          {""role"": ""assistant"", ""content"": ""sit""}]  ids = tokenizer.apply_chat_template(chat) print(tokenizer.decode(ids)) Which will result in this: <s> [INST] abc [/INST]ipsum</s> [INST] 123 [/INST]sit</s>","Dec 22, 2023 12:35 am","osanseviero","This is not related to the chat template, but how decoding works in this tokenizer and start of words. Let‚Äôs go step by step. Your example was from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""mistralai/Mistral-7B-Instruct-v0.2"")  chat = [{""role"": ""user"", ""content"": ""abc""},          {""role"": ""assistant"", ""content"": ""ipsum""},          {""role"": ""user"", ""content"": ""123""},          {""role"": ""assistant"", ""content"": ""sit""}]  ids = tokenizer.apply_chat_template(chat) print(tokenizer.decode(ids)) which returns <s> [INST] abc [/INST]ipsum</s> [INST] 123 [/INST]sit</s> Now, let‚Äôs not use chat templates and pass the raw string to the tokenizer (we don‚Äôt add beginning of sentence token as that‚Äôs added automatically - tokenizer.add_bos_token is True) ids = tokenizer(""[INST] abc [/INST]ipsum</s> [INST] 123 [/INST]sit</s>"")[""input_ids""] print(tokenizer.decode(ids)) We get, once again, the same string with extra space <s> [INST] abc [/INST]ipsum</s>  [INST] 123 [/INST]sit</s> So what‚Äôs going on? Within decode, we call convert_ids_to_tokens, so let‚Äôs see what‚Äôs going on there tokenizer.convert_ids_to_tokens(ids) will give ['<s>',  '‚ñÅ[',  'INST',  ']',  '‚ñÅabc',  '‚ñÅ[',  '/',  'INST',  ']',  'ip',  'sum',  '</s>',  '‚ñÅ[',  'INST',  ']',  '‚ñÅ', ... So that‚Äôs where the extra space is coming from. Ok, let‚Äôs dive deeper! What does convert_ids_to_tokens do? This is the exact implementation (minus docstrings/typing) def convert_ids_to_tokens(         self, ids, skip_special_tokens: False)          tokens = []         for index in ids:             index = int(index)             if skip_special_tokens and index in self.all_special_ids:                 continue             tokens.append(self._tokenizer.id_to_token(index))         return tokens So let‚Äôs replicate it for index in ids:   print(index, tokenizer._tokenizer.id_to_token(index)) Will yield same as above 1 <s> 733 ‚ñÅ[ 16289 INST 28793 ] 18641 ‚ñÅabc 733 ‚ñÅ[ 28748 / 16289 INST 28793 ] 508 ip 1801 sum 2 </s> 28705 ‚ñÅ 733 ‚ñÅ[ ... So the [ token is encoded to ID 733, which is decoded as _[. This actually makes sense. You can go to the tokenizer vocabulary and will find that the ID 733 indeed is _[ https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2/raw/main/tokenizer.json So finally, the main question is why there‚Äôs an _. The _ is usually added for start of the word.","Dec 27, 2023 12:00 pm","hamel","Thanks @osanseviero for being helpful as usual. I think my confusion comes from trying to decode the chat template which adds the space and compare that to the documentation which doesn‚Äôt have a space. It turns out that encoding the documented example without the space results in the same token ids as encoding the chat template. However, you should NOT try to take the decoded chat template and use that as your prompt template! I think this is very confusing and many people are making mistakes here. I think the main principle is you CANNOT do a round trip from a decoded example back to an encoded one.","Dec 27, 2023 3:39 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with Flaubert Tokenizer as word_ids() method is not available for NER Task","https://discuss.huggingface.co/t/issue-with-flaubert-tokenizer-as-word-ids-method-is-not-available-for-ner-task/20374","1","1.3k","Created: Jul 14, 2022 11:30 am Latest: Aug 15, 2022 11:33 am","pragmatic-coder","I am working with Flaubert for Token Classification Task but when I am trying to compensate for difference in an actual number of labels and now a larger number of tokens after tokenization takes place; it‚Äôs showing an error that word_ids() method is not available. The method is available as I did dir(tokenized_input) and it is showing in the available list of methods but when I try to use it‚Ä¶ Error: word_ids() is not available when using python-based tokenizer. For reference; Tokenizer - use of word_ids to map labels to newer tokens. I am using Flaubert for Named Entity Recognition Task! @lewtun","Jul 14, 2022 11:30 am","v-moayman","You can check this issue: DeBERTa V3 Fast Tokenizer ¬∑ Issue #14712 ¬∑ huggingface/transformers (github.com). I believe that it is a solution to your issue.","Aug 15, 2022 11:33 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Save custom components","https://discuss.huggingface.co/t/save-custom-components/6458","0","330","Created: May 29, 2021 9:45 pm Latest: May 29, 2021 9:47 pm","mozharovsky","Hi, I use a custom normalizer with my Tokenizer, and it seems there is no way to save the tokenizer with custom components. I don‚Äôt see another way than using a custom PreTrainedTokenizerFast which defines custom components on instance initialization. That is not convenient since I have to share this component every time I need to use it. Having a universal serialized tokenizer would be much easier to handle tokenization tasks. Are there any existing workarounds to save custom components or at least plans to support custom components serialization? Best, Eugene","May 29, 2021 9:45 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Passing list of inputs to tokenize","https://discuss.huggingface.co/t/passing-list-of-inputs-to-tokenize/17499","1","1.3k","Created: May 4, 2022 4:52 am Latest: May 9, 2022 4:28 pm","pooplepeople","In this tutorial (Processing the data - Hugging Face Course), they pass a collection of inputs into Tokenizer.tokenize(). But when I try to pass a list of texts, I get an error, and the source code (transformers/tokenization_utils.py at main ¬∑ huggingface/transformers ¬∑ GitHub) suggests it only accepts one input. So how do I tokenize lots of items at once as they seem to do in the tutorial?","May 4, 2022 4:52 am","courtneysprouse131","Can you post your code?","May 9, 2022 4:28 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Wav2vec2CTCTokenizer and vocab.json","https://discuss.huggingface.co/t/wav2vec2ctctokenizer-and-vocab-json/25146","2","1.0k","Created: Oct 28, 2022 3:45 pm Latest: Oct 29, 2022 7:29 pm","picheny","I am using Wav2Vec2CTCTokenizer.from_pretrained to read in the Facebook base librispeech model: tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(‚Äòfacebook/wav2vec2-base-960h‚Äô) I am seeing some behavior I am not sure I follow. It seems that if I have a vocab.json file already in the same directory from where I am running the above command, it ignores the vocab.json file in the base model and uses the one in my directory. Is this correct, and if so, where is this happening in the source code - I cannot find it.","Oct 28, 2022 3:45 pm","lianghsun","github.com huggingface/transformers/blob/main/src/transformers/tokenization_utils_base.py#L1570                `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the     vocab.                Returns:         `Dict[str, int]`: The vocabulary.     """"""     raise NotImplementedError()            @classmethod def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], *init_inputs, **kwargs):     r""""""     Instantiate a [`~tokenization_utils_base.PreTrainedTokenizerBase`] (or a derived class) from a predefined     tokenizer.                Args:         pretrained_model_name_or_path (`str` or `os.PathLike`):             Can be either:                        - A string, the *model id* of a predefined tokenizer hosted inside a model repo on huggingface.co.               Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a","Oct 28, 2022 11:00 pm","picheny","I am sorry, could you elaborate more? I still don‚Äôt see why it does not default to the base model vocab file - where/how does it wind up taking something from my directory.","Oct 29, 2022 7:29 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using customized algorithm","https://discuss.huggingface.co/t/using-customized-algorithm/21753","0","320","Created: Aug 17, 2022 4:38 pm","ockapuh","Hi everyone. Is it possible to use a customized algorithm for tokenizing and building the vocab during Language Modeling? Thanks!","Aug 17, 2022 4:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Training tokenizer takes too much RAM","https://discuss.huggingface.co/t/training-tokenizer-takes-too-much-ram/14256","1","1.3k","Created: Jan 31, 2022 10:09 am Latest: Feb 21, 2022 10:13 am","Klin556","Hello, I wanted to train my own tokenizer on multi-lingual corpus (115GB of oscar and mc4 data in 15 languages) . My machine has only 16GB RAM so I wrote a generator for this task. The problem is it still uses all my RAM. It progressively adds up from using 5GB to 16GB in maybe like 3 hours and then kernel dies. What can be the problem? Thanks in advance. My code: tokenizer = Tokenizer(BPE(unk_token=""[UNK]"")) tokenizer.normalizer = normalizers.Sequence([Lowercase(), NFKC(), StripAccents()]) tokenizer.pre_tokenizer = pre_tokenizers.Sequence([Metaspace(), ByteLevel()])  def text_generator():     files = [ str(x) for x in Path(""data/text/files/"").glob(""*.txt"") ]     for file in tqdm(files): # one file is 10_000 sentences         with open(file, ""r"") as f:             lines = f.read().split('\n')         yield lines # yields list of 10_000 strings   trainer = BpeTrainer(     vocab_size=200261, special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""] )  tokenizer.train_from_iterator(text_generator(), trainer=trainer)","Jan 31, 2022 10:09 am","ahmed2m","Same problem! Isn‚Äôt train_from_iterator supposed to be loading the data in batches and never have the complete data on memory at one time? Or is it the internal pair and vocab hashmaps getting bigger and bigger? Does anyone know the problem?","Feb 21, 2022 10:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Text preprocessing for fitting Tokenizer model","https://discuss.huggingface.co/t/text-preprocessing-for-fitting-tokenizer-model/24869","1","1.2k","Created: Oct 24, 2022 7:15 am Latest: Oct 25, 2022 7:47 pm","antoine2323231","Hello I have read that when preprocessing text it is best practice to remove stop words, remove special characters and punctuation, to end up only with list of words. My question is: If the original text I want my tokenizer to be fitted on is a text containing a lot of statistics (hence a lot of % = / etc‚Ä¶) but also some texts, then it makes sense to keep special characters and numbers as input to the tokenizer model? Or it should be removed in any case as a tokenizer can only understand words? Thanks a lot in advance","Oct 24, 2022 7:15 am","lianghsun","I think this question may depends on what kind of task you might want to deal with. In briefly, as my opinion, I think you should keep those symbol in your vocab, because those symbol needs to be the element in your context, no matter what kind of task you might to deal with.","Oct 25, 2022 7:47 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Word_ids not working with deberta_v2","https://discuss.huggingface.co/t/word-ids-not-working-with-deberta-v2/21523","1","1.2k","Created: Aug 11, 2022 12:43 pm Latest: Aug 12, 2022 12:27 pm","v-moayman","Hello all, Currently, I am working on a token classification. When I have tried to use word_ids function during tokenization, it gave me an error. Let me elaborate with the following example: #train is a dict having the tokens and labels import transformers from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(""microsoft/deberta-v3-small"", use_fast=True) tokenized_input = tokenizer(train['tokens'][0], is_split_into_words=True) Now, the problem is I want to use word_ids() function. Why? ‚ÄúSpecial tokens added by the tokenizer are mapped to None and other tokens are mapped to the index of their corresponding word (several tokens will be mapped to the same word index if they are parts of that word)‚Äù. When I call this function, the following error raises: ""word_ids() is not available when using Python-based tokenizers"" While using distillbert, it is working fine. Your help is appreciated!","Aug 11, 2022 12:43 pm","v-moayman","Check this issue: DeBERTa V3 Fast Tokenizer ¬∑ Issue #14712 ¬∑ huggingface/transformers (github.com)","Aug 12, 2022 12:27 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"There is no 0.11.0 tokenizers in pip","https://discuss.huggingface.co/t/there-is-no-0-11-0-tokenizers-in-pip/10381","4","784","Created: Sep 29, 2021 8:50 am Latest: Sep 30, 2021 5:03 am","xiami","I can only find 0.10.3 through pip install. Am I doing something wrong or I can only install 0.11.0 through source? Thank you!","Sep 29, 2021 8:50 am","sgugger","There is no version 0.11.0 of Tokenizers that has been released.","Sep 29, 2021 1:17 pm","BramVanroy","On PyPi there is no such release. There such a release on Github, however. @xiami If for some reason you need this version specifically you can get it from the GH release.","Sep 29, 2021 1:45 pm","anthony","Indeed, the release process failed and we weren‚Äôt able to fix it yet.","Sep 29, 2021 3:32 pm","xiami","Thank you! I will try to install it through source.","Sep 30, 2021 5:03 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer post_processor help","https://discuss.huggingface.co/t/tokenizer-post-processor-help/21926","1","1.2k","Created: Aug 22, 2022 4:17 pm Latest: Oct 27, 2022 7:48 am","smh36","I‚Äôm struggling to get the post_processor to work for tokenisation. Can anyone point me in the right direction? Here‚Äôs the minimal example from transformers import AutoTokenizer from tokenizers.processors import TemplateProcessing  tokenizer = AutoTokenizer.from_pretrained( 'bert-base-uncased' )  tokenizer.post_processor = TemplateProcessing(     single=""[CLS] [CLS] $0 [SEP] [CLS]"",     pair=""[CLS] $A [SEP] $B:1 [SEP]:1"",     special_tokens=[         (""[CLS]"", tokenizer.cls_token_id),          (""[SEP]"", tokenizer.sep_token_id)     ], )  text_string = ""The cat sat on the mat.""  tokens = tokenizer( text_string )  print( tokenizer.decode( tokens.token_ids ) ) # OUTPUT: [CLS] the cat sat on the mat. [SEP] However, I would expect to see extra [CLS] flags at the beginning and the end. Where am I going wrong?","Aug 22, 2022 4:17 pm","lianghsun","Hi @smh36, I think a lot of people confuse HF Transformers Tokenizer API with HF Tokenizers (so am I in the first time ). HF Tokenizers train new vocabularies and tokenizer, and you may design customized tokenization flow with Normalization, Pre-tokenization, Model, Post-tokenization, and ‚Ä¶etc. In contrast, HF Transformers Tokenizer API loads pre-trained tokenizer from hub or local files. So it‚Äôs clearly if you would want to re-design the pre-trained tokenizer, you should use HF Tokenizers. The following code should help you from tokenizers import Tokenizer from tokenizers.processors import TemplateProcessing  tokenizer = Tokenizer.from_pretrained(""bert-base-uncased"") # use HF Tokenizers instead tokenizer.post_processor = TemplateProcessing(     single=""[CLS] $A [CLS] [SEP]"",     special_tokens=[         (""[CLS]"", tokenizer.token_to_id(""[CLS]"")),          (""[SEP]"", tokenizer.token_to_id(""[SEP]"")),     ], )  tokens = tokenizer.encode('Hi there').tokens print(tokens)  # > ['[CLS]', 'hi', 'there', '[CLS]', '[SEP]']   from transformers import PreTrainedTokenizerFast  _tokenizer = PreTrainedTokenizerFast(     tokenizer_object=tokenizer,     # Load speacil tokens manually     # https://huggingface.co/course/chapter6/8?fw=pt     unk_token = '[UNK]',     sep_token = '[SEP]',     pad_token = '[PAD]',     cls_token = '[CLS]',     mask_token = '[MASK]',     model_max_length=128, # same as the block size of the model )  print(_tokenizer('Hi there').input_ids) # [101, 7632, 2045, 101, 102]","Oct 27, 2022 7:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ViTImageProcessor output visualization","https://discuss.huggingface.co/t/vitimageprocessor-output-visualization/76335","8","580","Created: Mar 7, 2024 6:59 am Latest: Apr 18, 2024 10:15 pm","Sushmitaupadhyay","when I am printing the output of ViTImageprocessor using Matplotlib I can see the same image is reduces in size and repeated 6 times in a frame of 224x224 with different contrast and intensity. If I have understood correctly, ViT divides the image into patches. So the entire image should have been divided into small patches of size 16x16. But the result is not like that. What is the issue here?","Mar 7, 2024 6:59 am","Sandy1857","ViT takes in an input of resolution 224x224. The ViTImageProcessor just handles the resizing and normalisation of the image to that of the correct resolution. The patches of 16x16 that you mentioned are taken over this processed image which the Vit model then consumes. And additionally, if a an RGB image is what you gave to the ViTImageProcessor, an RGB image is what you must get with just sizes of 224x224 and 3 channels. Don‚Äôt know how you got 6 channel image.","Mar 8, 2024 7:22 pm","Sushmitaupadhyay","This is my input image","Mar 12, 2024 5:58 am","Sushmitaupadhyay","This is my output after passing it through the ViTImageProcessor  I am visualising the processed image using matplotlib. Is that the issue?","Mar 12, 2024 6:00 am","Sandy1857","I don‚Äôt know what code you ran, but could you just post the output pixel_values after passing the image through ViTImageProcessor, removing/unsqeezing it‚Äôs batch dimension, and then get the image using PIL Image.fromarray function?","Mar 12, 2024 4:57 pm","nielsr","Hi, Here‚Äôs how you can visualize the output of ViTImageProcessor: from transformers import ViTImageProcessor import requests from PIL import Image  image_processor = ViTImageProcessor()  url = 'http://images.cocodataset.org/val2017/000000039769.jpg' image = Image.open(requests.get(url, stream=True).raw)  pixel_values = image_processor(image, return_tensors=""pt"").pixel_values  # denormalize the pixel values for visualization purposes mean = image_processor.image_mean std = image_processor.image_std  unnormalized_image = (pixel_values[0].numpy() * np.array(std)[:, None, None]) + np.array(mean)[:, None, None] unnormalized_image = (unnormalized_image * 255).astype(np.uint8) unnormalized_image = np.moveaxis(unnormalized_image, 0, -1) unnormalized_image = Image.fromarray(unnormalized_image) which gives me this: This is a 224x224 image.","Mar 12, 2024 7:48 pm","Sushmitaupadhyay","Sandy1857: unsqeezing This is how I have run the code","Mar 20, 2024 5:15 am","Sushmitaupadhyay","This output you get after un normalizing. My question is after passing through the ViTImageProcessor why the image gets small and gets arranged in patches. You can refer the output which I have pasted.","Mar 20, 2024 5:17 am","raygx","I also came across this very question. And finally I figured out what was happening. What is ViTImageProcessor doing? - #4 by raygx. Check the last reply that I gave. You‚Äôll know how to reconstruct the image. @everyone","Apr 18, 2024 10:15 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does the ByteLevelBPETokenizer need to be wrapped in a normal Tokenizer?","https://discuss.huggingface.co/t/does-the-bytelevelbpetokenizer-need-to-be-wrapped-in-a-normal-tokenizer/34121","0","1.7k","Created: Mar 18, 2023 7:08 pm","vedantroy","The Huggingface tokenizer documents say to use the following: from tokenizers import Tokenizer from tokenizers.models import BPE tokenizer = Tokenizer(BPE(unk_token=""[UNK]"")) However, it looks like the correct way to train a byte-level BPE is as follows: tokenizer = ByteLevelBPETokenizer() tokenizer.train([""path/to/train.txt""], vocab_size=1000, min_frequency=2, special_tokens=[     ""<s>"",     ""<pad>"",     ""</s>"",     ""<unk>"",     ""<mask>"", ]) Why is the ByteLevelBPETokenizer not just a normal tokenizer model?","Mar 18, 2023 7:08 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to decode with spaces?","https://discuss.huggingface.co/t/how-to-decode-with-spaces/17289","0","1.7k","Created: Apr 28, 2022 1:39 pm","timbmg","How can I decode token by token, i.e. without the tokenizer removing spaces for punctuation? In the example below, i would expect [CLS] hello world . [SEP], i.e. a space between world and .. tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") x = tokenizer.encode(""Hello World."") tokenizer.decode(x, clean_up_tokenization_spaces=False) # '[CLS] hello world. [SEP]'","Apr 28, 2022 1:39 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using `TFBertTokenizer` instead of `BertTokenizer` with `TFBertForQuestionAnswering`","https://discuss.huggingface.co/t/using-tfberttokenizer-instead-of-berttokenizer-with-tfbertforquestionanswering/26127","1","1.2k","Created: Nov 15, 2022 8:05 pm Latest: Nov 15, 2022 10:50 pm","SamAgarwal0","I am trying to use TFBertTokenizer instead of BertTokenizer with TFBertForQuestionAnswering, however, when I tokenize a text pair using TFBertTokenizer I get: >>> from transformers import TFBertTokenizer, TFBertForQuestionAnswering >>> import tensorflow as tf >>> tf_tokenizer = TFBertTokenizer.from_pretrained('bert-base-uncased') >>> tf_model = TFBertForQuestionAnswering.from_pretrained('bert-base-uncased') All model checkpoint layers were used when initializing TFBertForQuestionAnswering.  Some layers of TFBertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. >>> tf_inputs = tf_tokenizer(['Who is Jim Henson?', 'Jim Henson is a puppet master']) >>> print(tf_inputs) {'input_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy= array([[  101,  2040,  2003,  3958, 27227,  1029,   102,     0],        [  101,  3958, 27227,  2003,  1037, 13997,  3040,   102]],       dtype=int64)>, 'attention_mask': <tf.Tensor: shape=(2, 8), dtype=int64, numpy= array([[1, 1, 1, 1, 1, 1, 1, 0],        [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)>, 'token_type_ids': <tf.Tensor: shape=(2, 8), dtype=int64, numpy= array([[0, 0, 0, 0, 0, 0, 0, 0],        [0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)>} VS >>> from transformers import BertTokenizer, TFBertForQuestionAnswering >>> import tensorflow as tf  >>> tokenizer = BertTokenizer.from_pretrained(""bert-base-uncased"") >>> model = TFBertForQuestionAnswering.from_pretrained(""bert-base-uncased"")  >>> inputs = tokenizer(""Who was Jim Henson?"",  ""Jim Henson was a nice puppet"", return_tensors=""tf"") >>> print(inputs) {'input_ids': <tf.Tensor: shape=(1, 14), dtype=int32, numpy= array([[  101,  2040,  2001,  3958, 27227,  1029,   102,  3958, 27227,          2001,  1037,  3835, 13997,   102]])>, 'token_type_ids': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]])>, 'attention_mask': <tf.Tensor: shape=(1, 14), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])>} Basically, I get a (2,8) shape tensor from TFBertTokenizer vs a (1, 14) shape tensor with BertTokenizer. How can I get a (1, 14) shape tensor from TFBertTokenizer?","Nov 15, 2022 8:05 pm","sgugger","I‚Äôm not sure TFBertTokenizer supports sentence pairs yet, cc @Rocketknight1","Nov 15, 2022 10:50 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long","https://discuss.huggingface.co/t/pyo3-runtime-panicexception-likelihood-is-nan-input-sentence-may-be-too-long/18398","1","1.2k","Created: May 26, 2022 12:34 pm Latest: May 27, 2022 1:33 pm","carted-ml","We‚Äôre training a unigram tokenizer from scratch. This is how it‚Äôs instantiated: tokenizer = Tokenizer(Unigram()) tokenizer.normalizer = normalizers.Sequence(  [normalizers.Replace(""``"", '""'), normalizers.Replace(""''"", '""')] ) tokenizer.pre_tokenizer = pre_tokenizers.Metaspace() This is how the trainer is initialized: trainer = UnigramTrainer(         unk_token=""<unk>"",         special_tokens=[""[CLS]"", ""[SEP]"", ""<unk>"", ""<pad>"", ""[MASK]""],         vocab_size=10000,     ) We‚Äôre training on a batch by batch basis: tokenizer.train_from_iterator(dataloader, trainer=trainer) We‚Äôre running into the following toward the end: huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:52:53] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 50158000 /        0 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:52:57] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 50164000 /        0 huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either:         - Avoid using `tokenizers` before the fork if possible [04:53:25] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0 [00:01:37] Suffix array seeds                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18201510 / 18201510 [00:00:00] EM training                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 18201510 /       32 thread '<unnamed>' panicked at 'likelihood is NAN. Input sentence may be too long.', /__w/tokenizers/tokenizers/tokenizers/src/models/unigram/trainer.rs:413:17 Traceback (most recent call last):   File ""train_unigram_tokenizer.py"", line 106, in <module>     main(**args)   File ""train_unigram_tokenizer.py"", line 92, in main     tokenizer.train_from_iterator(datagen, trainer=trainer) pyo3_runtime.PanicException: likelihood is NAN. Input sentence may be too long. Here are the version info: transformers: 4.17.0 tokenizers: 0.11.6 Since we‚Äôre using a private corpus I‚Äôm afraid I can not provide a notebook for the reproduction of the error.","May 26, 2022 12:34 pm","nbroad","I‚Äôm not too familiar with tokenizers, but did you do a check for really long sentences? The error you received makes it seem like inputting shorter sentences may solve the issue.","May 27, 2022 1:33 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Masking Probability","https://discuss.huggingface.co/t/masking-probability/746","4","741","Created: Aug 18, 2020 9:19 am Latest: Aug 20, 2020 3:28 pm","abdallah197","Hi I am wondering whether the masking of tokens [MASK] is done by applying the masking probability to a given sequence or the whole batch altogether.","Aug 18, 2020 9:19 am","sgugger","In DataCollatorForLanguageModeling the masking is done on the tensor directly.","Aug 18, 2020 12:21 pm","RichardWang","If I read the code correctly, it is every ‚Äútoken‚Äù has masking probability to be masked (makes/replaced/not changed). Independent to how many tokens you have i.e. the size of tensor.","Aug 18, 2020 12:49 pm","abdallah197","In the paper, It‚Äôs mentioned that the masing is done on 0.15 f all WordPiece tokens in each sequence at random. In the code though, it‚Äôs based on the inputs in labels = inputs.clone() probability_matrix = torch.full(labels.shape, self.mlm_probability) masked_indices = torch.bernoulli(probability_matrix).bool() I don‚Äôt know whether inputs refers to one sequence or one batch of sequences","Aug 19, 2020 2:44 pm","valhalla","inputs is batch and probability_matrix creates prob for each sequence.","Aug 20, 2020 3:28 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Keeping special chars in translations","https://discuss.huggingface.co/t/keeping-special-chars-in-translations/58270","0","291","Created: Oct 12, 2023 8:52 am","sancelot","Hi, Translating applications texts, how can I keep special characters like \t ?: from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline  # load model  tokenizer = AutoTokenizer.from_pretrained(""facebook/nllb-200-distilled-600M"")  model = AutoModelForSeq2SeqLM.from_pretrained(""facebook/nllb-200-distilled-600M"")  source=""fra_Latn""  target=""eng_Latn""  text = ""\tCacher / Montrer ligne""  translator = pipeline('translation', model=model, tokenizer=tokenizer, src_lang=source, tgt_lang=target)  output = translator(text, max_length=400)  print(output)","Oct 12, 2023 8:52 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Load custom pretrained tokenizer","https://discuss.huggingface.co/t/load-custom-pretrained-tokenizer/11148","0","1.6k","Created: Oct 28, 2021 1:36 am Latest: Oct 28, 2021 1:43 am","jbmaxwell","I‚Äôm trying to run BigBird on my dataset but I‚Äôm hitting an error trying to load my custom/saved tokenizer. I train the tokenizer using: from tokenizers import Tokenizer from tokenizers.models import BPE from tokenizers.pre_tokenizers import Whitespace  tokenizer = Tokenizer(BPE()) tokenizer.pre_tokenizer = Whitespace() paths = ['./content/test.txt', './content/train.txt'] from tokenizers.trainers import BpeTrainer  trainer = BpeTrainer(special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""]) tokenizer.train(files=paths, trainer=trainer)  # Save files to disk tokenizer.model.save(""./tokenizer/bigbird_full"") tokenizer.save(""./tokenizer/bigbird_full/config.json"") Then I try to run run_mlm.py with: os.system(     f""python run_mlm.py \        --model_type 'big_bird' \        --config_name './models/bigbird_full/config.json' \        --tokenizer_name './tokenizer/bigbird_full' \        --train_file './content/train.txt' \        --output_dir './{out_dir}' \        --do_train \        --num_train_epochs 1 \        --overwrite_output_dir"" ) It fails with the error: OSError: Can't load tokenizer for './tokenizer/bigbird_full'. Make sure that:  - './tokenizer/bigbird_full' is a correct model identifier listed on 'https://huggingface.co/models'   (make sure './tokenizer/bigbird_full' is not a path to a local directory with something else, in that case)  - or './tokenizer/bigbird_full' is the correct path to a directory containing relevant tokenizer files  - or 'main' is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on 'https://huggingface.co/models' My tokenizer‚Äôs directory contains: config.json, merges.txt, vocab.json What is the error trying to tell me? Or rather, what are ‚Äúrelevant tokenizer files‚Äù if not these? PS ‚Äì It‚Äôs worth noting that, for some reason, I have to manually add ""model_type"":""big_bird"" to my saved config.json, otherwise I get an error telling me basically exactly that‚Ä¶ (i.e., it needs that key/val).","Oct 28, 2021 1:36 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TypeError when loading tokenizer with from_pretrained method for bart-large-mnli model","https://discuss.huggingface.co/t/typeerror-when-loading-tokenizer-with-from-pretrained-method-for-bart-large-mnli-model/3378","1","1.1k","Created: Jan 25, 2021 8:06 pm Latest: Jul 8, 2021 1:01 pm","StarDriver","Hello everyone. Here is my problem, (I wish someone can help me, I try so hard in vain to resolve it T.T) : I use transformers 4.2.1 lib, and I am in a context where I only can use it in offline mode (no internet). I want to use the bart-large-mnli model so I upload it on a specific server and I download the model with the following link : huggingface.co facebook/bart-large-mnli at main We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science. Then I try to use from_pretrained method like this : tokenizer = BartTokenizerFast.from_pretrained(‚Äò/appli/pretrainedModel/bart-large-mnli‚Äô) or like this : tokenizer = AutoTokenizer.from_pretrained(‚Äò/appli/pretrainedModel/bart-large-mnli‚Äô) But every time I do this I get the following error (more detailed log at the end of my post, I truncate the last line with [‚Ä¶] to replace all the content of merges.txt) : ‚ÄúTypeError: Can‚Äôt convert [(‚Äò√Ñ‚Äô, ‚Äòt‚Äô), (‚Äò√Ñ‚Äô, ‚Äòa‚Äô), (‚Äòh‚Äô, ‚Äòe‚Äô), [‚Ä¶] ] (list) to Union[Merges, Filename]‚Äù I definitively don‚Äôt know what‚Äôs going wrong with merges.txt but it seems like there is a problem‚Ä¶ The content of /appli/pretrainedModel/bart-large-mnli is : config.json merges.txt pytorch_model.bin rust_model.ot tokenizer_config.json vocab.json Does someone have any idea where is the problem ? Thanks in advance. More detailed error log : --------------------------------------------------------------------------- TypeError Traceback (most recent call last) in ----> 1 tokenizer = AutoTokenizer.from_pretrained(‚Äò/appli/pretrainedModel/bart-large-mnli‚Äô) 2 pipeline(‚Äòzero-shot-classification‚Äô, model=model, tokenizer=tokenizer) /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py in from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs) 383 tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[type(config)] 384 if tokenizer_class_fast and (use_fast or tokenizer_class_py is None): ‚Üí 385 return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs) 386 else: 387 if tokenizer_class_py is not None: /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs) 1767 1768 return cls._from_pretrained( ‚Üí 1769 resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs 1770 ) 1771 /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/tokenization_utils_base.py in _from_pretrained(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs) 1839 # Instantiate tokenizer. 1840 try: ‚Üí 1841 tokenizer = cls(*init_inputs, **init_kwargs) 1842 except OSError: 1843 raise OSError( /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/models/roberta/tokenization_roberta_fast.py in init(self, vocab_file, merges_file, tokenizer_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs) 171 mask_token=mask_token, 172 add_prefix_space=add_prefix_space, ‚Üí 173 **kwargs, 174 ) 175 /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py in init(self, vocab_file, merges_file, tokenizer_file, unk_token, bos_token, eos_token, add_prefix_space, **kwargs) 139 eos_token=eos_token, 140 add_prefix_space=add_prefix_space, ‚Üí 141 **kwargs, 142 ) 143 /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py in init(self, *args, **kwargs) 87 elif slow_tokenizer is not None: 88 # We need to convert a slow tokenizer to build the backend ‚Äî> 89 fast_tokenizer = convert_slow_tokenizer(slow_tokenizer) 90 elif self.slow_tokenizer_class is not None: 91 # We need to create and convert a slow tokenizer to build the backend /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py in convert_slow_tokenizer(transformer_tokenizer) 657 converter_class = SLOW_TO_FAST_CONVERTERS[tokenizer_class_name] 658 ‚Üí 659 return converter_class(transformer_tokenizer).converted() /appli/.conda/envs/bf_verbatim/lib/python3.7/site-packages/transformers/convert_slow_tokenizer.py in converted(self) 281 continuing_subword_prefix=‚Äú‚Äù, 282 end_of_word_suffix=‚Äú‚Äù, ‚Üí 283 fuse_unk=False, 284 ) 285 ) TypeError: Can‚Äôt convert [(‚Äò√Ñ‚Äô, ‚Äòt‚Äô), (‚Äò√Ñ‚Äô, ‚Äòa‚Äô), (‚Äòh‚Äô, ‚Äòe‚Äô), [‚Ä¶] ] (list) to Union[Merges, Filename]","Jan 25, 2021 8:06 pm","OptimusPrime","Hi, Were you able to resolve the issue? Thanks in advance","Jul 8, 2021 1:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with post-processing","https://discuss.huggingface.co/t/issue-with-post-processing/2703","1","1.1k","Created: Dec 8, 2020 4:49 pm Latest: Jun 15, 2022 8:09 am","YBS","I trained a new BertWordPieceTokenizer from scratch, using the same code from the example given in the docs. Then, I created a new TemplateProcessing object & assigned it as the tokenizer‚Äôs PostProcessor in order to add [CLS] and [SEP] tokens (also using the example code). However, when I encode sentences with the tokenizer, it doesn‚Äôt preform any post-processing. Code: from tokenizers import BertWordPieceTokenizer from tokenizers.processors import TemplateProcessing corpus = ‚Äú./corpus.txt‚Äù tokenizer = BertWordPieceTokenizer( clean_text=True, handle_chinese_chars=False, strip_accents=True, lowercase=True, ) tokenizer.train( corpus, vocab_size=32000, min_frequency=2, show_progress=True, special_tokens=[‚Äú[UNK]‚Äù, ‚Äú[CLS]‚Äù, ‚Äú[SEP]‚Äù], limit_alphabet=1000, wordpieces_prefix=‚Äú##‚Äù, ) tokenizer.post_processor = TemplateProcessing( single=‚Äú[CLS] $A [SEP]‚Äù, pair=‚Äú[CLS] $A [SEP] $B:1 [SEP]:1‚Äù, special_tokens=[ (‚Äú[CLS]‚Äù, tokenizer.token_to_id(‚Äú[CLS]‚Äù)), (‚Äú[SEP]‚Äù, tokenizer.token_to_id(‚Äú[SEP]‚Äù)), ], ) output = tokenizer.encode(‚ÄúHello, y‚Äôall! How are you ?‚Äù) print(output.tokens) Output: [‚Äòhello‚Äô, ‚Äò,‚Äô, ‚Äòy‚Äô, ‚Äú'‚Äù, ‚Äòall‚Äô, ‚Äò!‚Äô, ‚Äòhow‚Äô, ‚Äòare‚Äô, ‚Äòyou‚Äô, ‚Äò[UNK]‚Äô, ‚Äò?‚Äô]","Dec 8, 2020 4:49 pm","rbawden","Hi there! Did you end up finding a solution to the problem?","Jun 15, 2022 8:09 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer.encode not returning encodings","https://discuss.huggingface.co/t/tokenizer-encode-not-returning-encodings/10616","2","890","Created: Oct 8, 2021 1:25 pm Latest: Oct 9, 2021 2:31 am","astarostap","How do you get token encodings? The method isn‚Äôt working in this case. I need the token offsets in order to translate my labels to a normal list of token tags (my labels are in this format [{start_index: int, end_index: int, tag: str} ‚Ä¶ ] Thank you! Maybe it‚Äôs because this is a pre-trained fast tokenizer?","Oct 8, 2021 1:25 pm","sgugger","You need to use the tokenizer directly on your text, not the encode method: tokenizer(""hello world"")","Oct 8, 2021 2:03 pm","astarostap","Thank you Sylvain! That worked, I also had to pass return_offset_mapping=True","Oct 9, 2021 2:03 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Where to find the ‚Äúwiki-big.train.raw‚Äù data as mentioned in the snippet for tokenizers 0.9?","https://discuss.huggingface.co/t/where-to-find-the-wiki-big-train-raw-data-as-mentioned-in-the-snippet-for-tokenizers-0-9/1796","2","885","Created: Oct 29, 2020 9:49 am Latest: Oct 29, 2020 10:04 am","sugatoray","I came across this short snippet of code on LinkedIn by HuggingFace, introducing tokenizers 0.9. LinkedIn URL: snippet for tokenizers 0.9 How do I get the following dataset to run the code snippet? Is it available on huggingface.datasets? files = [""../../data/wiki-big.train.raw""]","Oct 29, 2020 9:49 am","BramVanroy","This dataset can probably get you started. This gist by @thomwolf may also prove useful.","Oct 29, 2020 9:53 am","sugatoray","Thank you.","Oct 29, 2020 10:04 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Should have a `model_type` key in its config.json","https://discuss.huggingface.co/t/should-have-a-model-type-key-in-its-config-json/10144","0","1.5k","Created: Sep 20, 2021 4:07 pm","SpamMe","I have trained a tokenizer from scratch, using: tokenizer.train(files=[pth], vocab_size=52_000, min_frequency=2, special_tokens=[     ""<s>"",     ""<pad>"",     ""</s>"",     ""<unk>"",     ""<mask>"", ]) I save the tokenizer, I use it to train a BERT model from scratch, and later I want to test this model using: unmasker = pipeline(‚Äòfill-mask‚Äô, model=model, tokenizer=tokenizer) But it complains that the tokenizer is unrecogized: ‚Äú[‚Ä¶] Should have a model_type key in its config.json‚Äù How can I save the tokenizer so that there is a model_type indicated in config.json?","Sep 20, 2021 4:07 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Finetuning GPT-J6B for custom dataset","https://discuss.huggingface.co/t/finetuning-gpt-j6b-for-custom-dataset/9924","1","1.1k","Created: Sep 13, 2021 6:54 am Latest: Mar 6, 2022 5:30 am","Syed313","How to prepare the dataset to feed GPT-J6B for finetuning. Any steps or tutorial is appreciated. Thanks","Sep 13, 2021 6:54 am","dynamicwebpaige","Hi, @Syed313! Thanks for the question. @deniskamazur modified EleutherAI‚Äôs GPT-J 6B model, so you can generate and fine-tune it in Colab or on an equivalent desktop GPU (e.g. single 1080Ti). The proof of concept notebook is available here. As you are probably already aware: the original GPT-J takes 22+ GB memory for float32 parameters; and even if you cast everything to 16-bit, it will still not fit onto most single-GPU setups short of A6000 and A100. You can inference it on TPU or CPUs, but fine-tuning is a bit more expensive. This implementation should be a bit more cost-effective.","Mar 6, 2022 5:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding too many tokens breaks tokenizer","https://discuss.huggingface.co/t/adding-too-many-tokens-breaks-tokenizer/77005","0","269","Created: Mar 12, 2024 2:56 am Latest: Mar 12, 2024 3:06 am","bonkchad","If I try to add too many tokens with add_tokens() everything grinds to a halt when trying to load the tokenizer. I have been banging my head against the wall for a couple days trying to find a work around and I think its just time to reach out to the community. What is the recommended course for a situation where I want to transfer learn from an existing model via fine tuning but also want to radically expand the vocabulary of the tokenizer? Or is that just not ever recommended? Or maybe its just generally never recommended to have more than 50k ish tokens. Or maybe I could just add one new layer directly between the embedding layer and first hidden layer to capture the sorts of patterns I would have captured with an expanded vocabulary. Or maybe I really don‚Äôt need to expand the vocabulary at all! I don‚Äôt know. I‚Äôm pretty new to all this. Any community wisdom here would be welcome and appreciated. I really could go into a lot of details of the things I‚Äôve tried over the last couple of days but I think it‚Äôs better to keep this short. Happy to talk more about that or my project if anyone is interested. But a brief mention of why I think expanding the vocabulary makes sense. I‚Äôm trying to teach a LLM stable-diffusion-prompt-ese which involves a lot of comma delineated expressions that reoccur and should be through t of as a single token. Thanks!","Mar 12, 2024 2:56 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"OSError: Can‚Äôt load tokenizer for ‚Äòfacebook/xmod-base‚Äô","https://discuss.huggingface.co/t/oserror-cant-load-tokenizer-for-facebook-xmod-base/57471","1","1.1k","Created: Oct 5, 2023 7:41 am Latest: Oct 6, 2023 8:49 am","amarahiqbal","If you were trying to load it from ‚Äòhttps: / / huggingface.co / models‚Äô, make sure you don‚Äôt have a local directory with the same name. Otherwise, make sure ‚Äòfacebook / xmod-base‚Äô is the correct path to a directory containing all relevant files for a XLMRobertaTokenizerFast / BertTokenizerFast / GPT2TokenizerFast / BertJapaneseTokenizer / BloomTokenizerFast / CodeGenTokenizerFast tokenizer","Oct 5, 2023 7:41 am","Sandy1857","If you are getting the error while initializing the tokenizer, do this instead: from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""xlm-roberta-base"") As per documentation, the model model reuses the tokenizer of XLM-R","Oct 6, 2023 8:49 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"`GPT2Tokenizer` Tokenizer handling `\n\n` differently in different settings","https://discuss.huggingface.co/t/gpt2tokenizer-tokenizer-handling-n-n-differently-in-different-settings/57289","4","670","Created: Oct 3, 2023 8:28 pm Latest: Oct 4, 2023 4:37 pm","ydukler","Hi all, I was inspecting the tokenization output of GPT2Tokenizer and observe that it will tokenize \n\n as a single token in some cases whereas in other cases it will tokenize it as two \n tokens. Can someone explain why would the tokenizations differ? Here is a quick example: from transformers import GPT2Tokenizer, GPT2Model tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"") tokenizer("" questions.\n\n Human"")['input_ids'] >>> [2683, 13, 198, 198, 20490]  #  [' questions', '.', '\n', '\n', 'Human'] if followed by a space, will tokenize as 2 separate '\n' tokenizer("" questions.\n\nHuman"")['input_ids']  >>> [2683, 13, 628, 5524] # [' questions', '.', '\n\n', ' Human']","Oct 3, 2023 8:28 pm","Bjornedt","The reason ‚Äú\n\n‚Äù might be tokenized differently in varying contexts could be influenced by a few factors: Whitespace Handling: Tokenizers are often sensitive to whitespace. In your example, when a space follows the ‚Äú\n\n‚Äù, the tokenizer might opt to handle each ‚Äú\n‚Äù separately. Context: The tokenizer may take into account the surrounding characters or tokens when determining how to tokenize a substring. It‚Äôs a way to better represent the syntactic and semantic properties of the text.","Oct 3, 2023 11:21 pm","ydukler","Thank you for the response To follow up I would like to understand of how is the context such as the whitespace come into play in the GPT2 tokenizer when it tokenizes a string? For example it would be great to see the step by step creation of the token list for the simple example I provided","Oct 4, 2023 12:17 am","Bjornedt","Do you mean like this? Starting Point: Tokenizer sees the string "" questions.\n\n Human"".  Tokenizing ""questions"": Recognizes "" questions"" as a single token [2683].  Tokenizing '.': Next is a period, recognized as [13].  Tokenizing '\n\n' with a following space: The presence of a space after \n\n causes the tokenizer to treat each newline as separate tokens. Thus, we get two tokens for \n as [198, 198].  Tokenizing ""Human"": The last word "" Human"" is recognized as a token [20490]. If you want to do this programmatically, I suppose that would be a interesting side project.","Oct 4, 2023 11:05 am","ydukler","Hi Thanks again for the response, but this is just stating the obvious‚Ä¶ I am interested to understand the rules of the tokenization, so that If I had such rules in hand, it would be clear why when \n\n is followed by a space, it is tokenized differently. Would be great to see the tokenization rules for the GPT2 tokenizer with a given vocabulary, explained in plain words.","Oct 4, 2023 4:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Cannot create an identical PretrainedTokenizerFast object from a Tokenizer created by tokenizers library","https://discuss.huggingface.co/t/cannot-create-an-identical-pretrainedtokenizerfast-object-from-a-tokenizer-created-by-tokenizers-library/9317","1","1.1k","Created: Aug 18, 2021 6:43 am Latest: Aug 30, 2021 6:15 pm","FeryET","I can‚Äôt seem to create a ‚ÄúPreTrainedTokenizerFast‚Äù object from my original tokenizers tokenizer object that has the same proporties. This is the code for a byte pair tokenizer I have experimented on. The resulting fast tokenizer does not have a [PAD] token, and does not have any special tokens at all.     tokenizer = ByteLevelBPETokenizer()     tokenizer.preprocessor = pre_tokenizers.BertPreTokenizer()     tokenizer.normalizer = normalizers.BertNormalizer()     tokenizer.train_from_iterator(docs, vocab_size=16_000, min_frequency=15, special_tokens = [""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""])     tokenizer._tokenizer.post_processor = processors.BertProcessing(         (""[SEP]"", tokenizer.token_to_id(""[SEP]"")),         (""[CLS]"", tokenizer.token_to_id(""[CLS]"")),     )     tokenizer.enable_truncation(max_length=256)     tokenizer.enable_padding(pad_id=3, pad_token=""[PAD]"")     fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer) The result of printing the fast_tokenizer is: PreTrainedTokenizerFast(name_or_path='', vocab_size=16000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={}) Which model_max_len and special_tokens are wrong in it. Also, there is no pad_token and pad_token_id in the fast_tokenizer object. (the warning for pad_token for example: Using pad_token, but it is not set yet. ) Have I done anything wrong, or is this not supposed to happen? The versions of libraries I‚Äôm using: 'tokenizers                    0.10.3', 'transformers                  4.10.0.dev0' I have also tested with these versions: 'tokenizers                    0.10.3',  'transformers                  4.9.2'","Aug 18, 2021 6:43 am","sgugger","When using PreTrainedTokenizerFast directly and not one of the subclasses, you have to manually set all the attributes specific to Transformers: the model_max_length as well as all the special tokens. The reason is that the Tokenizer has no concept of associated model (so it doesn‚Äôt know the model max length) and even if it has a concept of special tokens, it doesn‚Äôt know the differences between them, so you have to indicate which one is the pad token, which one the mask token etc.","Aug 30, 2021 6:15 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can‚Äôt save ConvBert tokenizer","https://discuss.huggingface.co/t/cant-save-convbert-tokenizer/16006","1","1.1k","Created: Mar 23, 2022 4:48 pm Latest: Dec 4, 2022 9:35 am","Gozdi","When i try to use tokenizer.save_pretrained() i get this error PanicException                            Traceback (most recent call last) <ipython-input-9-d95441fe8bb1> in <module>() ----> 1 tokenizer.save_pretrained('jebac_huggingface')  1 frames /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py in save_pretrained(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)    2106             file_names=file_names,    2107             legacy_format=legacy_format, -> 2108             filename_prefix=filename_prefix,    2109         )    2110   /usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_fast.py in _save_pretrained(self, save_directory, file_names, legacy_format, filename_prefix)     597                 save_directory, (filename_prefix + ""-"" if filename_prefix else """") + TOKENIZER_FILE     598             ) --> 599             self.backend_tokenizer.save(tokenizer_file)     600             file_names = file_names + (tokenizer_file,)     601   PanicException: no entry found for key what can i do about it??","Mar 23, 2022 4:48 pm","sjq","solved by: tokenizer.save_pretrained(save_dir, legacy_format=True)","Dec 4, 2022 9:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Word_to_tokens() and word_ids() ‚Äî- microsoft/deberta-v2/v3","https://discuss.huggingface.co/t/word-to-tokens-and-word-ids-microsoft-deberta-v2-v3/19984","2","478","Created: Jul 4, 2022 8:14 pm Latest: Jul 14, 2022 4:37 pm","shon711","Hello, My code using the mapping functions word_to_tokens() and word_ids() While using deberta-v2/v3 tokenizers I am getting the error msg:     raise ValueError(""word_to_tokens() is not available when using Python based tokenizers"") ValueError: word_to_tokens() is not available when using Python based tokenizers python-BaseException Any workaround for that? Thank in advance, Shon","Jul 4, 2022 8:14 pm","pragmatic-coder","I am facing a similar issue with Flaubert‚Äôs Tokenizer where word_ids() Method is not working and I a shown the same error as you are! If you have found a workaround already; do a post with an update!","Jul 14, 2022 4:25 pm","shon711","No‚Ä¶ I didn‚Äôt. Waiting for any help from here","Jul 14, 2022 4:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to get sp_model variable from T5Tokenizer?","https://discuss.huggingface.co/t/how-to-get-sp-model-variable-from-t5tokenizer/25171","1","994","Created: Oct 29, 2022 7:06 am Latest: Oct 29, 2022 7:49 pm","uunal","Is there a way to acquire sp_model variable from T5Tokenizer? Related code can be found at https://github.com/huggingface/transformers/blob/2e35bac4e73558d334ea5bbf96a1116f7c0d7fb3/src/transformers/models/t5/tokenization_t5.py#L155 When we initiate tokenizer with ex. ‚Äút5-base‚Äù, from_pretrained function only returns fast version which does not have sp_model variable. My aim is to initiate text.SentencepieceTokenizer from sp_model. If there is a way, this will help converting t5 tokenizer easily into tensorflow. Or if there is a alternative way, I‚Äôll be glad to learn Thanks all in advance,","Oct 29, 2022 7:06 am","uunal","I solved it this way, from transformers import AutoTokenizer import sentencepiece as spm from tensorflow_text import SentencepieceTokenizer  tokenizer = AutoTokenizer.from_pretrained(""t5-base"")  sp_model = spm.SentencePieceProcessor() sp_model.Load(tokenizer.vocab_file)  sp_proto = sp_model.serialized_model_proto()  tf_sp = SentencepieceTokenizer(     model=sp_proto,     alpha=0.1,     nbest_size=0,     add_bos=False,     add_eos=True,     reverse=False ) #now you can tokenize or detokenize with  input_ids = tf_sp.tokenize(""some text"") ""some text"" as tensor = tf_sp.detokenize(input_ids)","Oct 29, 2022 6:53 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Pytorch_model.bin not working because of lfs","https://discuss.huggingface.co/t/pytorch-model-bin-not-working-because-of-lfs/18290","2","801","Created: May 24, 2022 6:06 am Latest: May 25, 2022 7:51 am","pnr-svc","Hi dear community, I have a few questions that I would like to consult with you.Is it necessary to modify the model files when using the pre-trained model? I can‚Äôt get rid of the marks of lfs files for a long time.I don‚Äôt understand if it‚Äôs a windows issue.Can you help me with this? How can i recover pytorch file from lfs file.Or where can I use other than VSCode to get rid of the marks? Also, where can I get pre-trained model codes? Thank you for being here‚Ä¶","May 24, 2022 6:06 am","julien-c","not sure we can really help you with the info you give. What are the precise steps you take?","May 25, 2022 6:28 am","pnr-svc","Hi, Julien First of all thanks for your support, Here‚Äôs how I see the lfs model file,","May 25, 2022 7:51 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ValueError: Unable to create tensor for 1 dataset but not the other of same type","https://discuss.huggingface.co/t/valueerror-unable-to-create-tensor-for-1-dataset-but-not-the-other-of-same-type/15995","1","981","Created: Mar 23, 2022 11:37 am Latest: Mar 23, 2022 1:17 pm","ollibolli","Hi, I have split 2 datasets of the same type, tweets and labels for sequence classification. Both I create the exact same way, from pandas datasets. They have the same columns, texts, labels in pre dataset conversion and later labels, input_ids and attention_masks. For one I can call Trainer.train() but for the other I get this error: ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. i thought at first it might be due to having longer texts in the one where the error occurs, but the other one actually has a longer max sequence and the mean is about the same 115 characters long. The min length is exactly the same. 13. There are no None or nan values. Can someone point to what this means? This is the tokenize function I use from the docs: def tokenize(batch):     return tokenizer(batch[""texts""], padding=True, truncation=True) edit 1: hmm, could it be that it‚Äôs because of emojis like smiley faces being present? edit 2: hmm, no, all emojis removed still the same error.","Mar 23, 2022 11:37 am","ollibolli","OMG I am the worst. The labels in the other dataset were numbers as strings instead of ints. Changing to int has worked.","Mar 23, 2022 1:17 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Customized tokenization files in run_clm script","https://discuss.huggingface.co/t/customized-tokenization-files-in-run-clm-script/21460","3","691","Created: Aug 10, 2022 7:15 am Latest: Aug 18, 2022 4:19 pm","lianghsun","Hi there, I have the same problem like Load custom pretrained tokenizer this one. I have used Tokenizers to train a tokenization json file, and I would like to use it in script. The script i wrote for setting is: python -m torch.distributed.launch --nproc_per_node 1 run_clm-4.8.0.py \   --model_type gpt2 \   --train_file ./dataset/train_dataset.txt \   --use_fast_tokenizer true \   --tokenizer_name ./tokenization/my_tokenizer.json However, I got the following errors:  File ""run_clm-4.8.0.py"", line 492, in <module>     main()   File ""run_clm-4.8.0.py"", line 308, in main     tokenizer = AutoTokenizer.from_pretrained(model_args.tokenizer_name, **tokenizer_kwargs)   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 498, in from_pretrained     tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 359, in get_tokenizer_config     resolved_config_file = get_file_from_repo(   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/transformers/utils/hub.py"", line 678, in get_file_from_repo     resolved_file = cached_path(   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/transformers/utils/hub.py"", line 282, in cached_path     output_path = get_from_cache(   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/transformers/utils/hub.py"", line 545, in get_from_cache     raise ValueError( ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on. ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 163675) of binary: /home/user/miniconda3/envs/gpt/bin/python Traceback (most recent call last):   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/runpy.py"", line 194, in _run_module_as_main     return _run_code(code, main_globals, None,   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/runpy.py"", line 87, in _run_code     exec(code, run_globals)   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/launch.py"", line 193, in <module>     main()   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/launch.py"", line 189, in main     launch(args)   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/launch.py"", line 174, in launch     run(args)   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/run.py"", line 752, in run     elastic_launch(   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 131, in __call__     return launch_agent(self._config, self._entrypoint, list(args))   File ""/home/user/miniconda3/envs/gpt/lib/python3.8/site-packages/torch/distributed/launcher/api.py"", line 245, in launch_agent     raise ChildFailedError( torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================ run_clm-4.8.0.py FAILED ------------------------------------------------------------ Failures:   <NO_OTHER_FAILURES> ------------------------------------------------------------ Root Cause (first observed failure): [0]:   time      : 2022-08-10_14:56:08   host      : localhost   rank      : 0 (local_rank: 0)   exitcode  : 1 (pid: 163675)   error_file: <N/A>   traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html ============================================================ Dose anyone have ideas about how to set the customized tokenization file at script? Thanks","Aug 10, 2022 7:15 am","jbmaxwell","In my case I found I had to modify the run_clm.py script to use PreTrainedTokenizerFast rather than AutoTokenizer: tokenizer = PreTrainedTokenizerFast(tokenizer_file=model_args.tokenizer_name, model_max_length=256, mask_token=""<mask>"", pad_token=""<pad>"")","Aug 10, 2022 3:27 pm","lianghsun","After recently diving in this problem, I finally figured out how to deal with ‚ÄúUsing Tokenizers module to build tokenization map and using Transformers AutoTokenizer.from_pretrained() API‚Äù. I‚Äôll write down the details for those who might encounter this problem in the future. @jbmaxwell 's reply is one of the method that you need to customize the script, however this method is not the best one, bcz this may loss some important tokenization details likes: merges.txt , special_tokens_map.json , tokenizer_config.json , tokenizer.json and vocab.json. The best way to deal this problem is: # Tokenizers API tokenizer.save('./path/to/your/tokenization.json')  # Transformer API from transformers import PreTrainedTokenizerFast new_tokenizer = PreTrainedTokenizerFast(tokenizer_file='./path/to/your/tokenization.json') new_tokenizer.add_special_tokens(     {'bos_token': '[BOS]',      'eos_token': '[EOS]',      'unk_token': '[UNK]',      'sep_token': '[SEP]',      'pad_token': '[PAD]',      'cls_token': '[CLS]',      'mask_token': '[MASK]'} ) new_tokenizer.save_pretrained('./path/to/your/folder/') # then you have the everything! >>  ('./path/to/your/folder/tokenizer_config.json',  './path/to/your/folder/special_tokens_map.json',  './path/to/your/folder/tokenizer.json' ...)","Aug 17, 2022 6:34 am","jbmaxwell","Oops! Yes, you‚Äôre absolutely right! I encountered the same problem when I first used PreTrainedTokenizerFast, but completely forgot to indicate the (essential) detail of adding the special tokens and re-saving. Once you‚Äôre re-saved you no longer need to add the tokens (which is probably why that step completely slipped my mind!). Apologies for the partial answer!","Aug 18, 2022 4:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Microsoft/codebert-base produces two sep tokens","https://discuss.huggingface.co/t/microsoft-codebert-base-produces-two-sep-tokens/21366","2","794","Created: Aug 8, 2022 6:51 am Latest: Sep 5, 2022 6:47 am","gostrive","Hey Guys, this post is basically a copy of this one. The first one did not get any attention, but I think the problem might be important and this is probably a better place to ask the question. If this is the wrong place to post this, please let me know where to post. I noticed, that the ‚Äúmicrosoft/codebert-base‚Äù tokenizer adds two SEP Tokens between two sentences. I don‚Äôt think this is intended behavior, but feel free to correct me. This is the example I originally posted: from transformers import AutoTokenizer, AutoModel import torch device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  checkpoint = ""microsoft/codebert-base""  model = AutoModel.from_pretrained(checkpoint) tokenizer = AutoTokenizer.from_pretrained(checkpoint)  from datasets import load_dataset  raw_dataset = load_dataset('json', data_files='/home/<user>/Data/<DataDir>/dataset_v1.jsonl', split='train')  def toke(example):     return tokenizer(example[""sentence1""], example[""sentence2""])   tokenized_dataset = raw_dataset.select(list(range(10000))).map(toke, batched=True)  print(tokenized_dataset[7]['sentence1']) print(tokenized_dataset[7]['sentence2']) print(tokenized_dataset[7]['input_ids']) Output: train_nan_df.head() test_df[‚ÄòImageId‚Äô] = np.array(os.listdir (‚Äò‚Ä¶/input/test_images/‚Äô)) [0, 21714, 1215, 10197, 1215, 36807, 4, 3628, 43048, 2, 2, 21959, 1215, 36807, 48759, 8532, 28081, 44403, 5457, 46446, 4, 30766, 1640, 366, 4, 8458, 41292, 31509, 49445, 46797, 73, 21959, 1215, 39472, 73, 108, 35122, 2] Here is another example using publicly available data: In: !pip install transformers datasets  from transformers import AutoTokenizer, AutoModel import torch device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  checkpoint = ""microsoft/codebert-base""  model = AutoModel.from_pretrained(checkpoint) tokenizer = AutoTokenizer.from_pretrained(checkpoint)  tokenizer.sep_token_id Out: 2 In: tokenizer(""this is the first sentence"", ""this is the second sentence"") Out: {'input_ids': [0, 9226, 16, 5, 78, 3645, 2, 2, 9226, 16, 5, 200, 3645, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} Also calling the respective tokenizer.tokenize and tokenizer.convert_tokens_to_ids functions does not produce two SEP tokens.","Aug 8, 2022 6:51 am","vildhet","Looks like it‚Äôs the expected behavior for some of the tokenizers. At least according to this github issue.","Sep 4, 2022 9:53 am","gostrive","Thanks for your reply! I did not expect to still get an answer but the post you shard answered my question! Merci","Sep 5, 2022 6:47 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to make tokenizer convert subword token to an independent token?","https://discuss.huggingface.co/t/how-to-make-tokenizer-convert-subword-token-to-an-independent-token/1015","4","610","Created: Sep 7, 2020 6:31 am Latest: Sep 9, 2020 10:09 am","Khondoker","Recently, I have been using bert-base-multilingual-cased for my work in bengali. When I feed in a sentence like ‚Äú‡¶Ü‡¶ú‡¶ï‡ßá ‡¶π‡¶¨‡ßá ‡¶®‡¶æ‚Äù to BertTokenizer, I get the following output. Sentence:  ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶π‡¶¨‡ßá ‡¶®‡¶æ Tokens:  ['‡¶Ü', '##‡¶ú', '##‡¶ï‡ßá', '‡¶π‡¶¨‡ßá', '‡¶®‡¶æ'] To Int:  [938, 24383, 18243, 53761, 26109] But when I fed in a sentence like ‚Äú‡¶Ü‡¶ú‡¶ï‡ßá ‡¶π‡¶¨‡ßá‡¶®‡¶æ‚Äù with ‚Äú‡¶®‡¶æ‚Äù not spaced, I see tokens been ‚Äú##‡¶®‡¶æ‚Äù with corresponding index also been changed Sentence:  ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶π‡¶¨‡ßá‡¶®‡¶æ Tokens:  ['‡¶Ü', '##‡¶ú', '##‡¶ï‡ßá', '‡¶π‡¶¨‡ßá', '##‡¶®‡¶æ'] To Int:  [938, 24383, 18243, 53761, 20979] Now, I was hoping is there anyway to let tokenizer to know that if they find anything like ‚Äò##‡¶®‡¶æ‚Äô, convert them to ‚Äò‡¶®‡¶æ‚Äô for all such cases.","Sep 7, 2020 6:31 am","rgwatwormhill","It might be easier to replace the ‡¶®‡¶æ in the sentence with ‚Äúspace‚Äù ‡¶®‡¶æ before you tokenize. Is it just the ##‡¶®‡¶æ that is a problem, or do you want to get rid of all the ## continuation tokens?","Sep 9, 2020 12:05 am","Khondoker","Recently I realized, it is not for all ‡¶®‡¶æ. And no, not for all ## continuation tokens, for few only.","Sep 9, 2020 5:28 am","BramVanroy","Can‚Äôt you just replace the tokens before converting them to IDs? # set of all tokens that should you be replaced NEED_REPL = {""##‡¶®‡¶æ""}  def retokenize(tokens):     return [t.replace(""##"", """", 1) if t in NEED_REPL else t for t in tokens]   tokens = ['‡¶Ü', '##‡¶ú', '##‡¶ï‡ßá', '‡¶π‡¶¨‡ßá', '##‡¶®‡¶æ'] replaced = retokenize(tokens) print(replaced) # ['‡¶Ü', '##‡¶ú', '##‡¶ï‡ßá', '‡¶π‡¶¨‡ßá', '‡¶®‡¶æ']","Sep 9, 2020 8:15 am","Khondoker","Yes but when I pass in the whole dataset for the tokenizer to handle, I had to do something like     encoding = self.tokenizer.encode_plus(     reviews,     add_special_tokens = True,     max_length = self.max_len,     return_token_type_ids=True,     pad_to_max_length=True,     return_attention_mask=True,     return_tensors='pt', ) where the encoding variable consists the input_ids and attention_masks for each sentence respectively. How could I overwrite or overcome the tokenizer function?","Sep 9, 2020 10:09 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unable to load image using llama-index","https://discuss.huggingface.co/t/unable-to-load-image-using-llama-index/38304","0","1.4k","Created: May 1, 2023 1:44 pm","itsamamir","Hi everyone, i am trying to load an image using llama-index but got the error below. Does anyone have any clue? --------------------------------------------------------------------------- RuntimeError                              Traceback (most recent call last) File C:\ProgramData\anaconda3\lib\site-packages\transformers\feature_extraction_utils.py:164, in BatchFeature.convert_to_tensors(self, tensor_type)     163 if not is_tensor(value): --> 164     tensor = as_tensor(value)     166     self[key] = tensor  File C:\ProgramData\anaconda3\lib\site-packages\transformers\feature_extraction_utils.py:146, in BatchFeature.convert_to_tensors.<locals>.as_tensor(value)     145     value = np.array(value) --> 146 return torch.tensor(value)  RuntimeError: Could not infer dtype of numpy.float32  During handling of the above exception, another exception occurred:  ValueError                                Traceback (most recent call last) Cell In[5], line 43      41 ImageReader = download_loader(""ImageReader"")      42 loader = ImageReader(text_type = ""key_value"") ---> 43 documents = loader.load_data(file=Path(path))      45 index = GPTSimpleVectorIndex(documents,service_context)      47 question = ""what is the total amount of invoice?""  File C:\ProgramData\anaconda3\lib\site-packages\llama_index\readers\llamahub_modules/file/image/base.py:88, in ImageReader.load_data(self, file, extra_info)      83 task_prompt = ""<s_cord-v2>""      84 decoder_input_ids = processor.tokenizer(      85     task_prompt, add_special_tokens=False, return_tensors=""pt"",padding=True      86 ).input_ids ---> 88 pixel_values = processor(image, return_tensors=""pt"").pixel_values      90 outputs = model.generate(      91     pixel_values.to(device),      92     decoder_input_ids=decoder_input_ids.to(device),    (...)     100     return_dict_in_generate=True,     101 )     103 sequence = processor.batch_decode(outputs.sequences)[0]  File C:\ProgramData\anaconda3\lib\site-packages\transformers\models\donut\processing_donut.py:69, in DonutProcessor.__call__(self, *args, **kwargs)      66     raise ValueError(""You need to specify either an `images` or `text` input to process."")      68 if images is not None: ---> 69     inputs = self.feature_extractor(images, *args, **kwargs)      70 if text is not None:      71     encodings = self.tokenizer(text, **kwargs)  File C:\ProgramData\anaconda3\lib\site-packages\transformers\models\donut\feature_extraction_donut.py:209, in DonutFeatureExtractor.__call__(self, images, return_tensors, random_padding, **kwargs)     207 # return as BatchFeature     208 data = {""pixel_values"": images} --> 209 encoded_inputs = BatchFeature(data=data, tensor_type=return_tensors)     211 return encoded_inputs  File C:\ProgramData\anaconda3\lib\site-packages\transformers\feature_extraction_utils.py:75, in BatchFeature.__init__(self, data, tensor_type)      73 def __init__(self, data: Optional[Dict[str, Any]] = None, tensor_type: Union[None, str, TensorType] = None):      74     super().__init__(data) ---> 75     self.convert_to_tensors(tensor_type=tensor_type)  File C:\ProgramData\anaconda3\lib\site-packages\transformers\feature_extraction_utils.py:170, in BatchFeature.convert_to_tensors(self, tensor_type)     168         if key == ""overflowing_values"":     169             raise ValueError(""Unable to create tensor returning overflowing values of different lengths. "") --> 170         raise ValueError(     171             ""Unable to create tensor, you should probably activate padding ""     172             ""with 'padding=True' to have batched tensors with the same length.""     173         )     175 return self  ValueError: Unable to create tensor, you should probably activate padding with 'padding=True' to have batched tensors with the same length.","May 1, 2023 1:44 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using truncated fragments as input samples in training","https://discuss.huggingface.co/t/using-truncated-fragments-as-input-samples-in-training/6978","3","655","Created: Jun 18, 2021 6:27 pm Latest: Jul 1, 2021 12:38 pm","lesscomfortable","Hi! I am using the tokenizers library, roughly following the run_mlm.py script to train a Masked Language Model (MobileBert) from scratch. Since I am training an unsupervised model using truncated sentences, I was wondering if the truncated (left-out) fragments are included by default in the dataset for training since they would be valid examples for my use case (MLM setting). If they are not used (which I believe to be the case) I wanted to ask if there is any easy way in which I might include them in my training dataset (maybe by using return_overflowing_tokens and stride in a smart way?). As an additional related question, I would like to know if there is any native way of sorting by length before batching to reduce the dataset size to the minimum. Something along these lines: pommedeterresautee gist and McCormickML blogpost. EDIT: The best way I have found to do the smart batching is to create an ‚Äòsample_length‚Äô column and use the .sort method to sort by that column before tokenizing. Thanks in advance!","Jun 18, 2021 6:27 pm","sgugger","By default, those are not included (unless you use the --line_by_line option which will concatenate all the samples then create block of the size you picked). Using return_overflowing_tokens is definitely an option to get those truncated part! stride is only if you want some overlap between the two parts of a long sentence, which is useful for question answering, but not necessarily for masked language modeling pretraining. For the sorting by length before batching, we have the --group_by_length option in the Trainer, though it‚Äôs for the dataset so it happens after tokenization, which may not be what you are looking for.","Jun 21, 2021 1:06 pm","lesscomfortable","Thanks Sylvain! As per your video, I understand that the --group_by_length option is compatible with the DataCollatorWithPadding. Is it also compatible with the DataCollatorForLanguageModelling? I understand it is, since according to the docs the DataCollatorForLanguageModelling dynamically pads to make batches even.","Jun 30, 2021 9:27 pm","sgugger","Yes, it‚Äôs compatible with any data collator, it changes the sampler of the dataset only.","Jul 1, 2021 12:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BertTokenizerFast for stsb-xlm-r-multilingual model","https://discuss.huggingface.co/t/berttokenizerfast-for-stsb-xlm-r-multilingual-model/4742","3","653","Created: Mar 19, 2021 7:28 pm Latest: Apr 8, 2021 9:19 pm","Matthieu","Hi community, Would there be a fast tokenizer for the stsb-xlm-r-multilingual model? Thanks !","Mar 19, 2021 7:28 pm","Matthieu","Hi community and @lewtun, Could anyone have an idea on how to get a fast tokenizer for stsb-xlm-r-multilingual model? I am blocked with low latency response due to tokenizer computation. Is there a fast tokenizer model as BertTokenizerFast or is there a way to run tokenizer on GPU ?","Apr 8, 2021 7:04 am","lewtun","hey @Matthieu, as far as i know the ‚Äúfast‚Äù refers to the rust implementations of the tokenizers: tokenizers/tokenizers at master ¬∑ huggingface/tokenizers ¬∑ GitHub there are bindings for python, so perhaps you can adapt the suggestion here to your use case? e.g. download the tokenizer.json file for stsb and load the fast version as follows: from transformers import RobertaTokenizerFast tokenizer = RobertaTokenizerFast(tokenizer_file=""tokenizer.json"") (i‚Äôm not super familiar with the stsb-xlm-r-multilingual model but am assuming it‚Äôs using the same tokenization strategy as XLM-R)","Apr 8, 2021 7:38 am","Matthieu","Hi @lewtun thanks. I finally found that there is a xlmrobertatokenizerfast implementation.","Apr 8, 2021 9:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Difference between tokenizer and convert_tokens_to_ids","https://discuss.huggingface.co/t/difference-between-tokenizer-and-convert-tokens-to-ids/86361","0","233","Created: May 12, 2024 5:57 pm Latest: May 12, 2024 6:07 pm","mcallara","I was trying to convert tokens that contain spaces into ids and realized that I don‚Äôt get the same result if I use convert_tokens_to_ids. Shouldn‚Äôt this map to the same ids? Thank you for your help. from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(             ""meta-llama/Meta-Llama-3-8B"",             use_fast=False,         )  # This returns None for both tokenizer.convert_tokens_to_ids(["" "","" ,""]) # This returns the correct ids (220 and 1174) tokenizer(["" "","" ,""],add_special_tokens=False).input_ids # To get the right tokens I need to replace spaces with ƒ†, shouldn't this be handle by the convert_tokens_to_ids method? tokenizer.convert_tokens_to_ids([""ƒ†"",""ƒ†,""])","May 12, 2024 5:57 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Reused tokenizer returns unk","https://discuss.huggingface.co/t/reused-tokenizer-returns-unk/23358","1","513","Created: Sep 19, 2022 8:40 pm Latest: Mar 14, 2024 9:41 pm","EmnaBou","Hello I‚Äôm training a tokenizer from an old one (Bert based) the new tokenizer returns [UNK] for words already exist in vocabulary and run correctly with the old tokenizer from transformers import AutoTokenizer  old_tokenizer = AutoTokenizer.from_pretrained(""asafaya/bert-base-arabic"")  tokens = old_tokenizer.tokenize('ŸÖÿπ') tokens returns [‚ÄòŸÖÿπ‚Äô] while with the new tokenizer new_tokenizer= old_tokenizer.train_new_from_iterator(training_corpus, 10) tokens = new_tokenizer.tokenize('ŸÖÿπ') tokens returns [‚Äò[UNK]‚Äô] Can any one help me please !","Sep 19, 2022 8:40 pm","KashMkj18","I am also facing similar issue. Were you able to sort it our ?","Mar 14, 2024 9:41 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Batch tokenize (split into tokens, without processing)","https://discuss.huggingface.co/t/batch-tokenize-split-into-tokens-without-processing/60238","4","565","Created: Oct 28, 2023 10:27 am Latest: Oct 28, 2023 1:43 pm","Sifal","Is there a function that does tokenizer.tokenize(‚Äòtext‚Äô) except on a batch? (ie return tokens rather than ids)","Oct 28, 2023 10:27 am","AIGeekProgrammer","You mean like this? tokens = tokenizer.tokenize(""This is the extraction of tokens."") print(tokens) ['This', 'is', 'the', 'extraction', 'of', 'token', '##s', '.']","Oct 28, 2023 12:48 pm","Sifal","yes but for a batch of sequences, like tokenizer.batch_tokenize(batch_size * sequences) returns a batch of tokenized sequences ( not the ids , just the split tokens)","Oct 28, 2023 1:17 pm","AIGeekProgrammer","AFAIK, the tokenizer does not have a built-in method for processing a batch of sequences into tokens. You can achieve this by using a list comprehension: sequences = [[""This is the extraction of tokens."",                ""This is the second sentence""]] tokenized_sequences = [tokenizer.tokenize(sequence) for sequence in sequences] print(tokenized_sequences)","Oct 28, 2023 1:34 pm","Sifal","That‚Äôs what I‚Äôve been using and it‚Äôs been causing major bottleneck issues, and I need the tokens to pass them to another function","Oct 28, 2023 1:43 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to ensure the `overflow` with `stride` always starts with a full word?","https://discuss.huggingface.co/t/how-to-ensure-the-overflow-with-stride-always-starts-with-a-full-word/14030","0","1.2k","Created: Jan 24, 2022 3:35 am Latest: Jan 24, 2022 3:35 am","nasheed","In continuation to my previous question, When I tokenize a sentence to max_length while keeping the overflowing tokens which are returned with a stride (overlap) on the previous segment, as is in the code below: tokenizer= SomeTokenizer.from_pretrained('some/path') tokenizer('I am Nasheed and I like xylophones.', truncation=True, max_length=12, return_overflowing_tokens=True, stride=7) The above snippet segments my original string as below: Segment1: ‚ÄòI am Nasheed and I like xylo‚Äô Segment2: ‚Äòheed and I like xylophones.‚Äô I want to ensure that the overflow segments I get always start with whole words as below*: Segment1: ‚ÄòI am Nasheed and I like‚Äô Segment2: ‚ÄòNasheed and I like‚Äô Segment3: ‚Äòand I like xylophones.‚Äô In Segment1 I have removed the partial word xylo, this can be done using what has been suggested here","Jan 24, 2022 3:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer extremely slow when deployed to a container","https://discuss.huggingface.co/t/tokenizer-extremely-slow-when-deployed-to-a-container/36569","0","1.2k","Created: Apr 14, 2023 12:55 pm Latest: Apr 14, 2023 1:14 pm","igormarques76","Hey there, I need some help with the performance of my classifier. I have trained a new model for text classification using the pre-trained BERT model (bert-base-uncased). I created a class that handles the text transformations from raw to model-ready data. I am currently using the bert-base-uncased tokenizer for processing the raw text. When running my code on a Jupyter Notebook or on my local machine, I can get predictions on new, unseen data in a matter of seconds. When I deploy my model to a Docker container it is extremely slow, to the point where the container times out even before producing the predictions. From debugging my code, I can see that it gets stuck in the tokenizer part. I have already tried batch encoding, single encoding, using the fast tokenizer from the AutoTokenizer library, and no matter what I do or change, the performance does not improve. I am not using GPUs, but I am trying to infer data from a relatively small dataset (approx. 100 data points). This is how I defined my class for transforming my raw dataset into a model-ready dataset: class textDataset(Dataset):     # Constructor Function     def __init__(self, features, tokenizer, max_len):         self.features = features         self.tokenizer = tokenizer         self.max_len = max_len      # Length method     def __len__(self):         return len(self.features)      # get item method     def __getitem__(self, item):         feature = str(self.features[item])          # Encoded format to be returned         encoding = self.tokenizer.encode_plus(             feature,             add_special_tokens=True,             max_length=self.max_len,             return_token_type_ids=False,             pad_to_max_length=True,             return_attention_mask=True,             truncation=True,             return_tensors='pt',         )          return {             'feature': feature,             'input_ids': encoding['input_ids'].flatten(),             'attention_mask': encoding['attention_mask'].flatten(),         } Function for creating a PyTorch dataset: def create_data_loader(df, tokenizer, max_len, batch_size):     ds = textDataset(         features=df.feature.to_numpy(),         tokenizer=tokenizer,         max_len=max_len     )      return DataLoader(         ds,         batch_size=batch_size,         # num_workers=4     ) Tokenizers used (I tried a few to see if performance would improve): tokenizer = BertTokenizer.from_pretrained(""bert-base-uncase"") tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncase"", use_fast=True) Class for model instance: class textClassifier(nn.Module):      # Constructor class     def __init__(self, n_classes):         super(textClassifier, self).__init__()         self.bert = BertModel.from_pretrained(""bert-base-uncased""), return_dict=False)         self.drop = nn.Dropout(p=0.3)         self.out = nn.Linear(self.bert.config.hidden_size, n_classes)      # Forward propagation class     def forward(self, input_ids, attention_mask):         _, pooled_output = self.bert(             input_ids=input_ids,             attention_mask=attention_mask         )         #  Add a dropout layer         output = self.drop(pooled_output)         return self.out(output) Function for predicting new values (which is where it gets stuck): def get_predictions(model, data_loader):     model = model.eval()      feature_texts = []     predictions = []     prediction_probs = []      with torch.no_grad():         for d in data_loader:             texts = d[""feature""]             input_ids = d[""input_ids""]             attention_mask = d[""attention_mask""]              # Get outputs             outputs = model(                 input_ids=input_ids,                 attention_mask=attention_mask             )             _, preds = torch.max(outputs, dim=1)              feature_texts.extend(texts)             predictions.extend(preds)             prediction_probs.extend(outputs)      predictions = torch.stack(predictions).cpu()     prediction_probs = torch.stack(prediction_probs).cpu()      return feature_texts, predictions, prediction_probs And finally the main part where I call all the functions: # Create data loader from processed df to use for model inference              # Instantiating PyTorch model             model = textClassifier(len(class_names))             batch_size = 16             prod_data_loader = create_data_loader(                 features_transformed_df,                 tokenizer,                 max_len,                 batch_size)              feature_texts, pred_ids, pred_probs = get_predictions(model, prod_data_loader) I am relatively new to Transformers/PyTorch and I am having a hard time figuring out why my code is so slow when deployed to a container or an AWS lambda. Would anyone know the reason for this performance issue? Am I doing anything in the code above that is compromising my performance? Thanks a lot in advance.","Apr 14, 2023 12:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Writing custom tokenizer and wrapping it in tokenizer object","https://discuss.huggingface.co/t/writing-custom-tokenizer-and-wrapping-it-in-tokenizer-object/39679","2","709","Created: May 12, 2023 5:53 pm Latest: Jun 26, 2023 12:16 am","arnab9learns","how to write my own tokenizer and wrap it in huggingface tokenizer object? we will follow this procedure instead of using any pretokenizer and get a tokenizer object like if we use one pretrained tokenizer? huggingface.co Byte-Pair Encoding tokenization - Hugging Face NLP Course We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.","May 12, 2023 5:53 pm","maveriq","Did you find a solution?","Jun 25, 2023 8:25 am","arnab9learns","No, Nobody seems to be interested in it. and I am kind of disappointed. let me know if you find something. I am still stuck on this issue.","Jun 26, 2023 12:16 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A problem about FutureWarningÔºü","https://discuss.huggingface.co/t/a-problem-about-futurewarning/9323","0","1.2k","Created: Aug 18, 2021 9:17 am","ccfeidao","when a user my vocab to pretrain, my code is tokenizer = BertTokenizer.from_pretrained('my_vocab.txt' ) the out is: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the Datasets library. You can have a look at this example script for pointers: transformers/run_mlm.py at master ¬∑ huggingface/transformers ¬∑ GitHub how can i do is corect in future? I don`t understand this.","Aug 18, 2021 9:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FutureWarning about BertTokenizer.from_pretrained() at latest version","https://discuss.huggingface.co/t/futurewarning-about-berttokenizer-from-pretrained-at-latest-version/18750","0","1.2k","Created: Jun 6, 2022 7:22 am Latest: Jun 6, 2022 8:37 am","ccfeidao","I use BertTokenizer.from_pretrained(‚Äòfile path‚Äô); The file is my manually write the vaocb.txt. , It‚Äôs have FutureWarning.How can I continue from my handwritten vocab.txt to load my own tokenizer? FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead. vacob.txt is this: [PAD] [UNK] [CLS] [SEP] [MASK] ... my_word1 my_word2 my_word3 My goal is to train a new ‚Äúlanguage‚Äù, in which Tokenizer needs to be manually specified, rather than directly loading from the pretrained model.","Jun 6, 2022 7:22 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Offset mappings differ for tokenizers","https://discuss.huggingface.co/t/offset-mappings-differ-for-tokenizers/60424","0","1.2k","Created: Oct 30, 2023 12:37 pm","UWinch","Hi, I‚Äôm trying to map offsets of subtokens back to their original text. It seems that they are off by one in some cases depending on the used tokenizer and if special tokens are used. I would like to know if there are patterns to this behaviour e.g. always if a ByteLevel encoding is used, the offsets need to be shifted by one to account for the begin-prefix (ƒ†). My goal is to implement this mapping so it works for every tokenizer. Is this possible? Are there rules that the tokenizers follow when it comes to the offsets? For example running the following code: from transformers import AutoTokenizer  model_name = ""microsoft/mdeberta-v3-base"" text = ""This is a great \n test.""  tokenizer: PreTrainedTokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.add_tokens([""\n""], special_tokens=True)   encoding: BatchEncoding = tokenizer(text, return_offsets_mapping=True, add_special_tokens=False) print(tokenizer.convert_ids_to_tokens(encoding[""input_ids""]))  # map back to original text using the offsets tokens = [text[begin:end] for begin, end in encoding.offset_mapping] print(tokens) with deberta results in ['‚ñÅThis', '‚ñÅis', '‚ñÅ', 'a', '‚ñÅgreat', '\n', '‚ñÅtest', '.'] ['This', ' is', ' ', 'a', ' great', '\n', 'test', '.'] and with gpt-2, I get ['This', 'ƒ†is', 'ƒ†a', 'ƒ†great', 'ƒ†', '\n', 'ƒ†test', '.'] ['This', ' is', ' a', ' great', ' ', '\n', ' test', '.']. For gpt-2, I could correct the offsets by adding one to the begin offset if the subtoken starts with a begin-prefix. However, this does not work for the deberta subtokens, because e.g. the offset after the special token does not follow this logic. Thanks for every reply.","Oct 30, 2023 12:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ONNX T5 - Decoding seq2seq tokens","https://discuss.huggingface.co/t/onnx-t5-decoding-seq2seq-tokens/36321","1","479","Created: Apr 12, 2023 9:03 am Latest: May 8, 2024 1:36 pm","eusip","I am trying to debug an issue decoding text I generate using a finetuned T5 model that I have created yet I continue to encounter the TypeError shown below. Traceback (most recent call last):   File ""generate-mirage-onnx.py"", line 258, in <module>     output_text, output_logits = flan_t5(prompt, max_length=512, temperature=0.)   File ""/opt/conda/envs/accelerate/lib/python3.7/site-packages/torch/nn/modules/module.py"", line 1194, in _call_impl     return forward_call(*input, **kwargs)   File ""generate-mirage-onnx.py"", line 112, in forward     return self.tokenizer.decode(new_tokens), new_logits   File ""/opt/conda/envs/accelerate/lib/python3.7/site-packages/transformers/tokenization_utils_base.py"", line 3480, in decode     **kwargs,   File ""/opt/conda/envs/accelerate/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py"", line 549, in _decode     text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens) TypeError: argument 'ids': 'float' object cannot be interpreted as an integer I am quite familiar with the tokenizer functionality of Transformers but it appear that there is something about seq2seq models that I am missing. Any tips would be greatly appreciate! My full script can be found here. @nielsr any tips or suggestions would be greatly appreciated.","Apr 12, 2023 9:03 am","Hope2000","I have the same issue with T5, if you fixed your issue, please let me know how did you do it.","May 8, 2024 1:36 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using a pretrained tokenizer vs training a one from scratch","https://discuss.huggingface.co/t/using-a-pretrained-tokenizer-vs-training-a-one-from-scratch/783","1","843","Created: Aug 20, 2020 10:14 am Latest: Aug 21, 2020 9:15 am","abdallah197","Hi For domain-specific data, let‚Äôs say medical drug data with complicated chemical compounds names. Would it be beneficial to train a tokenizer on the text if the size was nearly 18 M entries? In the bioBERT paper, they used a pre-trained BERT paper for the following reasons: compatibility of BioBERT with BERT, which allows BERT pre-trained on general domain corpora to be re-used, and makes it easier to interchangeably use existing models based on BERT and BioBERT any new words may still be represented and fine-tuned for the biomedical domain using the original WordPiece vocabulary of BERT.","Aug 20, 2020 10:14 am","rgwatwormhill","How many different chemical compound names are there in the 18 M entries? Having lots of data is good, but I don‚Äôt think training a tokenizer would help you unless the words you are interested in are frequent enough to be selected for the tokenizer‚Äôs vocabulary. I‚Äôm not sure how the tokenizer chooses its vocabulary, but word-frequency must be important. I‚Äôm guessing that ‚Äúmedical drug data‚Äù would still include lots of normal-English words, many of which would be more frequent than the chemicals. [I am not an expert].","Aug 21, 2020 9:15 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using Custom Vocab.txt","https://discuss.huggingface.co/t/using-custom-vocab-txt/10847","0","1.2k","Created: Oct 17, 2021 1:37 pm","mlanires","I am using BERT model for finetuning for a simple classification task. However, I want to add words which are very frequent to our domain which are not present in the original vocab.txt. When I added the words in the ‚Äòunused x‚Äô placeholders in the original vocab file ( vocab.txt and trained using that), it is not recognising the added words. Can you please guide as to what is the proper way to do it ? Regards,","Oct 17, 2021 1:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Asking to pad but the tokenizer does not have a padding token","https://discuss.huggingface.co/t/asking-to-pad-but-the-tokenizer-does-not-have-a-padding-token/85344","0","1.2k","Created: May 6, 2024 4:23 am","anon72224015","tokenizer = AutoTokenizer.from_pretrained( base_model_id, padding=True, padding_side=‚Äúleft‚Äù, pad_token=‚Äú[PAD]‚Äù, add_eos_token=True, add_bos_token=True, ) Set the padding token in the tokenizer tokenizer.pad_token = tokenizer.eos_token Now initialize the trainer trainer = SetFitTrainer( model=model, train_dataset=tokenized_train_dataset, eval_dataset=tokenized_val_dataset, loss_class=CosineSimilarityLoss, batch_size=16, num_iterations=20, # The number of text pairs to generate for contrastive learning column_mapping={‚Äútext‚Äù: ‚Äútext‚Äù, ‚Äúlabel‚Äù: ‚Äúlabel‚Äù} # Map dataset columns to text/label expected by trainer ) ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as pad_token (tokenizer.pad_token = tokenizer.eos_token e.g.) or add a new pad token via tokenizer.add_special_tokens({'pad_token': '[PAD]'}).","May 6, 2024 4:23 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does T5Tokenizer support the Greek language?","https://discuss.huggingface.co/t/does-t5tokenizer-support-the-greek-language/12224","1","821","Created: Nov 24, 2021 12:36 am Latest: Nov 24, 2021 10:23 am","evroschris98","Does T5Tokenizer support the Greek language? When I run the 3 lines of code below, then the input_ids are just 2 and 3 which correspond to the unknown token and the underscore respectively. This is the same for any input text of Greek letters. from transformers import T5Tokenizer tokenizer = T5Tokenizer.from_pretrained(‚Äút5-small‚Äù) input_ids = tokenizer(‚ÄòŒìŒµŒπŒ¨ œÉŒøœÖ ŒöœåœÉŒºŒµ‚Äô, return_tensors=‚Äòpt‚Äô).input_ids","Nov 24, 2021 12:36 am","nielsr","Hi, T5 itself was trained on English data only. However, there‚Äôs a multilingual variant called mT5 which supports Greek.","Nov 24, 2021 10:23 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Generate tokenizer.json for Marian(Opus) MT","https://discuss.huggingface.co/t/generate-tokenizer-json-for-marian-opus-mt/28157","2","628","Created: Dec 20, 2022 8:33 am Latest: Nov 4, 2024 4:02 am","chriszz1","Hi all, I am deploying MT model on an Android device with Java. The Deep Java Lib (DJL) support the Autotokenizer of Huggingface, but it only supports the one with the tokenizer.json file. However, the tokenizer of Opus MT requires SnetencePiece files including source.spm, target.spm and vocab.json. Is there any way to convert the files to one tokenizer.json so I can apply it in DJL?","Dec 20, 2022 8:33 am","musram","@has anybody solved this problem?","Feb 26, 2024 1:04 pm","jgpmymi","any luck on this?","Nov 4, 2024 4:02 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LlamaTokenizerFast returns token_type_ids but the forward pass of the LlamaModel does not receive token_type_ids","https://discuss.huggingface.co/t/llamatokenizerfast-returns-token-type-ids-but-the-forward-pass-of-the-llamamodel-does-not-receive-token-type-ids/42431","1","753","Created: Jun 7, 2023 3:13 pm Latest: Jun 9, 2023 7:37 am","cfierro","github.com huggingface/transformers/blob/092c14c37d9baa6a12be5481138564a720a49a8a/src/transformers/models/llama/modeling_llama.py#L484-L495 def forward(     self,     input_ids: torch.LongTensor = None,     attention_mask: Optional[torch.Tensor] = None,     position_ids: Optional[torch.LongTensor] = None,     past_key_values: Optional[List[torch.FloatTensor]] = None,     inputs_embeds: Optional[torch.FloatTensor] = None,     use_cache: Optional[bool] = None,     output_attentions: Optional[bool] = None,     output_hidden_states: Optional[bool] = None,     return_dict: Optional[bool] = None, ) -> Union[Tuple, BaseModelOutputWithPast]: Python 3.10.11 transformers==4.28.1","Jun 7, 2023 3:13 pm","ArthurZ","Hey thanks for reporting! This was adressed in [Lllama] Update tokenization code to ensure parsing of the special tokens [core] by ArthurZucker ¬∑ Pull Request #24042 ¬∑ huggingface/transformers ¬∑ GitHub !","Jun 9, 2023 7:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Batch encode plus in Rust Tokenizers","https://discuss.huggingface.co/t/batch-encode-plus-in-rust-tokenizers/12722","1","742","Created: Dec 11, 2021 6:51 am Latest: Dec 13, 2021 2:35 am","CaioW","In python, BertTokenizerFast has batch_encode_plus, is there a similar method in rust?","Dec 11, 2021 6:51 am","CaioW","I will assume due to the lack of reply that there‚Äôs no way to do this.","Dec 13, 2021 2:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer mapping the same token to multiple token_ids","https://discuss.huggingface.co/t/tokenizer-mapping-the-same-token-to-multiple-token-ids/82328","4","468","Created: Apr 18, 2024 2:56 am Latest: Apr 22, 2024 8:12 am","Chahnwoo","Why do tokenizers sometimes map the same text to different tokens? from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained('TeamUNIVA/Komodo_7B_v1.0.0')  short_text = ""tokenize""  long_text = ""tokenizer.tokenize() vs tokenizer()"" encoded_short = tokenizer.encode(short_text, add_special_tokens=False) encoded_long = tokenizer.encode(long_text, add_special_tokens=False)  print(""===== SHORT TEXT ====="") for token_id in encoded_short:     print(f""{str(token_id)} : '{tokenizer.decode(token_id)}'"")  print(""===== LONG TEXT ====="") for token_id in encoded_long:     print(f""{str(token_id)} : '{tokenizer.decode(token_id)}'"") If you run the code I‚Äôve provided above, you‚Äôll see the following result: What confuses me about this result is the fact that the same word (‚Äútokenize‚Äù in this case), is tokenized differently at different points of the text. Specifically, the token ‚Äútoken‚Äù seems to have two corresponding token ids. Here are some questions I have regarding the above: Is it normal that the same token is mapped to multiple token_ids? In the case that it IS normal, is there any way to know which token_id the token will be mapped to without having to encode the text? In the case that it IS NOT normal, would you recommend that I use a different model? The problem is that I‚Äôm trying to create ‚Äúlabels‚Äù for a particular dataset by masking up to a specific index of the ‚Äúinput_ids‚Äù returned by the tokenizer. The code I am working on right now determines that specific index by searching for a specific sequence of tokens (the token_ids that correspond to a specific part of my input prompt). If the tokenizer I am using is not consistent in the way that it encodes the same tokens, it would probably lead to failures with this particular approach. I would appreciate any guidance, and thank you for reading!","Apr 18, 2024 2:56 am","Chahnwoo","Hello, I‚Äôm hoping to re-raise the question because I haven‚Äôt made any progress towards figuring this particular phenomenon out. The HuggingFace guide on tokenizers seems to imply that tokenizers are expected to be consistent, which is not what I am experiencing.  Summary of the tokenizers by HuggingFace If anyone even knows whether the phenomenon pointed out in the initial post is normal or abnormal, please let me know!","Apr 22, 2024 12:22 am","RaushanTurganbay","Hey! This is because of the special token used by SentencePiece to mark the whitespace or the beginning of the word. You can check here for more about how sentence piece algorithm works In the case of ‚Äútoken‚Äù, those are actually two different tokens in str format and you can see that by converting ids to tokens. When you used the .decode() method it drops underlines. >>> from transformers import AutoTokenizer >>> tokenizer = AutoTokenizer.from_pretrained('TeamUNIVA/Komodo_7B_v1.0.0') >>> tokenizer.convert_ids_to_tokens(6029) '‚ñÅtoken' >>> tokenizer.convert_ids_to_tokens(5263) 'token'","Apr 22, 2024 7:35 am","Chahnwoo","Thank you for that! I hadn‚Äôt taken that into consideration but that makes a lot of sense. I assume that is also part of why the same word may be tokenized differently at different points of a text.","Apr 22, 2024 8:12 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"All my sequences get tokenized the same","https://discuss.huggingface.co/t/all-my-sequences-get-tokenized-the-same/14657","2","598","Created: Feb 12, 2022 5:16 pm Latest: Feb 12, 2022 5:24 pm","timaie","I‚Äôm using ProtBert models (GitHub - agemagician/ProtTrans: ProtTrans is providing state of the art pretrained language models for proteins. ProtTrans was trained on thousands of GPUs from Summit and hundreds of Google TPUs using Transformers Models.) for protein embeddings. When using the BertTokenizer (and similarly for T5Tokenizer), all my sequences get tokenized to [2, 1, 3]. Does anyone know what‚Äôs going wrong? from transformers import BertTokenizer  tokenizer = BertTokenizer.from_pretrained(""Rostlab/prot_bert"", do_lower_case=False) sequences = ['MEGGGKPNSASNSRDDGNSVYPSKAPATGPAAAD', 'MQLKAKEELLRNMELGLIPDQEIRQLIRVE', 'MTVSTSKTPKKNIKYTLTHTLQKWKETLKKITHETLSSI']  tokenizer(sequences) gves a result like {'input_ids': [[2, 1, 3], [2, 1, 3], [2, 1, 3]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1]]}","Feb 12, 2022 5:16 pm","beneyal","Hi there! You need to use spaces between the string characters. [2, 1, 3] is [CLS] [UNK] [SEP]. With spaces, it looks better >>> tokenizer(' '.join(list(""MEGGGKPNSASNSRDDGNSVYPSKAPATGPAAAD"")))  {'input_ids': [2, 21, 9, 7, 7, 7, 12, 16, 17, 10, 6, 10, 17, 10, 13, 14, 14, 7, 17, 10, 8, 20, 16, 10, 12, 6, 16, 6, 15, 7, 16, 6, 6, 6, 14, 3],  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}","Feb 12, 2022 5:22 pm","timaie","I just also realized that, thanks anyway!","Feb 12, 2022 5:24 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with tokenizer.tokenize","https://discuss.huggingface.co/t/issue-with-tokenizer-tokenize/1891","3","500","Created: Nov 4, 2020 1:37 pm Latest: Nov 16, 2020 7:23 pm","xiakun","when I test tokenizer.tokenize(‚Äòhow do you do‚Äô) ÔºàRobertaTokenizer in pytorch_transformers.tokenization_roberta.py Ôºâ, it returns [‚Äòhow‚Äô, ‚Äòƒ†‚Äô, ‚Äòdo‚Äô, ‚Äòƒ†‚Äô, ‚Äòyou‚Äô, ‚Äòƒ†‚Äô, ‚Äòdo‚Äô], wants to know where is the wrong","Nov 4, 2020 1:37 pm","Karthik12","There is some discussion in this therad and this. Perhaps it helps?","Nov 5, 2020 8:19 am","facehugger2020","See also this post in the forum.","Nov 16, 2020 7:21 pm","facehugger2020","where is the wrong That‚Äôs a new one. I haven‚Äôt seen that expression before. LOL","Nov 16, 2020 7:23 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Inputs.word_ids() length not matching word label length","https://discuss.huggingface.co/t/inputs-word-ids-length-not-matching-word-label-length/58976","3","498","Created: Oct 18, 2023 11:18 am Latest: Mar 22, 2024 8:28 am","enicholsonbmj","Hi there, I‚Äôm having trouble with the tokenizer word_ids referring to words not in the index. To elaborate, here‚Äôs code and error message: from transformers import AutoTokenizer model_checkpoint = ‚Äúbionlp/bluebert_pubmed_uncased_L-24_H-1024_A-16‚Äù tokenizer = AutoTokenizer.from_pretrained(model_checkpoint) tokenized_column_list =  word_id_list =  input_id_list =  token_type_id_list =  attention_mask_list = for index, row in ta_train_df.iterrows(): text = row[‚ÄòRaw text‚Äô].lower() inputs = tokenizer(text) tokenized_column_list.append(inputs.tokens()) word_id_list.append(inputs.word_ids()) input_id_list.append(inputs[‚Äòinput_ids‚Äô]) token_type_id_list.append(inputs[‚Äòtoken_type_ids‚Äô]) attention_mask_list.append(inputs[‚Äòattention_mask‚Äô]) ta_train_df[‚ÄòTokenized_text‚Äô] = tokenized_column_list ta_train_df[‚ÄòWord_ID‚Äô] = word_id_list ta_train_df[‚Äòinput_ids‚Äô] = input_id_list ta_train_df[‚Äòtoken_type_ids‚Äô] = token_type_id_list ta_train_df[‚Äòattention_mask‚Äô] = attention_mask_list label_to_numeric = {‚ÄòO‚Äô:0, ‚Äò-‚Äô: -100, ‚ÄòB-rx‚Äô:1‚Ä¶ etc def convert_labels_to_numeric(labels): numeric_labels =  for label in labels: if label in label_to_numeric: numeric_labels.append(label_to_numeric[label]) else: numeric_labels.append(-100) return numeric_labels ta_train_df[‚Äònumeric_iob_tags‚Äô] = ta_train_df[‚Äòiob_tags‚Äô].apply(convert_labels_to_numeric) def align_labels_with_tokens(labels, word_ids): new_labels =  current_word = None for word_id in word_ids: if word_id != current_word: # Start of a new word! current_word = word_id label = -100 if word_id is None else labels[word_id] new_labels.append(label) elif word_id is None: # Special token new_labels.append(-100) else: # Same word as previous token label = labels[word_id] # If the label is B-XXX we change it to I-XXX if label % 2 == 1: label += 1 new_labels.append(label) return new_labels labels = ta_train_df[‚Äònumeric_iob_tags‚Äô][0] word_ids = ta_train_df[‚ÄòWord_ID‚Äô][0] align_labels_with_tokens(labels, word_ids) IndexError Traceback (most recent call last) in <cell line: 6>() 4 print(labels) 5 print(word_ids) ----> 6 align_labels_with_tokens(labels, word_ids) in align_labels_with_tokens(labels, word_ids) 7 current_word = word_id 8 print(current_word) ----> 9 label = -100 if word_id is None else labels[word_id] 10 new_labels.append(label) 11 elif word_id is None: IndexError: list index out of range The input.word_ids are referring to word 356, whereas there are only 352 words in the labels. I‚Äôm not sure where the tokenizer is producing 4 extra words. The text in df_train_df[‚ÄòRaw text‚Äô][0] is this: 'acute. confirmed CES. 1. DECOMPRESSION SURGERY Decompression surgery should be performed as soon as possible. Although surgery within 48 hours of symptom onset has been used by some clinicians as a guide, this has been challenged, and remains controversial. It is likely that the level of neurologic dysfunction at the time of surgery (rather than time since symptom onset) is the most significant determinant of prognosis. One retrospective cohort study of 20,924 patients with CES reported that patients undergoing surgical decompression on hospital day 0 or 1 had better improved inpatient outcomes, including lower complication and mortality rates, than patients having surgery on day 2 or later. Evidence on the benefits of earlier surgery (e.g., within 24 hours) is equivocal. This may be due to differences in neurologic dysfunction among participants; some studies suggest that surgery within 24 hours of symptom onset may reduce postoperative bladder dysfunction in patients with incomplete CES, but not in patients with CES with urinary retention, compared with surgery between 24 and 48 hours.  Therefore, as the 48-hour time window is controversial, urgent surgery should not be delayed, especially since the precise time of symptom onset can be difficult to define. British Association of Spine Surgeons guidelines recommend that surgery should take place as soon as possible, while taking into account the duration and clinical course of symptoms and signs, as well as the potential for increased morbidity when operating at night.  The goal of surgery is to alleviate compression of the cauda equina, which may be achieved through a number of surgical techniques (e.g., wide-decompressive laminectomy, lumbar microdiskectomy). The appropriate surgical technique should be chosen based on pathology and the experience of the surgeon.  Intraoperative monitoring of somatosensory and motor-evoked potentials allows for evaluation of radiculopathy and neuropathy, but is not a necessary part of urgent procedures.'","Oct 18, 2023 11:18 am","enicholsonbmj","I think I‚Äôve figures this out. It‚Äôs due to special characters in the raw text, such as ‚Äò~~~‚Äô, ‚Äò{{‚Äô, ‚Äò}}‚Äô, ‚Äò|‚Äô , ‚Äò20,357‚Äô, etc. It turn out my text labels and the tokenizers handle these cases differently - where the tokenizer separates them and counts them individually, but my labels don‚Äôt.","Oct 18, 2023 12:18 pm","mokshmalik5757","Hi Have you been able to solve this problem? Did you write a new function, or created a new tokenizer?","Mar 19, 2024 12:03 pm","enicholsonbmj","Hi, I didn‚Äôt create a new function or tokenizer, I just cleaned my raw text a bit better to get rid of the special characters","Mar 22, 2024 8:28 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprocessing raw text","https://discuss.huggingface.co/t/preprocessing-raw-text/24559","2","571","Created: Oct 17, 2022 1:51 pm Latest: Oct 26, 2022 3:21 pm","antoine2323231","When fitting a custom tokenizer, I am preprocessing raw text. I remove punctuation, but then there is no indication of the end/start of a sentence, therefore if I keep the ‚Äú.‚Äù as an example that would be understood by the tokenizer model? Do you have any suggestions?","Oct 17, 2022 1:51 pm","lianghsun","Hey bro, have you tried TemplateProcessing ? You should add the following code: tokenizer.post_processor = TemplateProcessing(     single=""[BOS] $0 [EOS]"",     special_tokens=[         (""[BOS]"", tokenizer.token_to_id(""[BOS]"")),         (""[EOS]"", tokenizer.token_to_id(""[EOS]""))     ] Please note that you may need to add [PAD] through tokenizer.enable_padding().","Oct 25, 2022 7:42 pm","antoine2323231","Hey bro, Thanks a lot! Gonna try this out.","Oct 26, 2022 3:21 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BART Tokenizer tokenises same word differently?","https://discuss.huggingface.co/t/bart-tokenizer-tokenises-same-word-differently/21835","1","694","Created: Aug 19, 2022 4:53 pm Latest: Aug 24, 2022 2:43 pm","AndreaSottana","Hello, I have noticed that if I tokenize a full text with many sentences, I sometimes get a different number of tokens than if I tokenise each sentence individually and add up the tokens. I have done some debugging and have this small reproducible example to show the issue from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn')  print(tokenizer.tokenize(""Thames is a river"")) print(tokenizer.tokenize(""We are in London. Thames is a river"")) I get the following output ['Th', 'ames', 'ƒ†is', 'ƒ†a', 'ƒ†river'] ['We', 'ƒ†are', 'ƒ†in', 'ƒ†London', '.', 'ƒ†Thames', 'ƒ†is', 'ƒ†a', 'ƒ†river'] I would like to understand why the word Thames has been split into two tokens when it‚Äôs at the start of sequence, whereas it‚Äôs a single word if it‚Äôs not at the start of sequence. I have noticed this behaviour is very frequent and, assuming it‚Äôs not a bug, I would like to understand why the BART tokeniser behaves like this. Many thanks","Aug 19, 2022 4:53 pm","AndreaSottana","UPDATE: For those interested, the reply to this question has now been provided by a StackOverflow user here","Aug 24, 2022 2:43 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WordPiece tokenizer doesn‚Äôt work for long sequences","https://discuss.huggingface.co/t/wordpiece-tokenizer-doesnt-work-for-long-sequences/27391","1","387","Created: Dec 7, 2022 12:08 pm Latest: Mar 28, 2024 1:58 pm","dotan1111","I‚Äôm trying to train a WordPiece (WPC) tokenizer from HuggingFace on long sequences. I know the tokenizer is created successfully by looking at the saved file. When I‚Äôm trying to encode a new sequences, the tokenizer return only unknow tokens. Once shortening the sequence, the tokenizer return a valid tokens. When encoding the same sequences with a different type of tokenizer (Unigram or BPE) the tokenizers returns valid results. I‚Äôm not getting any errors or warning from the library.","Dec 7, 2022 12:08 pm","lkurlandski","I have encountered the exact same issue with tokenizers==0.14.1.","Mar 28, 2024 1:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer is not being loaded on Huggingface Inference","https://discuss.huggingface.co/t/tokenizer-is-not-being-loaded-on-huggingface-inference/23505","0","966","Created: Sep 22, 2022 7:30 pm","ritwikm","I was using this to finetune a GPT medium. tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>') #gpt2-medium configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False) model = GPT2LMHeadModel.from_pretrained(""gpt2"", config=configuration) model.resize_token_embeddings(len(tokenizer))  # after finetuning  model.save_pretrained(output_dir) tokenizer.save_pretrained(output_dir) But when I upload the model and tokenizer on the model hub (ritwikm/gandhi-gpt), I see the following error Can't load tokenizer using from_pretrained, please update its configuration: No such file or directory (os error 2) On my local machine, I am loading the same tokenizer and model using the following lines: model = model.from_pretrained(output_dir).to(device) tokenizer = tokenizer.from_pretrained(output_dir) And it works fine. It just fails on the inference. I have tried many solutions like using only those tokenizer files which are available in the official repo (gpt2). I saw the post by bala1802, even that didn‚Äôt help. What can be done?","Sep 22, 2022 7:30 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Best way to get the closest token indices of input of char_to_token is a whitespace","https://discuss.huggingface.co/t/best-way-to-get-the-closest-token-indices-of-input-of-char-to-token-is-a-whitespace/32004","0","964","Created: Feb 19, 2023 3:44 am","SantoshGupta","For many text datasets, often the character spans start or end at a whitespace, and if you put that into char_to_token , it will return None since the whitespace does not correspond to a token. Is there a way to get the closest token on either side of the white space? Or an indicator if the character span is out of range [in cases where chunking and striding where the character span may not be in the overflow tokens], I tried to see if there‚Äôs some variable in the class or function that I can play around with in the source code github.com huggingface/transformers/blob/v4.26.1/src/transformers/tokenization_utils_base.py#L537         raise ValueError(""token_to_chars() is not available when using Python based tokenizers"")     if token_index is not None:         batch_index = batch_or_token_index     else:         batch_index = 0         token_index = batch_or_token_index     span_indices = self._encodings[batch_index].token_to_chars(token_index)                return CharSpan(*span_indices) if span_indices is not None else None            def char_to_token(     self, batch_or_char_index: int, char_index: Optional[int] = None, sequence_index: int = 0 ) -> int:     """"""     Get the index of the token in the encoded output comprising a character in the original string for a sequence     of the batch.                Can be called as:                - `self.char_to_token(char_index)` if batch size is 1     - `self.char_to_token(batch_index, char_index)` if batch size is greater or equal to 1 But it seems that it goes into the RUST part of the tokenizers library. My work around is to convert every character in the span to token indices and then take the max and min.","Feb 19, 2023 3:44 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Load SentencePieceBPETokenizer in TF","https://discuss.huggingface.co/t/load-sentencepiecebpetokenizer-in-tf/17257","0","956","Created: Apr 27, 2022 9:07 pm","jbmaxwell","I have a tokenizer trained using SentencePieceBPETokenizer from the tokenizers library. tokenizer = SentencePieceBPETokenizer(add_prefix_space=True)  # Customize training tokenizer.train(     files=paths,      vocab_size=4000,      min_frequency=2,      show_progress=True,      special_tokens=special_tokens, )  # Saving model tokenizer.save(""sp/tokenizer.json"") It‚Äôs the first one I‚Äôve trained that really fits the task (in particular because it fills the requested vocab_size ), so I‚Äôd like to use it everywhere I can. I‚Äôm wondering whether it‚Äôs possible to load the tokenizer.json in the SentencepieceTokenizer from tensorflow-text (i.e., text.SentencepieceTokenizer())? Or, can it be loaded with PreTrainedTokenizerFast, then converted to the TF format? Obviously the vocab and merges are the crucial parts I want to utilize in TF. Thanks in advance.","Apr 27, 2022 9:07 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How truncation works when applying BERT tokenizer on the batch of sentence pairs in HuggingFace?","https://discuss.huggingface.co/t/how-truncation-works-when-applying-bert-tokenizer-on-the-batch-of-sentence-pairs-in-huggingface/17977","0","924","Created: May 15, 2022 1:26 pm","AbuUbaida","Say, I have three sample sentences: s0 = ""This model was pretrained using a specific normalization pipeline available here!"" s1 = ""Thank to all the people around,"" s2 = ""Bengali Mask Language Model for Bengali Language"" I could make a batch like: batch = [[s[0], s[1]], [s[1], s[2]]] Now, if I apply the BERT tokenizer on the sentence pairs, it truncates the sentence pairs if the length exceeds in such a way that the ultimate sum of the sentence pairs‚Äô lengths meets the max_length parameter, which was supposed to be done, okay. Here‚Äôs what I meant: tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"") model = AutoModelForPreTraining.from_pretrained(""bert-base-uncased"")  encoded = tokenizer(batch, padding=""max_length"", truncation=True, max_length=10)[""input_ids""] decoded = tokenizer.batch_decode(encoded) print(decoded)  >>>Output: ['[CLS] this model was pre [SEP] thank to all [SEP]', '[CLS] thank to all [SEP] bengali mask language model [SEP]'] My question is, how does the truncation work here in the pair of sentences where the number of tokens from each sentence of each pair is not equal? For example, in the first example output '[CLS] this model was pre [SEP] thank to all [SEP]' number of tokens from the two sentences has not come equally i.e [CLS] 4 tokens [SEP] 3 tokens [SEP] .","May 15, 2022 1:26 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer taking extremely long time to train","https://discuss.huggingface.co/t/tokenizer-taking-extremely-long-time-to-train/1875","0","905","Created: Nov 3, 2020 4:29 pm","wjs20","I am trying to tokenize some amino acid sequences using a BPE tokenizer, but it hasn‚Äôt finished training after several hours. There are ~500,000 sequences in a .txt file formatted like so: MKTLLLTLVVVTIVCLDLGYTLKCHNTQLPFIYNTCPEGKNLCFKATLKFPLKFPVKRGCAATCPRSSSLVKVVCCKTDKCN MALFRKKDKYIRINPNRSRIESAPQAKPEVPDELFSKCPACKVILYKNDLGLEKTCQHCSYNFRITAQERRALTVDEGSFEELFTGIET ADNRRPIWNLGHMVNALKQIPTFLXDGANA I have included the code I used below ##Import Data  import pandas as pd import numpy as np  url = 'https://www.uniprot.org/uniprot/?query=reviewed:yes&format=tab&columns=sequence'  seqs = pd.read_csv(url)  seqs_arr = seqs.to_numpy()  #take 1% sample of larger dataset for testing  n_rows = seqs_arr.shape[0] rand_ind = np.random.choice(n_rows, size=5000, replace=False)  #seqs_arr_small = seqs_arr[rand_ind, :].shape  np.savetxt('seqs_arr_small.txt', seqs_arr_small, fmt='%s')  ## Train tokenizer  # We won't need TensorFlow here !pip uninstall -y tensorflow # Install `transformers` from master !pip install git+https://github.com/huggingface/transformers !pip list | grep -E 'transformers|tokenizers' # transformers version at notebook update --- 2.11.0 # tokenizers version at notebook update --- 0.8.0rc1  %%time  from pathlib import Path  from tokenizers import ByteLevelBPETokenizer  paths = 'seqs_arr_small.txt'  # Initialize a tokenizer tokenizer = ByteLevelBPETokenizer()  # Customize training tokenizer.train(files=paths, vocab_size=52000, min_frequency=3, special_tokens=[     ""<s>"",     ""<pad>"",     ""</s>"",     ""<unk>"",     ""<mask>"", ])","Nov 3, 2020 4:29 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why multilingual BERT tokenizer doesn‚Äôt remove accent markers?","https://discuss.huggingface.co/t/why-multilingual-bert-tokenizer-doesnt-remove-accent-markers/8468","0","896","Created: Jul 18, 2021 7:25 am Latest: Jul 18, 2021 7:27 am","xuxuxu","Hi everyone I‚Äôm wondering why the pretrained ‚Äòbert-base-multilingual-cased‚Äô tokenizer cannot remove accent markers? To reproduce: from transformers import AutoTokenizer, AutoModel tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased') tokenizer.convert_ids_to_tokens(tokenizer.encode('r√©sum√©')) and the result gives ['[CLS]', 'r', '##√©s', '##um', '##√©', '[SEP]‚Äô] However, the document says ‚ÄúWhen calling encode() or encode_batch(), the input text(s) go through the following pipiline: Normalization, Pre-Tokenization, The Model, Post-Processing‚Äù in this page: The tokenization pipeline ‚Äî tokenizers documentation. The Normalization includes stripping whitespace, removing accented characters or lowercasing all text, which is also mentioned in the BERT GitHub README: bert/multilingual.md at master ¬∑ google-research/bert ¬∑ GitHub. According to the huggingface tokenizer introduction, we can evaluate as follows from tokenizers import normalizers  from tokenizers.normalizers import NFD, StripAccents  normalizer = normalizers.Sequence([NFD(), StripAccents()]) normalizer.normalize_str('r√©sum√©') and the result gives: 'resume‚Äô Thus, it seems to me that tokenizer does not perform accent removal. What‚Äôs happening here? I‚Äôm quite confused. Please help clarify this problem.","Jul 18, 2021 7:25 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Show Submodels of PegasusTokenizer","https://discuss.huggingface.co/t/show-submodels-of-pegasustokenizer/17286","1","629","Created: Apr 28, 2022 12:33 pm Latest: Apr 28, 2022 1:01 pm","rmeier","Hi, for understanding the process of the PegasusTokenizer, I would like to print the different outputs of the Tokenizer after each submodule (Normalization, Pre Tokenization and so on. I used the google/pegasus-cnn_dailymail model with the PegasusTokenizer. Is there s way to show on one data example how the Tokenizer transforms the input text to the output integer sequences? I would like to see the transformation step wise for each step the Tokenizer is performing? It would be great if somebody can point me in one direction because it is very urgent. Thanks Ralf","Apr 28, 2022 12:33 pm","rmeier","I have some ideas but I don‚Äôt think this is the right solution? import sentencepiece as spm  s = spm.SentencePieceProcessor(tokenizer.vocab_file) str(s.encode(sample_text,  out_type=str, enable_sampling=True, alpha=0.1, nbest_size=-1))[:1000]","Apr 28, 2022 1:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer vs Model","https://discuss.huggingface.co/t/tokenizer-vs-model/93689","0","160","Created: Jun 24, 2024 2:47 am","NaimaVahab","From what I have understood from the hf-tutorial is that we should use a pretrained model with its own tokenizer for a good performance. My doubt is that, BERT uses wordpiece but RoBERTA (again BERT architecture) uses BPE as tokenization approaches. Can we mix match any model and tokenizer if we are pretraining the model from scratch, like in the case of RoBERTA ? In that case, can I pretrain the BERT/DistiBert model from scratch using BPE/Unigram tokenizer? Is the rule of using same model with same tokenizer applicable on finetuning or inference purpose only ? Or is the architecture of each model itself is related to the tokenizing approach ? I am trying to train a distilbert using unigram approach.","Jun 24, 2024 2:47 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error when doing tokenization","https://discuss.huggingface.co/t/error-when-doing-tokenization/17344","0","882","Created: Apr 29, 2022 8:23 pm Latest: Apr 29, 2022 8:28 pm","JoaoCP","Hello, I have a dataset with various documents and this documents are in a jsonl file, and each document is an entry with an id and contents fields. The contents field contains all the information about the document. This information is distributed in 5 fields separated by ‚Äò\n‚Äô. For instance: {‚Äúid‚Äù: ‚ÄúNCT03538132‚Äù, ‚Äúcontents‚Äù: ‚ÄúPatients‚Äô Perception on Bone Grafts\nPatients‚Äô Perception on Bone Biomaterials Used in Dentistry : a Multicentric Study\nThe goal of this study is to collect the patients‚Äô opinion about the different types of bone graft, to assess which are the most rejected by the patients and if the demographic variables (such as the gender or the age) and the level of education influence their decision.\nNowadays, many procedures may need regenerative techniques. Some studies have already assessed the patients‚Äô opinion regarding soft tissue grafts, some investigators have centered their studies on the techniques‚Äô efficiency without assessing the patient‚Äôs perception.\nInclusion criteria: - Adult (18 years old or more) - Able to read and write - Not under the influence of alcohol or drugs - Had not previously undergone any surgery involving bone graft or bone augmentation. Exclusion criteria: - Any patient who doesn‚Äôt fullfill the inclusion criterias.‚Äù} For each document I would like to know how many tokens are generated for each field and plot a distribution of the number of tokens with the respect to the field. I only can get the tokens for 66% of my dataset, after that I get this error: line 145, in inputs = tokenizer( File ‚Äú/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py‚Äù, line 2413, in call return self.batch_encode_plus( File ‚Äú/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py‚Äù, line 2598, in batch_encode_plus return self._batch_encode_plus( File ‚Äú/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py‚Äù, line 439, in _batch_encode_plus for key in tokens_and_encodings[0][0].keys(): IndexError: list index out of range I saved the documents that are breaking in a log file and tried to run again with a few documents and it breaks it some documents. I tried to change the other of the documents, the ones breaking I put them in the beginning and it worked for these but then it breaks in other documents. Can anyone help me? This is my code: collection_iterator = JsonlCollectionIterator(f'{ct_utils.paths[""input_dir""]}/documents.jsonl', fields=   [""brief_title"", ""official_title"", ""brief_summary"", ""detailed_description"", ""criteria""])  fields_tokens = {                 ""brief_title"": [],                  ""official_title"": [],                 ""brief_summary"": [],                 ""detailed_description"": [],                 ""criteria"": []             }  log_file = open('documents_log.txt', 'w')  for index, batch_info in enumerate(collection_iterator(1, 0, 1)):          for field in collection_iterator.fields:          # try:         inputs = tokenizer(             batch_info[field],             padding='longest',             truncation=False,             add_special_tokens=True,             return_tensors='pt'         )         # except:         #     log_file.write(batch_info[""id""][0] + ""\n"")          fields_tokens[field].append({'index': index + 1, 'document_id': batch_info['id'][0], 'tokens_count': inputs[""input_ids""].shape[1]})  #log_file.close()  with open(""fields_tokens.json"", ""w"") as file:     json.dump(fields_tokens, file, indent=4) I didn‚Äôt pass the max length and set the truncation to false in the tokenizer because I want it to generate the tokens for the complete text.","Apr 29, 2022 8:23 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BOS tokens for mBERT tokenizer","https://discuss.huggingface.co/t/bos-tokens-for-mbert-tokenizer/5467","1","624","Created: Apr 13, 2021 2:49 pm Latest: Apr 14, 2021 10:13 am","rahular","Default mBERT tokenizes a sentence as ['[CLS]', 'This', 'is', 'a', 'sample', 'sentence', '[SEP]']. I want to change this behaviour and add a language specific token after the CLS token like this: ['[CLS]', '__en__', 'This', 'is', 'a', 'sample', 'sentence', '[SEP]'] I know TemplateProcessing can be used to achieve this if the language token doesn‚Äôt change from tokenizers.processors import TemplateProcessing tokenizer._tokenizer.post_processor = TemplateProcessing(     single=f""{_lang_token} $A [SEP]"",     pair=f""{_lang_token} $A [SEP] $B:1 [SEP]:1"",     special_tokens=[(""[SEP]"", tokenizer.convert_tokens_to_ids(""[SEP]"")),                      (_lang_token, tokenizer.convert_tokens_to_ids(_lang_token))], ) But in my case, the language token changes with every batch. What is the best way to add these tokens? Creating TemplateProcessing objects every time seems inefficient.","Apr 13, 2021 2:49 pm","rahular","@sgugger any suggestions?","Apr 14, 2021 10:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Padding and truncation for custom tokenizer","https://discuss.huggingface.co/t/padding-and-truncation-for-custom-tokenizer/30180","1","616","Created: Jan 22, 2023 5:34 pm Latest: Jan 22, 2023 6:10 pm","re90","Hi, Is there a way to ensure that my custom tokenizer pads and tunicates my inputs? This is my code: # Tokenizer tokenizer = Tokenizer(models.WordPiece())  spl_tokens = [""[UNK]"", ""[SEP]"", ""[MASK]"", ""[CLS]"", ""[PAD]""]  # special tokens trainer = trainers.WordPieceTrainer(special_tokens = spl_tokens)  tokenizer.normalizer = normalizers.BertNormalizer(lowercase=False)  pre_tokenizer = CharDelimiterSplit("" "")  tokenizer.pre_tokenizer = pre_tokenizer  tokenizer.decoder = decoders.WordPiece()  tokenizer.train([file_path], trainer) # training the tokenzier  cls_token_id = tokenizer.token_to_id(""[CLS]"") sep_token_id = tokenizer.token_to_id(""[SEP]"") print(""[CLS] id = "", cls_token_id, "", [SEP] id = "", sep_token_id)  tokenizer.post_processor = processors.TemplateProcessing(     single=f""[CLS]:0 $0 [SEP]:0"",     special_tokens=[         (""[CLS]"", cls_token_id),         (""[SEP]"", sep_token_id),     ], )   tokenizer.save(""tokenizer-trained.json"") I can‚Äôt seem to get my model to behave like the tokenizer in this example: huggingface.co Preprocess We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.","Jan 22, 2023 5:34 pm","re90","Also, I‚Äôve tried wrapping it inside the BertTokenizerFast object and calling in in the following way: new_tokenizer = BertTokenizerFast(tokenizer_object=tokenizer, max_len=1024)  new_tokenizer.encode_plus(example_str, padding=True, truncation=True, add_special_tokens=True) It still doesn‚Äôt seem to work","Jan 22, 2023 6:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Custom PostProcessor?","https://discuss.huggingface.co/t/custom-postprocessor/25847","0","871","Created: Nov 10, 2022 6:46 am","tonyswoo","Is there a way to implement a custom post-processor? On this forum, I did see people discussing how to implement a custom pre-tokenizer and normalizer, but I haven‚Äôt seen any posts discussing the post-processor. My first guess was to subclass PostProcessor and implement num_special_tokens_to_add and process, but trying to initialize the subclass just gives a TypeError: No constructor defined. (Assuming it has something to do with the Rust-Python binding and not having a __new__ method) So, does the HuggingFace tokenizers library support custom post-processing? If so, is there any instruction or example code available? Thank you.","Nov 10, 2022 6:46 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How can I change the token id of a special token?","https://discuss.huggingface.co/t/how-can-i-change-the-token-id-of-a-special-token/13445","0","869","Created: Jan 6, 2022 3:09 pm","vitvit","I am combining a tokenizer from model A with a pretrained model B. I want to align special token ids. I am able to change the token name such as but I cannot change the token id number. How can I aproach this problem?","Jan 6, 2022 3:09 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to change the size of model_max_length?","https://discuss.huggingface.co/t/how-to-change-the-size-of-model-max-length/32942","0","863","Created: Mar 3, 2023 2:34 am","jyang18","Hello everyone, I try to use tokenizer = GPT2Tokenizer.from_pretrained('gpt2') and saw that model_max_length was 1024, then I used gpt2-medium and it was also 1024. Can the size of model_max_length be changed? If so, how do I do it? Because I always exceed the size of 1024 on my data","Mar 3, 2023 2:34 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I get the predicted token as ` ‡•á` . What am I doing wrong?","https://discuss.huggingface.co/t/i-get-the-predicted-token-as-what-am-i-doing-wrong/29038","1","609","Created: Jan 6, 2023 6:42 am Latest: Mar 27, 2023 10:17 am","sadafthegreekgod","I was trying to solve this problem We‚Äôre going to use the wikitext (link) dataset with the distilbert-base-cased (link) model checkpoint. Start by loading the wikitext-2-raw-v1 version of that dataset, and take the 11th example (index 10) of the train split. We‚Äôll tokenize this using the appropriate tokenizer, and we‚Äôll mask the sixth token (index 5) the sequence. When using the distilbert-base-cased checkpoint to unmask that (sixth token, index 5) token, what is the most probable predicted token (please provide the decoded token, and not the ID)? and I coded the solution as import torch import transformers import nlp  # Load the Wikitext-2 dataset dataset = nlp.load_dataset('wikitext', 'wikitext-2-raw-v1')  # Get the 11th example (index 10) of the train split example = dataset['train'][10]  # Load the DistilBERT model and tokenizer model = transformers.DistilBertModel.from_pretrained('distilbert-base-cased') tokenizer = transformers.DistilBertTokenizer.from_pretrained('distilbert-base-cased')  # Tokenize the example input_ids = tokenizer.encode(example['text'], return_tensors='pt')  # Mask the sixth token (index 5) in the sequence masked_input_ids = input_ids.clone() masked_input_ids[:, 5] = tokenizer.mask_token_id  # Use the model to predict the most probable token for the masked token output = model(masked_input_ids)[0] prediction_scores, prediction_indexes = output[:, 5, :].max(dim=-1) # prediction_scores, prediction_indexes = output[:, 5, :].max(dim=-1)  # Decode the predicted token ID to obtain the actual token predicted_token = tokenizer.decode(prediction_indexes, skip_special_tokens=True)  # Replace the masked token with the predicted token in the input sequence decoded_input_ids = input_ids.squeeze().tolist() decoded_input_ids[5] = prediction_indexes.item() decoded_input = tokenizer.decode(decoded_input_ids, skip_special_tokens=True)  print(f'Input: {example[""text""]}') print(f'Predicted token: {predicted_token}') print(f'Decoded input: {decoded_input}') The outout I get is Predicted token: ‡•á Decoded input: The game‚Äôs ‡•á system, the BliTZ system, is carried over directly from Valkyira Chronicles. During missions, players select each unit using a top @ - @ down perspective of the What am I doing wrong here?","Jan 6, 2023 6:42 am","PaulDahab","hello , i got the same problem , did you find the solution? if yes please share","Mar 27, 2023 10:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer behaviour with pipeline","https://discuss.huggingface.co/t/tokenizer-behaviour-with-pipeline/48969","0","860","Created: Aug 1, 2023 10:49 am","LukeFiasco","Hey all, I am trying to use one of the Llama2 models to generate text (summarise docs). This means that I am dealing with long texts that need to be truncated. Upon using the Pipeline API I have discovered some weird behaviour. tokenizer = AutoTokenizer.from_pretrained(   local_model_dir,    padding_side=""right"",    model_max_length=MAX_LENGTH,    truncation=True )  # error in source, context should be 4k upldated_config = LlamaConfig(max_position_embeddings=MAX_POS_EMBEDDINGS)  # 8 bit precision to fit GPU -> maybe try 4 bits? model = AutoModelForCausalLM.from_pretrained(     local_model_dir,     torch_dtype=torch.float16,     trust_remote_code=True,     device_map=""auto"",     load_in_8bit=True,     config=upldated_config )  pipeline = transformers.pipeline(     ""text-generation"",     model=model,     tokenizer=tokenizer,     return_full_text=False, ) it seems to me that when I call pipeline(long_input) the input text is not correctly truncated. This is checked by calling pipeline.pre_process(long_input) which results in the following message: Token indices sequence length is longer than the specified maximum sequence length for this model (12351 > 3896). I have come to understand that truncation is only done when calling tokenizer(long_input, truncation=True, max_length=MAX_LENGTH) However when i try to pass truncation to the Pipeline like this pipeline(long_input, truncation=True). It also throws as error: The following model_kwargs are not used by the model: [‚Äòtruncation‚Äô] (note: typos in the generate arguments will also show up in this list) So this all begs the question: How do i make truncation happen while using a pipeline? To me it seems inuitive that when Truncation=True while initiating the tokenizer, and adding it to the pipeline. All downstream texts should be tokenized.","Aug 1, 2023 10:49 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why we need to add special tokens to tasks other than classification?","https://discuss.huggingface.co/t/why-we-need-to-add-special-tokens-to-tasks-other-than-classification/11975","0","854","Created: Nov 17, 2021 3:20 pm","shon711","Hello, while tokenizing, is there any reason to add special tokens (<s>, </s>) if the task is NOT text/sentence classification ? I am using RoBERTa model to have tokens embedding. I will use these embedding later to my specific task. More generally, I have text longer than 512 tokens and I need the embedding for each token. Thus I am splitting the text to segments where each segment is up to 512 tokens. Is there any reason to add to each segment the models special tokens? Thanks.","Nov 17, 2021 3:20 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"No PreTrainedTokenizerFast for Deberta-V3, no doc_stride","https://discuss.huggingface.co/t/no-pretrainedtokenizerfast-for-deberta-v3-no-doc-stride/20345","0","842","Created: Jul 13, 2022 4:44 pm","numomcmc","Hi All I would like to fine-tune a deberta-v3 pytorch model for Squad-v2 and then other downstream Q&A tasks. The problem is that the doc_stride option in my arg is causing the following error: NotImplementedError: return_offset_mapping is not available when using Python tokenizers.To use this feature, change your tokenizer to one deriving from transformers.PreTrainedTokenizerFast.More information on available tokenizers at https://github.com/huggingface/transformers/pull/2674 because there is no PreTrainedTokenizerFast for deberta-v3 yet. So‚Ä¶ Can I use deberta-v2 PreTrainedTokenizerFast instead? I would like to think that just because v3 switched to ELECTRA, that change may not be affected by the tokenizers, so maybe I can get away with the v2 tokenizer? Is this just wishful thinking? Also, just so happens that v3 only has ‚Äúbase‚Äù, ‚Äúlarge‚Äù and ‚Äúxsmall‚Äù, while v2 only has all the other sizes‚Ä¶ I would suppose that because of the vocab size difference, token indices and embeddings will be different across different model sizes. That just sounds like a recipe for disaster if I mix and match them‚Ä¶ Any suggestions on how to proceed is much appreciated! SteX","Jul 13, 2022 4:44 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizing Float Tensor?","https://discuss.huggingface.co/t/tokenizing-float-tensor/30604","0","842","Created: Jan 28, 2023 4:14 am","refreshed","Forgive if this is naive, I am somewhat new to transformers. I am exploring a paper (see here) that implements an end-to-end sign language translation model. They propose that instead of converting sign language to intermediate glosses (form of written language) to then pass into seq2seq transformer (mbart), they use a ‚Äòdense gloss‚Äô representation. This dense representation is simply the output of a two layer MLP, and is just a sequence of floats (shape (batch_size, seq_length, 1024)). From my limited understanding, tokenization is a process specifically for text (sequence of ints), to produce an encoded version of the text. So how does do this process for floats? Does one just convert these floats into ints (e.g. sum each seq)?","Jan 28, 2023 4:14 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to save a tokenizer only consisting of added tokens","https://discuss.huggingface.co/t/how-to-save-a-tokenizer-only-consisting-of-added-tokens/17825","0","829","Created: May 11, 2022 2:25 pm","Gback","So I want to create a tokenizer, that instead of learning the tokens takes a list of predifinied tokens (in my case DNA dimers (‚ÄúAT‚Äù, ‚ÄúAG‚Äù,‚ÄúAC‚Äù,‚ÄúAT‚Äù‚Ä¶) and so on. This works for the BertWordPieceTokenizer from tokenizers import BertWordPieceTokenizer  # Initialize an empty BERT tokenizer tokenizer = BertWordPieceTokenizer(   clean_text=False,   handle_chinese_chars=False,   strip_accents=False,   lowercase=False,   unk_token=""[UNK]"",   sep_token=""[SEP]"",   cls_token=""[CLS]"",   mask_token=""[MASK]"",   pad_token=""[PAD]""    )  tokenizer.add_tokens([""AA"",""AT"",""AG"", ""AC"",""TA"", ""TT"", ""TG"", ""TC"", ""GA"", ""GT"", ""GG"", ""GC"", ""CA"", ""CT"", ""CC"", ""CG""]) tokenizer.add_special_tokens(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']) howerver saving this does not work. tokenizer.save_model(‚Ä¶) creates empty files, and while tokenizer.save() creates a file, it cannot be reloaded due to TypeError: sep_token not found in the vocabulary Is there a better suited tokenizer class/ other way of saving/loading tokenizer? tokenizers version 0.10.1 transformers version 4.16.2","May 11, 2022 2:25 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unk_token not set after training a BPETokenizer tokenizer","https://discuss.huggingface.co/t/unk-token-not-set-after-training-a-bpetokenizer-tokenizer/60733","1","579","Created: Nov 1, 2023 8:13 pm Latest: Nov 1, 2023 10:06 pm","Emanuel","I trained a BPETokenizer with the following code: import tokenizers import transformers from tokenizers import models, pre_tokenizers, trainers, decoders, processors import datasets  if __name__ == ""__main__"":     data = datasets.load_dataset(""wmt14"", ""de-en"")     unk_token = ""<unk>""     pad_token = ""<pad>""     mask_token = ""<mask>""      tok = tokenizers.Tokenizer(models.BPE(unk_token=unk_token))     tok.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)     tok.post_processor = processors.ByteLevel()     tok.decoder = decoders.ByteLevel()      def batch_iterator():         for i in range(0, len(data), 1_000):             yield [                 "" "".join([x[""de""], x[""en""]])                 for x in data[""train""][i : i + 1_000][""translation""]             ]      trainer = trainers.BpeTrainer(vocab_size=37_000, special_tokens=[unk_token, pad_token])      tok.train_from_iterator(batch_iterator(), trainer=trainer)     tok.enable_padding(pad_id=tok.token_to_id(pad_token), pad_token=pad_token)      tok = transformers.PreTrainedTokenizerFast(tokenizer_object=tok)     tok.save_pretrained(""bpe_tok"") After training and loading the tokenizer, it correctly assigns the token for unknown tokens such as , for example: In [27]: tok.decode(tok(""ü§ó"")[""input_ids""]) Out[27]: '<unk>ÔøΩÔøΩ<unk>' But when I try to access the unk_token property, I get the following message: In [28]: tok.unk_token Using unk_token, but it is not set yet. What am I missing?","Nov 1, 2023 8:13 pm","Emanuel","The solution is to manually pass unk_token and mask_token to the PreTrainedTokenizerFast constructor:     tok = transformers.PreTrainedTokenizerFast(tokenizer_object=tok, unk_token=unk_token, mask_token=mask_token)","Nov 1, 2023 10:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to perform tokenization on an ONNX model in JS?","https://discuss.huggingface.co/t/how-to-perform-tokenization-on-an-onnx-model-in-js/17581","0","811","Created: May 6, 2022 1:06 am","msamogh","I‚Äôve exported a custom PyTorch-based Transformer model into ONNX to run it on NodeJS. However, the exported model seems to expect input_ids directly (and not raw text). Is there any way I can perform tokenization in JS?","May 6, 2022 1:06 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Make correct padding for text generation with GPT-NEO","https://discuss.huggingface.co/t/make-correct-padding-for-text-generation-with-gpt-neo/45800","0","795","Created: Jul 5, 2023 9:57 pm Latest: Jul 5, 2023 10:16 pm","junoriosity","In order to make generate text sequences with GPT-NEO, I first load all the relevant components for sequence generation for GPTNeoForCausalLM. from transformers import AutoTokenizer, GPTNeoForCausalLM import torch from torch.nn import functional as F   tokenizer = AutoTokenizer.from_pretrained(""EleutherAI/gpt-neo-125m"") model = GPTNeoForCausalLM.from_pretrained(""EleutherAI/gpt-neo-125m"") There are two ways how I can generate input_ids and attention_mask. I take the standard approach without padding inputs = tokenizer(""Hello, my dog is cute"", return_tensors=""pt"") I use padding instead tokenizer.pad_token = tokenizer.eos_token tokenizer.padding_side = 'left' tokenizer.truncation_side = 'left' no_items_for_history = 30  inputs = tokenizer.encode_plus(""Hello, my dog is cute"", max_length=no_items_for_history, padding='max_length', truncation=True, return_tensors=""pt"") Then for both approaches, I iteratively loop through everything in order generate the sequence on token at a time. input_ids = inputs['input_ids'] attention_mask = inputs['attention_mask']   for i in range(10):     if i == 0:         outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=inputs[""input_ids""])     else:         outputs = model(input_ids=new_input_ids, attention_mask=attention_mask, past_key_values=past_key_values)     loss = outputs.loss     logits = outputs.logits[:, -1, :]      logits = F.softmax(logits, dim=1)      topk_values, topk_indices = torch.topk(logits, 5)     inputs_in_topk = torch.multinomial(topk_values, num_samples=1, replacement=True)     new_input_ids = torch.gather(topk_indices, 1, inputs_in_topk)      past_key_values = outputs.past_key_values     attention_mask = torch.concat((attention_mask, torch.ones(1, 1).to(attention_mask.device)), dim=1)     input_ids = torch.concat((input_ids, new_input_ids), dim=1)   print(tokenizer.decode(input_ids.tolist()[0], skip_special_tokens=True)) Here is the problem: The starting input_ids and attention_mask for the first approach look like: input_ids = tensor([[15496,    11,   616,  3290,   318, 13779]]) attention_mask = tensor([[1, 1, 1, 1, 1, 1]]) The output looks very sensible: Hello, my dog is cute! This post is about dogs and cats However, for the second approach the starting input_ids and attention_mask look like input_ids = tensor([[50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 15496,    11,   616,  3290,   318, 13779]]) attention_mask = tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]]) and it always generates nonsense like Hello, my dog is cute pet is my pet pet pet is my dog is Question: Do you know how to make it work with padding, i.e., the second approach?","Jul 5, 2023 9:57 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to convert HuggingFace tokenizers into ONNX format?","https://discuss.huggingface.co/t/how-to-convert-huggingface-tokenizers-into-onnx-format/27272","1","540","Created: Dec 5, 2022 3:14 pm Latest: Dec 5, 2022 9:42 pm","yerma","I have faced with a problem, I am trying to convert BertClassification model into ONNX format but, I also have to carry out about preprocessing step , which means my Tokenizer also must be converted to ONNX , is it even possible ?","Dec 5, 2022 3:14 pm","lianghsun","According to Export to ONNX , I think it only allow you transform the model to ONNX format not tokenizer.","Dec 5, 2022 9:42 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Period ID in RobertaTokenizer with is_split_into_words","https://discuss.huggingface.co/t/period-id-in-robertatokenizer-with-is-split-into-words/24142","1","531","Created: Oct 8, 2022 2:23 pm Latest: Oct 27, 2022 1:06 pm","shon711","Hello, I have an issue when I using is_split_into_words flag. This is a two sentences text. The period ID is different. import spacy from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained('roberta-base', use_fast=True, add_prefix_space=True) nlp = spacy.load(""en_core_web_sm"", exclude=[""tagger"", ""parser"", ""lemmatizer"", ""ner"", ""textcat""])  text = ""This is a two sentences text. The period ID is different.""  spacy_tokenized = [tok.text for tok in nlp(text)]  print(tokenizer(text, add_special_tokens=False)['input_ids']) > [713, 16, 10, 80, 11305, 2788, 4, 20, 675, 4576, 16, 430, 4]  print(tokenizer(spacy_tokenized, add_special_tokens=False, is_split_into_words=True)['input_ids']) > [152, 16, 10, 80, 11305, 2788, 479, 20, 675, 4576, 16, 430, 479] As you can see, the period ID is different, when is_split_into_words=True the ID is 479 and 4 otherwise. Now I know that this is expected behavior, the tokenizer adds a space before each word when is_split_into_words=True so technically these are 2 different entries in the vocabulary. But I don‚Äôt need this extra space when decoding, Any workaround or suggestion to solve it? Thanks in advance, Shon","Oct 8, 2022 2:23 pm","lianghsun","Hi @shon711, I think the problem may be the Spacy, please take a look at the following result from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained('roberta-base',add_prefix_space=True)  text = ""This is a two sentences text. The period ID is different."" _text = text.split() # Replace spacy for testing  print(tokenizer(text).input_ids) print(tokenizer(_text, is_split_into_words=True).input_ids) # > the results is same! So, I think you should modify spacy_tokenized to be same format as _text, then you will get the same result","Oct 27, 2022 1:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer tend to choose added tokens first rather than token in vocab","https://discuss.huggingface.co/t/tokenizer-tend-to-choose-added-tokens-first-rather-than-token-in-vocab/63965","1","530","Created: Nov 29, 2023 8:02 am Latest: Nov 30, 2023 4:13 am","Hakase-Noonna","Hello, I am working on customizing a tokenizer. Tokenizer.json structure looks like this. {version, truncation, padding, added_tokens : [     {added_token1},     {added_token2},     ], normalizer, pre_tokenizer, post_processor, model : {..., ...,      vocab : {a : 0,                   b : 1,                   c : 2, ...}          } } So I build it up from scratch using BPE model. Below is the code. class SettingTokenizer:          @staticmethod     def set_tokenizer_and_trainer():         tokenizer = Tokenizer(models.BPE())         tokenizer.normalizer = normalizers.Sequence(             [normalizers.BertNormalizer(strip_accents=True), normalizers.Replace(""\\r\\n"", "" "")]         )         tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()         tokenizer.decoder = decoders.ByteLevel()         trainer = trainers.BpeTrainer(             vocab_size=100000,             min_frequency=10,             initial_alphabet=pre_tokenizers.ByteLevel.alphabet(),             special_tokens=[""[PAD]"", ""[UNK]"", ""[CLS]"", ""[SEP]"", ""[MASK]""],         )         return tokenizer, trainer Everything seems fine to me at first train step. Let‚Äôs say I trained a new empty tokenizer with 10,000 train data. And I got the vocab with size 300. (No added tokens yet) The next day I trained with other 5,000 train data. The new tokens are updated in added_tokens. The following image explains the logic I implemented. The problem I am having now is that loaded tokenizer seems to use tokens in added_tokens first rather than tokens in vocab. For example, a word ‚Äúrequest‚Äù is tokenized into ‚Äúrequ‚Äù and ‚Äúest‚Äù (input_ids = [1336, 400] ) But the tokenizer could use the token ‚Äúreques‚Äù,‚Äút‚Äù or ‚Äúrequest‚Äù. The input_ids would be [265,88] or [266]. As you can see the above image, the token ‚Äúrequest‚Äù is already in vocab. (‚Äúrequ‚Äù and ‚Äúest‚Äù are in the added_tokens) Why tokenizer prefer added_tokens to vocab? How can I set this tokenizer to use vocab tokens first? Thx!!","Nov 29, 2023 8:02 am","Hakase-Noonna","I find a trick. Instead of just using list of new tokens for the function add_token, Use with AddedToken class like this. at3 = AddedToken('est', single_word=True) You can use some arguments in there. I choose single_word as True. If it is False, then it would tokenize ‚ÄúWest‚Äù and ‚ÄúRequest‚Äù into [‚Äúw‚Äù, ‚Äúest‚Äù] and [‚Äúrequ‚Äù, ‚Äúest‚Äù]. But if it is True, then it would not tokenize into seperate tokens. This is not the solution, but I am good with this. GLTA!","Nov 30, 2023 4:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Version incompatibility between transformers and tokenizers","https://discuss.huggingface.co/t/version-incompatibility-between-transformers-and-tokenizers/87843","0","742","Created: May 22, 2024 1:23 pm","PeteBleackley","I keep getting the following error message.     from transformers import AutoTokenizer   File ""/home/peter/.local/lib/python3.10/site-packages/transformers/__init__.py"", line 30, in <module>     from . import dependency_versions_check   File ""/home/peter/.local/lib/python3.10/site-packages/transformers/dependency_versions_check.py"", line 41, in <module>     require_version_core(deps[pkg])   File ""/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py"", line 122, in require_version_core     return require_version(requirement, hint)   File ""/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py"", line 116, in require_version     _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)   File ""/home/peter/.local/lib/python3.10/site-packages/transformers/utils/versions.py"", line 49, in _compare_versions     raise ImportError( ImportError: tokenizers>=0.11.1,!=0.11.3,<0.13 is required for a normal functioning of this module, but found tokenizers==0.19.1. Try: pip install transformers -U or pip install -e '.[dev]' if you're working with git main I‚Äôve tried reinstalling both transformers and tokenizers, but this doesn‚Äôt help. It seems strange that, given that both transformers and tokenizers come from HuggingFace, one should be incompatible with the other. Can anyone tell me what‚Äôs wrong and how to fix it?","May 22, 2024 1:23 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to add a new token without expanding the vocabulary","https://discuss.huggingface.co/t/how-to-add-a-new-token-without-expanding-the-vocabulary/34536","0","739","Created: Mar 24, 2023 4:19 am","jingyey","Hi all, I had a question regarding making changes to the tokenizer‚Äôs vocabulary. I want to register a new token‚Äôs id in the vocab, but I don‚Äôt want to add a new token and expand the size of the vocab. Specifically, assume we are working with tokenizer from bert-base-cased. Now I want to register ‚Äúqazwsx‚Äù to id 1, which is occupied by ‚Äú[unused1]‚Äù. So I want to replace ‚Äú[unused1]‚Äù : 1 by ‚Äúqazwsx‚Äù : 1. Do you know how to achieve this? Another question is how to synchronize two tokens with the same id, instead of overwrite it as above. For example, instead of replacing ‚Äú[unused1]‚Äù : 1 by ‚Äúqazwsx‚Äù : 1, I want to keep both in the new vocab, so that both of them will be tokenized as 1. Thank you so much for the help!","Mar 24, 2023 4:19 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Option to load only tokenizer and model configuration into ‚Äútoken-classification‚Äù pipeline","https://discuss.huggingface.co/t/option-to-load-only-tokenizer-and-model-configuration-into-token-classification-pipeline/26697","0","738","Created: Nov 25, 2022 2:55 pm","Leoprat","Hi everyone I stumbled into this issue integrating Nvidia triton and a NER model I trained. For practical purpose the triton inference server is receiving the tokenized text and returning the logits. So on the client I want to use a token classification pipeline, in particular: 1 - I want to use the pipeline.preprocess() to encode the text 2 - I don‚Äôt need to use the pipeline.forward() because the triton server is doing the inference and return the logits to the client. 3 - I want to use the pipeline.postprocess() to retrieve the entites The issue with this configuration is that I must load the full model locally even though I‚Äôm not using it. I would like to load a pipeline composed only of preprocess and postprocess. Calling: token_classifier = pipeline(         ""token-classification"", model=model_checkpoint) without giving it the model (only the tokenizer config) results in an error.","Nov 25, 2022 2:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Simple Transformers Multilabelclassification","https://discuss.huggingface.co/t/simple-transformers-multilabelclassification/24568","1","520","Created: Oct 17, 2022 4:30 pm Latest: Oct 18, 2022 8:18 am","Jamesyyjj","Hi there, I am having trouble importing DistilBertForMultilabelSequenceClassification In the attached two pictures you can see I am having no issues importing the AutoModelForSequenceClassification but when I try and import the DistilBertForMultilabelSequenceClassification I get: ImportError Traceback (most recent call last) Cell In [3], line 1 ----> 1 from transformers import AutoTokenizer, DistilBertForMultilabelSequenceClassification 3 tokenizer = AutoTokenizer.from_pretrained(‚Äúbhadresh-savani/bert-base-go-emotion‚Äù) 5 model = DistilBertForMultilabelSequenceClassification.from_pretrained(‚Äúbhadresh-savani/bert-base-go-emotion‚Äù) ImportError: cannot import name ‚ÄòDistilBertForMultilabelSequenceClassification‚Äô from ‚Äòtransformers‚Äô (C:\Users\name\AppData\Roaming\Python\Python310\site-packages\transformers_init_.py)","Oct 17, 2022 4:30 pm","nielsr","Hi, the correct way to instantiate a DistilBERT model for multi-label text classification is as follows: from transformers import DistilBertForSequenceClassification  model = DistilBertForSequenceClassification.from_pretrained(""distilbert-base-uncased"", problem_type=""multi_label_classification"", num_labels=10)","Oct 18, 2022 8:18 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding tokens, but tokenizer doesn‚Äôt use them","https://discuss.huggingface.co/t/adding-tokens-but-tokenizer-doesnt-use-them/78674","1","292","Created: Mar 25, 2024 11:18 am Latest: Aug 14, 2024 12:15 pm","WonderYear1905","Hi, Im trying to add tokens to a pretrained tokenizer. First I initialized the tokenizer: tokenizer = AutoTokenizer.from_pretrained(‚Äúmistralai/Mistral-7B-Instruct-v0.2‚Äù) Next I created a training iterator and trained a new tokenizer: training_corpus = get_training_corpus() new_tokenizer = tokenizer.train_new_from_iterator(training_corpus,vocab_size=10000)#,10000) Taking the diff: tokens_to_add = list(set(new_tokenizer.vocab.keys())- set(tokenizer.vocab.keys())) output = tokenizer.add_tokens(tokens_to_add) tokenizer is updated, I can see its now in the correct size (i.e original size=32000+ added tokens), I can also see the new token under added_tokens_decoder and added_tokens_encoder. Everything seems great. But, when Im trying to tokenize my input data: tokenizer.tokenize(x, return_tensors=‚Äúpt‚Äù) The tokenizer just doesnt use the new tokens. Any idea what Im doing wrong?! Thanks!","Mar 25, 2024 11:18 am","Butanium","Just experienceed the same issue with llama2-7b, did you find a solution to that? In my case adding normalized=False fixed the issue I had on my single example.","Aug 14, 2024 11:57 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Trained tokenizer API as PretrainedTokenizer","https://discuss.huggingface.co/t/trained-tokenizer-api-as-pretrainedtokenizer/23639","1","520","Created: Sep 26, 2022 4:35 pm Latest: Oct 26, 2022 9:21 am","AfonsoSousa","Hi. I have trained a BPE Tokenizer successfully using:  tokenizer = Tokenizer(BPE(unk_token=unk_token))  trainer = BpeTrainer(special_tokens=special_tokens)  tokenizer.train(text_files, trainer) But I lack things like tokenizer.pad_token to access the padding token or len(tokenizer) to get the vocabulary size. Is this the best way to train a BPE tokenizer? How can I get the same API as PretrainedTokenizer? Thanks in advance for any help you can provide.","Sep 26, 2022 4:35 pm","lianghsun","Not pretty sure what questions you met, so I just explain how to deal with padding and get vocab size here: Padding: the reason we set [PAD] in training phase is to tell tokenizer that we might have a special token there, but we do not tell the function about each special token at moment. Then after the training you finally get a trained tokenizer instance, then you can specify tokenizer.enable_padding(pad_id=tokenizer.tokens_to_id('[PAD]')) to tell the tokenizer that we want to set [PAD] to the roll of the padding token which we have already setup in vocab list before. After setting up .enable_padding(), the tokenized sentense might contain padding. Vocab size: tokenizer.get_vocab_size()","Oct 25, 2022 8:29 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Discussing the Pros and Cons of Using add_tokens vs. Byte Pair Encoding (BPE) for Adding New Tokens to an Existing RoBERTa Model","https://discuss.huggingface.co/t/discussing-the-pros-and-cons-of-using-add-tokens-vs-byte-pair-encoding-bpe-for-adding-new-tokens-to-an-existing-roberta-model/46829","0","726","Created: Jul 14, 2023 1:49 pm","RCarroll","I‚Äôm currently working on a project that involves adding new tokens (specifically, domain-specific terms) to a pre-trained biomed_RoBERTa model. I have come across two main strategies for this: Using the add_tokens method provided by the Transformers library to directly add new tokens to the tokenizer‚Äôs vocabulary. Training a new Byte Pair Encoding (BPE) tokenizer on the new data and merging it with the existing vocabulary. I‚Äôm interested in hearing your thoughts on the advantages and disadvantages of each method, particularly in the context of adding domain-specific vocabulary to a pre-existing model. Add_tokens is a more straightforward approach and allows the model to be fine-tuned on the new data, rather than being trained from scratch. However, BPE has advantages, such as handling out-of-vocabulary words and producing more efficient tokenization. I‚Äôd appreciate your insights on these points: How does each method impact the model‚Äôs generalization of new, unseen data? How do these methods influence computational efficiency during training and inference? How does each strategy handle out-of-vocabulary words? Are there any potential drawbacks or challenges in implementing these methods? Thank you in advance for your insights!","Jul 14, 2023 1:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Best solution for train tokenizer and MLM from scratch","https://discuss.huggingface.co/t/best-solution-for-train-tokenizer-and-mlm-from-scratch/12579","0","725","Created: Dec 6, 2021 4:13 pm","Stefano","Hi guys, I searched over different discussions but I was not able to find an effective answer. I‚Äôd like to train from scratch an MLM model (roberta) wherein the original paper concatenate full sentences in the same line up to 512 tokens. This means that they have split the text of the documents into sentences, but actually I was not sure what is the best tool to do this. Moreover, on the web, there are thousands of different approaches to training an MLM from scratch. Some people put a document into a single line, others split a document into sentences and put a sentence in each line. So my questions are: what is the best solution to train the tokenizer and the model from scratch? split a document into lines or keep the same document in one line? What is in your opinion the best choice to split a document into sentences? (different languages) Thanks","Dec 6, 2021 4:13 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding token to t5-base vocab does not respect space","https://discuss.huggingface.co/t/adding-token-to-t5-base-vocab-does-not-respect-space/13662","0","718","Created: Jan 13, 2022 6:48 pm Latest: Jan 13, 2022 6:48 pm","rahular","I am adding { and } to the t5-base tokenizer with tokenizer.add_tokens(['{', '}']) This successfully adds the tokens, but with a slight problem. When I try to tokenize the following sentence, the space before the { and } is gone >>> tokenizer.tokenize('hello { this is a sentence } bye') ['‚ñÅhello', '{', '‚ñÅthis', '‚ñÅis', '‚ñÅ', 'a', '‚ñÅsentence', '}', '‚ñÅby', 'e'] This will result in an imperfect reconstruction of the sentence when decoding >>> tokenizer.batch_decode(tokenizer([""hello { this is a sentence } bye""])['input_ids'], skip_special_tokens=True, clean_up_tokenization_spaces=False) ['hello{ this is a sentence} bye'] I also tried adding the tokens with different configs using AddedToken without success. Any ideas on how I can make { and } their own words?","Jan 13, 2022 6:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LongFormer tokenizer has the same token_type_ids for sequence pairs","https://discuss.huggingface.co/t/longformer-tokenizer-has-the-same-token-type-ids-for-sequence-pairs/12992","0","712","Created: Dec 20, 2021 11:44 pm","Matigol","Hey Guys, I was trying to process a sequence pair of sentences together using the tokenizer for the longformer model and the problem is that the token_type_ids is a list always with zero elements. This is a snippet what the data looks like This is the code to create my CustomDataset: class PlagiarismDetectorDataset(Dataset):          def __init__(self, data: pd.DataFrame, tokenizer, max_token_len: int = 4096):         self.data = data         self.tokenizer = tokenizer         self.max_token_len = max_token_len              def __len__(self):         return len(self.data[self.data['Datatype'] == 'train'])          def __getitem__(self, item: int):                  data_row = self.data.iloc[item]                  review = data_row.Text         original = self.data[(self.data['Task'] == data_row.Task) & (self.data.Datatype == 'orig')]['Text'].iloc[0]         label = data_row.Class                  encoding = self.tokenizer(             text=review,             text_pair=original,             add_special_tokens=True,             max_length=self.max_token_len,             return_token_type_ids=True,             truncation=True,             return_attention_mask=True,             return_tensors=""pt"")                  return dict(             review=review,             original=original,             label=label,             input_ids=encoding[""input_ids""].flatten(),             token_type_ids=encoding[""token_type_ids""].flatten(),             attention_mask=encoding[""attention_mask""].flatten(),             labels=torch.DoubleTensor(label)         ) Finally, this the way to get a sample of the processed data: from transformers import AutoTokenizer, LongformerForSequenceClassification tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')  train_dataset = PlagiarismDetectorDataset(data=aux, tokenizer=tokenizer) sample_data = train_dataset[0] Based on the video of preprocessing I was expecting to have a token_type_ids with 0 and 1 elements. However, the result I get is a list of zero elements. What am I doing wrong?","Dec 20, 2021 11:44 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Fast tokenizer for marianMTModel","https://discuss.huggingface.co/t/fast-tokenizer-for-marianmtmodel/23638","1","499","Created: Sep 26, 2022 3:58 pm Latest: Sep 26, 2022 5:06 pm","Matthieu","Hi, I use Helsinki-NLP/opus-mt-fr-en model for translation from french to english. When I load the tokenizer, I see that the tokenizer isn‚Äôt fast even if I use the use_fast=True flag: tokenizer = AutoTokenizer.from_pretrained(Helsinki-NLP/opus-mt-fr-en, use_fast=True) PreTrainedTokenizer(name_or_path='Helsinki-NLP/opus-mt-fr-en', vocab_size=59514, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}) Doesn‚Äôt it exist fast tokenizer for MarianMTModel?","Sep 26, 2022 3:58 pm","sgugger","No, there is no fast tokenizer for Marian models.","Sep 26, 2022 5:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer.train() running out of memory","https://discuss.huggingface.co/t/tokenizer-train-running-out-of-memory/31355","0","702","Created: Feb 9, 2023 1:15 am","5GreatApes","I am trying to train a tokenizer using the following code: tokenizer= ByteLevelBPETokenizer()   tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[     ""<s>"",     ""<pad>"",     ""</s>"",     ""<unk>"",     ""<mask>"", ])  # Save files to disk tokenizer.save_model(""tokenizer"") It works fine when using a small dataset but when using my full dataset it reads the following error: [00:00:00] Pre-processing files (485 Mo) ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 100% memory allocation of 21474836480 bytes failed My system has 16gb of RAM. Is there a way around this issue that isn‚Äôt upgrading RAM? I‚Äôm not finding solutions online. Thanks","Feb 9, 2023 1:15 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Lower case with input ids","https://discuss.huggingface.co/t/lower-case-with-input-ids/18490","0","692","Created: May 29, 2022 6:50 pm","shon711","Hello, It is possible to do lower case to given input ids without decode and then encode again ? for example tokenizer = AutoTokenizer.from_pretrained('roberta-base')  text1 = tokenizer.decode([713, 16, 10, 3645, 4]) print(text1) >>> This is a sentence.  text2 = tokenizer.decode([9226, 16, 10, 3645, 4]) print(text2) >>> this is a sentence. I would like to know if there is some fast way to convert the id 713 to 9226, without decode, do lower and then encode again. Thanks, Shon","May 29, 2022 6:50 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Performance difference between ByteLevelBPE and Wordpiece tokenizers","https://discuss.huggingface.co/t/performance-difference-between-bytelevelbpe-and-wordpiece-tokenizers/10203","0","682","Created: Sep 22, 2021 2:13 am","dexhrestha","I pretrained two models for Nepali language which is in devnagari script and similar to hindi, first model was a DistillBertmodel with Wordpiece tokenizer and second was Roberta model with ByteLevelBPE tokenizer. For the first model i used oscar nepali dataset which is relatively small dataset. Even for first 50000 optimization steps the model was performing really well, it could predict words based on the context. However for the second model with ByteLevelBPE tokenizer I used a bigger dataset with almost 700k lines. But for this model it is not performing as well as the DistillBert model. So coming to my question. I have few questions about the tokenizer and model. Wordpiece tokenizer normalized the words in the sentence. Why did this happen? Is Distillbert a better model for pre training than roberta or is it due to the tokenizer that i am getting bad results for roberta model? Because while studying about models Roberta was said to have higher parameters and better model than distillbert.","Sep 22, 2021 2:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to handle parenthesis, quotation marks, \n etc when creating tokenizer from scratch","https://discuss.huggingface.co/t/how-to-handle-parenthesis-quotation-marks-n-etc-when-creating-tokenizer-from-scratch/19628","0","677","Created: Jun 26, 2022 6:15 am","jonathanalis","Hello, I have a large corpus from a specific domain. We decided that it was better to create a tokenizer from scratch, there was too many important tokens that was‚Äônt in the general language tokenizer vocabulary. We use SentensePieceBPETokenizer. However, in my first tests, odd things happened. Lets consider the generic token ‚Äútoken‚Äù. It and its capitalized variations apear on the vocabulary. However, also ‚Äú(token‚Äù, ‚Äú‚Äútoken‚Äù, ‚Äú\ntoken‚Äù, ‚Äú\n\ntoken‚Äù, ‚Äú\n\n\ntoken‚Äù, ‚Äú\n(token‚Äù, ‚Äú.\nToken‚Äù (parentesis, quotation mark, a sequence of 1 or more new line (\n), and their combination. Even period, new line and the token ) apear on the vocabulary. Therefore, the tokenizer is not segmenting the \n, quotation marks, parenthesis, etc from the tokens. The tokenizer is adding more words with the variations of the tokens with these annoying characters. So, words that have the same meaning are considered other words in the vocabulary. How should I handle this problem? And how to do it using Tokenizer class? For now, my code is going like this: Blockquote tokenizer = SentencePieceBPETokenizer() tokenizer.normalizer=normalizers.Sequence( [normalizers.NFD() ] ) tokenizer.train_from_iterator( dataset[‚Äòtext‚Äô], vocab_size=60_000 min_frequency=5, show_progress=True, limit_alphabet=500, ) I am aware that I could include the special tokens as the parameter special_tokens on the train_from_iterator function of the tokenizer, for instance: Blockquote special_tokens=[‚Äò[PAD]‚Äô, ‚Äò[UNK]‚Äô, ‚Äò[CLS]‚Äô, ‚Äò[SEP]‚Äô, ‚Äò[MASK]‚Äô] But how to include more special tokens, and attribute specific characters (like newline, quotation mark, parenthesis) to them? Also, if doing so, these tokens are treated as segmented from other tokens when building a tokenizer? Will it solve my problem? Thank you","Jun 26, 2022 6:15 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Bypassing tokenizers","https://discuss.huggingface.co/t/bypassing-tokenizers/2162","2","390","Created: Nov 22, 2020 11:08 am Latest: Nov 23, 2020 1:49 pm","thenoone","Hi everyone, Is it possible to bypass the tokenizer and directly provide the input embeddings to train the BERT model? Thanks!","Nov 22, 2020 11:08 am","BramVanroy","You can just create a subclass of the model that you want and modify its forward pass.","Nov 22, 2020 4:51 pm","sgugger","You can also feed input_embeds instead of input_ids to your mdel.","Nov 23, 2020 1:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer Trainer Crashing","https://discuss.huggingface.co/t/tokenizer-trainer-crashing/36645","0","664","Created: Apr 15, 2023 3:46 am Latest: Apr 15, 2023 3:46 am","jeromeku","Trying to train a tokenizer on a corpus of protein sequences ~8 GB in size (32M examples). However, the memory usage grows incrementally until the process is killed when training from iterator: from tokenizers import SentencePieceBPETokenizer from tokenizers.processors import BertProcessing  import datasets from datasets import load_dataset  import itertools  from datetime import datetime from absl import flags, app  flags.DEFINE_integer(""num_batches"", 1, help=""Number of batches to sample"") flags.DEFINE_integer(""batch_size"", 100, help=""Batch size"") FLAGS = flags.FLAGS  dataset_path = ""agemagician/uniref30""  def batch_iterator(ds: datasets.Dataset, batch_size = 1000, num_batches = -1, split='train', feature='text'):          iterator = iter(ds[split])          def batch_generator(iterable):         while (batch := [ex[feature] for ex in itertools.islice(iterable, batch_size)]):             yield batch                return batch_generator(iterator) if num_batches == -1 else itertools.chain(itertools.islice(batch_generator(iterator), num_batches))      def train(batch_size, num_batches):     uniref = load_dataset(dataset_path)     tokenizer = SentencePieceBPETokenizer()          it = batch_iterator(uniref, batch_size=batch_size, num_batches=num_batches)     tokenizer.train_from_iterator(it, vocab_size=1000, min_frequency=2, special_tokens=[         ""<s>"",         ""<pad>"",         ""</s>"",         ""<unk>"",         ""<mask>"",     ])     tokenizer.save('proteins-tmp')  def main(args):     start = datetime.now()     print(f""Starting tokenizer training on {FLAGS.num_batches} of batch size {FLAGS.batch_size}"")     train(FLAGS.batch_size, FLAGS.num_batches)     end = datetime.now()     dur = end - start     print(f""Took {dur.total_seconds() / 60.:.1f} minutes"")      if __name__ == ""__main__"":    app.run(main) I included batch_size and num_batches commandline args to enable testing. However, when I try to train the tokenizer on the entire dataset (setting num_batches=-1), the memory usage during the ‚Äútokenizing‚Äù phase starts to grow until the entire process crashes. Are there any suggested workarounds for this? Is it possible to train the tokenizer on chunks of data (i.e., call tokenizer.train_from_iterator multiple times), or does it need to see the entire dataset at once for the BPE algorithm to work correctly? Thanks!","Apr 15, 2023 3:46 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Dialogue classification","https://discuss.huggingface.co/t/dialogue-classification/18466","0","656","Created: May 28, 2022 5:01 pm","gillan","I want to make a classification of a dialogue (client and assistant). What is the best way to encode the chat so that model knows whose speech where? Maybe to use some custom special tokens?","May 28, 2022 5:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using the Tokenizers library in a Unity project","https://discuss.huggingface.co/t/using-the-tokenizers-library-in-a-unity-project/17533","0","655","Created: May 4, 2022 6:50 pm","cj-mills","Hi everyone! Any advice on the best way to leverage the Tokenizers library when using a transformer model in a Unity project (C#)? I‚Äôm considering creating a Rust plugin, embedding Python in a C++ plugin, or using a server. Ideally, I could run everything locally, model size allowing.","May 4, 2022 6:50 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What are the equivalent manner for using texts_to_sequences?","https://discuss.huggingface.co/t/what-are-the-equivalent-manner-for-using-texts-to-sequences/13220","0","644","Created: Dec 29, 2021 11:19 pm","toyl","what are the equivalent manner for using seq = tokenizer.texts_to_sequences([desc])[0] in bert","Dec 29, 2021 11:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Error finetuning XLM-RoBERTa-Large when training","https://discuss.huggingface.co/t/error-finetuning-xlm-roberta-large-when-training/20412","2","372","Created: Jul 15, 2022 9:03 am Latest: Jul 15, 2022 9:07 am","JeunesseAfricaine","I followed the guide on the documentation HERE while pre-training a zero-shot classifier. These are screenshots of Tokenization, Compute Metrics, Training Arguments, and Trainer.","Jul 15, 2022 9:03 am","JeunesseAfricaine","","Jul 15, 2022 9:07 am","JeunesseAfricaine","","Jul 15, 2022 9:07 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Space token ‚Äô ‚Äô cannot be add when is_split_into_words = True","https://discuss.huggingface.co/t/space-token-cannot-be-add-when-is-split-into-words-true/4305","1","455","Created: Mar 11, 2021 9:27 am Latest: Mar 11, 2021 9:35 am","Boltzmachine","for example, >>> tokenizer = BertTokenizer.from_pretrained('bert-base-chinese') >>> tokenizer.add_tokens(' ') 1 >>> tokenizer.encode('‰Ω†Â•Ω ‰∏ñÁïå', add_special_tokens=False) [872, 1962, 21128, 686, 4518] >>> tokenizer.encode(['‰Ω†','Â•Ω',' ', '‰∏ñ', 'Áïå'], is_split_into_words=True, add_special_tokens=False)  [872, 1962, 686, 4518] Obviously, the blank token is ignored. But if you change it to another token like ‚Äò[balabala]‚Äô, it works. So what is the proper way to do this?","Mar 11, 2021 9:27 am","Boltzmachine","I found that one way is to use convert_tokens_to_ids, yet by which I cannot use the convenient features in encode and __call__ such as padding and automatically generating attention_mask","Mar 11, 2021 9:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Chunk tokens into desired chunk length without simply getting rid of rest of tokens","https://discuss.huggingface.co/t/chunk-tokens-into-desired-chunk-length-without-simply-getting-rid-of-rest-of-tokens/43399","0","624","Created: Jun 15, 2023 4:01 pm","bennicholl","Is there a way to break up a tokens based on max token length? For example if I tokenize the sentence toked_sent = tokenizer([""I have two rottweilers and a black lab""]) I‚Äôll get {'input_ids': [[27, 43, 192, 3, 14369, 15337, 277, 11, 3, 9, 1001, 7690, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]} If I want to set some parameter max_length to 8, the output would be something like {'input_ids': [[27, 43, 192, 3, 14369, 15337, 277, 11], [3, 9, 1001, 7690, 1] ], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1,1,1,1,1] ]} Is this possible?","Jun 15, 2023 4:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to return custom `token_type_ids` or other values from a tokenizer?","https://discuss.huggingface.co/t/how-to-return-custom-token-type-ids-or-other-values-from-a-tokenizer/38525","0","625","Created: May 3, 2023 9:07 am Latest: May 3, 2023 9:09 am","lonewar","I have two text features, ‚Äòtitle‚Äô and ‚Äòabstract‚Äô. I want to tokenize these two columns in a Dataset. # https://huggingface.co/jjzha/jobbert-base-cased checkpoint = 'jjzha/jobbert-base-cased' tokenizer = AutoTokenizer.from_pretrained(checkpoint) tokenizer.model_max_len = 512  def tokenize(batch):     tokenized_text1 = tokenizer(batch[""title""], truncation=True, padding='max_length', max_length=512)     tokenized_text2 = tokenizer(batch[""abstract""], truncation=True, padding='max_length', max_length=512)      return {'input_ids1': tokenized_text1['input_ids'], 'attention_mask1': tokenized_text1['attention_mask'],         'token_type_ids1': tokenized_text1['token_type_ids'],         'input_ids2': tokenized_text2['input_ids'], 'attention_mask2': tokenized_text2['attention_mask'],         'token_type_ids2': [1] * len(tokenized_text2['token_type_ids']) #100 * [[1] * 512]             }  dataset_dict.map(tokenize, batched=True)['train'].to_pandas()#['input_ids1'] In the above code, in this line ‚Äòtoken_type_ids2‚Äô, I want to create the token_type_ids full of 1s for the second text feature (like next sentence prediction in BERT). But it is not working as expected. I assume ‚Äòtoken_type_ids2‚Äô should be a list of size (max_len). Is it correct? By examining the tokenized dataset, the column ‚Äútoken_type_ids2‚Äù for each row, is a single value 1. But it is supposed to be a list of 1s with length 512. How to solve this problem? BTW, If I fix the value there to be ‚Äú100 * [[1] * 512]‚Äù, it shows the correct token_type_ids. 100 is the size of the original dataset. But shouldn‚Äôt the map function applies the tokenize function to each row?","May 3, 2023 9:07 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Huggingface t5 models seem to not download a tokenizer file","https://discuss.huggingface.co/t/huggingface-t5-models-seem-to-not-download-a-tokenizer-file/27935","0","623","Created: Dec 16, 2022 12:14 pm","jantuitman","If I run this code i get an error: model_name = 'yhavinga/t5-base-dutch' #model_name = 'flax-community/t5-base-dutch' model: PreTrainedModel = T5ForConditionalGeneration.from_pretrained(model_name) tokenizer = T5Tokenizer.from_pretrained(model_name) I have debugged the code and i see there is no resolved filename that is passed in to the underlying SentencePiece tokenizer. But this tokenizer crashes if it is not initialized with a file. So is this a bug in the model files? How do i otherwise get the same tokens as the model was trained with on my local machine?","Dec 16, 2022 12:14 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Multilang bert vs translating to english","https://discuss.huggingface.co/t/multilang-bert-vs-translating-to-english/18454","0","603","Created: May 28, 2022 8:32 am","jl22","I am using sentence transformers (i tried many multilang tokenizers) but they do not perform as well vs english. For now I translare first using the deepl api and tokenize the translated version. Good results so far. I was wondering what the downside of this method is? The translation is very fast and costs are manageable.","May 28, 2022 8:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Questions on model‚Äôs tokens","https://discuss.huggingface.co/t/questions-on-models-tokens/5023","0","592","Created: Mar 24, 2021 1:46 am","ape","Hi, I have a few questions regarding tokenizing word/characters/emojis for different huggingface models. From my understanding, a model would only perform best during inference if the token of the input sentence are within the tokens that the model‚Äôs tokenizer was trained on. My questions are: is there a way to easily find out if a particular word/emoji is compatible (included during model training) with the model? if this word/emoji is not was not included during model training, what are the best ways to deal with these words/emojis, such that model inference would give best possible output considering the inclusion of these word/emoji as input. (for 2. it would be nice if it could be answered in the context of my setup below, if possible) My current setup is as follows: from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification pre_trained_model = 'facebook/bart-large-mnli' task = 'zero-shot-classification' candidate_labels = ['happy', 'sad', 'angry', 'confused'] tokenizer = AutoTokenizer.from_pretrained(pre_trained_model) model = AutoModelForSequenceClassification.from_pretrained(pre_trained_model) zero_shot_classifier = pipeline(model=model, tokenizer=tokenizer, task=task)  zero_shot_classifier('today is a good day üòÉ', candidate_labels=candidate_labels) Any help is appreciated","Mar 24, 2021 1:46 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How do I remove tokens from a BPE Tokenizer‚Äôs vocabulary?","https://discuss.huggingface.co/t/how-do-i-remove-tokens-from-a-bpe-tokenizers-vocabulary/94593","2","341","Created: Jun 28, 2024 12:27 pm Latest: Jul 3, 2024 8:28 am","ForBo7","Hi there! I need to remove specific tokens from my tokenizer‚Äôs vocabulary, and I am not quite sure how to do so. Specifically, I am using Qwen2Tokenizer, a BPE tokenizer, and I would like to remove specific Chinese tokens from its vocabulary. I have tried various methods, shown below, but to no avail. Deleting Tokens from Vocabulary for tok in long_toks: vocab.pop(tok) tokz.vocab = vocab This results in unknown tokens being output. The length of vocabulary also differs in certain places. That is, `len(vocab) ‚â† len(tokz.vocab). Tweaking vocab.json and merges.txt vocab.json import json with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f) for tok in long_toks: vocab.pop(tok) with open('tokenizer/vocab.json', 'w') as f: json.dump(vocab, f, indent=2) merges.txt with open('tokenizer/merges.txt', 'r') as f: merges = L(f.readlines()[1:]) While I am able to update vocab.json, I do not know how to work with the contents of merges.txt. This file stores pairs as follows: (#151387) ['ƒ† ƒ†\n','ƒ†ƒ† ƒ†ƒ†\n','i n\n','ƒ† t\n','ƒ†ƒ†ƒ†ƒ† ƒ†ƒ†ƒ†ƒ†\n','e r\n','ƒ†ƒ† ƒ†\n','o n\n','ƒ† a\n','r e\n'...] I am unable to determine what special characters represent Chinese characters. Is there any way I can decode or figure out what the characters such as ‚Äòƒ†‚Äô and ‚Äò√¢¬Ω ƒπ \n‚Äô represent? Training a Tokenizer I feel training a tokenizer might not be feasible, because of the amount of data required? I want to have the exact same Qwen2Tokenizer, save for certain tokens removed. Backend import json tokz_state = json.loads(tokz.backend_tokenizer.model.__getstate__()) for tok in long_toks: del tokz_state['vocab'][tok]  from tokenizers import models model_class = getattr(models, tokz_state.pop('type')) tokz.backend_tokenizer.model = model_class(**tokz_state) However, this approach results in the following error: ---------------------------------------------------------------------------TypeError                                 Traceback (most recent call last)Cell In[67], line 3       1 from tokenizers import models       2 model_class = getattr(models, tokz_state.pop('type')) ----> 3 tokz.backend_tokenizer.model = model_class(**tokz_state) TypeError: argument 'merges': failed to extract enum PyMerges ('Merges | Filename') - variant Merges (Merges): TypeError: failed to extract field PyMerges::Merges.0, caused by TypeError: 'str' object cannot be converted to 'PyTuple' - variant Filename (Filename): TypeError: failed to extract field PyMerges::Filename.0, caused by TypeError: 'list' object cannot be converted to 'PyString' This approach references this GitHub issue, where others also noted the same error. I would really appreciate any pointers regarding how correctly remove tokens!","Jun 28, 2024 12:27 pm","ForBo7","I figured out a way to do so to remove tokens from a BPE Tokenizer. I took each pair in the merges.txt file and combined them to produce the resulting token. I then looked up the resulting token in the tokenizer‚Äôs vocabulary to obtain its input ID. I then decoded the obtained input ID to determine what each resulting merge represented. From that, I was able to filter out the undesired tokens from the merge file. with open('tokenizer/merges.txt', 'r') as f: mrules = L(f.read().split('\n')[1:-1]) import json with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f)  merged = L(merge.replace(' ', '') for merge in mrules) merged_ids = L(vocab[merge] for merge in merged) pairs = dict(zip(merged, merged_ids))  for i, _merge in enumerate(mrules):   merge = _merge.replace(' ', '')   idx = vocab[merge]   if is_long(tokz.decode(idx))[0]: mrules.remove(_merge) Then I simply removed the same undesired tokens from the tokenizer‚Äôs vocabulary. with open('tokenizer/vocab.json', 'r') as f: vocab = json.load(f) long_toks = {tok: idx for tok, idx in vocab.items() if is_long(tokz.decode(idx))[0]} for tok in long_toks: vocab.pop(tok)","Jul 3, 2024 8:28 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Avoid creating certain tokens when training a tokenizer","https://discuss.huggingface.co/t/avoid-creating-certain-tokens-when-training-a-tokenizer/20864","0","583","Created: Jul 26, 2022 2:49 am","jonathanalis","Hello. There is a way to train a tokenizer from scratch, but force the algorithm (lets say, WordPiece) to not include in the vocabulary some tokens based on a given rule? For instance, I would like to Lets say we have the words: ['rond√¥nia', 'Rond√¥nia', 'ROND√îNIA'] Training a lowercase wordpiece tokenizer on my dataset tokenizes them as ['rond√¥n', '##ia'] However, i tryed something different. I preprocess them (using a custom pretokenizer) into, respectively: ['rond√¥nia', 'rond√¥nia_FU', 'rond√¥nia_U'] I tryed tokenizing, but I got ['rond√¥n', '##ia'], ['rond√¥n', '##ia_FU'], ['rond√¥n', '##ia_U'] Is not what I expected. I expect : ['rond√¥n', '##ia'], ['rond√¥n', '##ia', '##_FU'], ['rond√¥n', '##ia', '##_FU'] (or either ['rond√¥nia'], ['rond√¥nia', '##_FU'], ['rond√¥nia', '##_FU']) I want to train the tokenizer in such way the ‚Äò_FU‚Äô and ‚Äò_U‚Äô to not being part of tokens, to be an exception, only when finding these terms in the words it ignores and break them separately, like they were something like ponctuations‚Ä¶ It is possible? There is such option when training a (wordpiece) tokenizer? Plan B If not, it is possible to remove words from vocabulary? I mean, I could remove every token that ends in _FU (except ‚Äò##_FU‚Äô). By doing this, it will force the tokenizer to tokenize as I want because it wont find any tokens larger than ‚Äò##_FU‚Äô when tokenizing words that ends with _FU, because they wont be on the vocabulary anymore. Makes sense? Is it possible to remove words from vocabulary? Side note: I didnt save a vocabulary in disk, it seems it is on the memory while running the code. As it is a customized tokenizer, it wont let me save it: tok_wpc.save(""tokenizer.json"") Returns the error: Exception: Custom PreTokenizer cannot be serialized So I cant save externally, neither see the generated vocabulary‚Ä¶ Tks in advance.","Jul 26, 2022 2:49 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with Tokenizer","https://discuss.huggingface.co/t/issue-with-tokenizer/33796","0","580","Created: Mar 14, 2023 5:16 pm Latest: Mar 14, 2023 5:16 pm","pnandhini","I am trying to download audio files using streaming mode. The steps go like this Download the dataset, processor feature extractor, prepare the dataset, seq2seqtrainer, compute metrics and then trainer.train(). When the training steps starts, after few downloads getting an error in tokenizer.utils.base as ‚ÄúYou need to specify either ‚Äô text‚Äô or ‚Äô text_target‚Äô‚Äù. Have two questions here, one is not getting this error while downloading non streaming mode. Secondly after passing the parameter as "" tex"" / "" text_target"" still getting this error. Any help would be appreciated!!","Mar 14, 2023 5:16 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding a special language token to MBART","https://discuss.huggingface.co/t/adding-a-special-language-token-to-mbart/25967","0","555","Created: Nov 12, 2022 1:09 pm","BramVanroy","Is there a straightforward way of adding a new language to MBART‚Äôs tokenizer? The implementation seems quite intricate so it does not seem straightforward to add a new language code.         self.sp_model_size = len(self.sp_model)         self.lang_code_to_id = {             code: self.sp_model_size + i + self.fairseq_offset for i, code in enumerate(FAIRSEQ_LANGUAGE_CODES)         }         self.id_to_lang_code = {v: k for k, v in self.lang_code_to_id.items()}         self.fairseq_tokens_to_ids[""<mask>""] = len(self.sp_model) + len(self.lang_code_to_id) + self.fairseq_offset          self.fairseq_tokens_to_ids.update(self.lang_code_to_id)         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}         self._additional_special_tokens = list(self.lang_code_to_id.keys())          if additional_special_tokens is not None:             # Only add those special tokens if they are not already there.             self._additional_special_tokens.extend(                 [t for t in additional_special_tokens if t not in self._additional_special_tokens]             )          self._src_lang = src_lang if src_lang is not None else ""en_XX""         self.cur_lang_code_id = self.lang_code_to_id[self._src_lang] Even with subclassing it is not immediately clear to me if and how one would add a custom language code that will be correctly recognized when using tokenizer(target_text=...) or other target language related things. Any tips?","Nov 12, 2022 1:09 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to train target tokenizer","https://discuss.huggingface.co/t/how-to-train-target-tokenizer/22291","0","554","Created: Aug 30, 2022 2:48 am","nubol23","I‚Äôm trying to retrain t5-small with a japanese to spanish dataset, I want to retrain the tokenizer to handle the words in those languages Currently I‚Äôve done this: def get_training_corpus(lang: str):     ds = dataset[""train""]     for start_idx in range(0, len(ds), 1000):         samples = ds[start_idx : start_idx + 1000]         yield samples[lang]  tokenizer = AutoTokenizer.from_pretrained(""t5-small"") new_tokenizer = tokenizer.train_new_from_iterator(     get_training_corpus(""ja""),      52000, ) but I don‚Äôt know how to also train the target side of the tokenizer, I would like to be able to tokenize in japanese like model_inputs = tokenizer(examples[""ja""], max_length=max_input_length, truncation=True) and in spanish using with tokenizer.as_target_tokenizer():         labels = tokenizer(examples[""es""], max_length=max_target_length, truncation=True)","Aug 30, 2022 2:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer for German lang","https://discuss.huggingface.co/t/tokenizer-for-german-lang/44222","0","547","Created: Jun 22, 2023 7:34 pm","whatisslove11","Hi! I try to implement neural machine translition model from scratch and now I choose tokenizer for languages. I read what tokenizer has special Unicode Normalizer (NFC, NFD, etc.). I have a few questions for this normalizers Do I need to use Unicode normalizers for German or will any other one be suitable? Is there any additional information on unicode normalizers? Thanks a lot for your help!","Jun 22, 2023 7:34 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How does a tokenzier (eg., AutoTokenizer) generate word_ids intergers?","https://discuss.huggingface.co/t/how-does-a-tokenzier-eg-autotokenizer-generate-word-ids-intergers/44639","0","545","Created: Jun 26, 2023 6:49 pm Latest: Jun 27, 2023 10:58 am","mohit-sv","Context of question: (scroll down to get to the Real question) I need to find start_position and end_position of the sequence that I input to QA model (eg., RoBERTa, LLMv3) from the start-end character positions of the answer in a context. @NielsRogge in his notebook (Transformers-Tutorials/LayoutLMv2/DocVQA/Fine_tuning_LayoutLMv2ForQuestionAnswering_on_DocVQA.ipynb at master ¬∑ NielsRogge/Transformers-Tutorials ¬∑ GitHub) does this by using word_ids information of the tokenized context and start-end token indices of the answer in the given context. eg., context_token_indicies = [0, 1, 2, 3, 4, 5, 6, 7, || 8, 9, 10, 11, 12, 13 ||, 14, 15, 16, 17, 18] # pipes represent the scope of answer in tokenized context ans_start_token_idx_in_context = 8 ans_start_token_idx_in_context = 13 context_word_ids = [0, 1, 2, 3, 3, 4, 5, 6, 7, || 8, 8, 9, 10, 10, 10, 11, 12, 13 ||, 14, 15, 16, 17, 18, 18] # pipes represent the scope of answer in tokenized context (FYI: Repeated indices are created when a word like ‚Äúbookworm‚Äù is broken down into ‚Äúbook‚Äù and ‚Äúworm‚Äù by the tokenizer.) we infer that‚Ä¶ start_position = 9 end_position = 17 Therefore, the success of the whole process is dependent on how we tokenize the context. @NielsRogge used match, word_idx_start, word_idx_end = subfinder(words, answer.split()) (gets first match; doesn‚Äôt work for me) which is the same as doing context.split(), but this fails to split at ‚Äú,‚Äù, ‚Äú.‚Äù etc. as done by AutoTokenizer. eg., ‚ÄúGreat! Keep it up.‚Äù must be tokenized as [‚ÄúGreat‚Äù, ‚Äú!‚Äù, ‚ÄúKeep‚Äù, ‚Äúit‚Äù, ‚Äúup‚Äù, ‚Äú.‚Äù] => context_token_indicies = [0, 1, 2, 3, 4, 5] not [0, 1, 2, 3] So I tried using nltk.word_tokenize. Though it does better, again this fails to split at ‚Äú-‚Äù. eg., ‚ÄúGreat! Keep-it-up.‚Äù must be tokenized as [‚ÄúGreat‚Äù, ‚Äú!‚Äù, ‚ÄúKeep‚Äù, ‚Äú-‚Äù, ‚Äúit‚Äù, ‚Äú-‚Äù, ‚Äúup‚Äù, ‚Äú.‚Äù] (Using offset_mapping instead of word_ids gives some other problems. RoBERTa, LLMv3 offsets look very different.) I wonder how many other such edge cases exist. Thus the important point to be clarified is‚Ä¶ Real question: ‚ÄúHow do we need to tokenize context to find start_position and end_position for Question answering task?‚Äù This can be understood if we know ‚Äúhow huggingface tokenizers generate the integers in word_ids‚Äù. Can anyone please answer one of the questions in the quotes?","Jun 26, 2023 6:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPT2 long text approach","https://discuss.huggingface.co/t/gpt2-long-text-approach/28165","0","542","Created: Dec 20, 2022 10:54 am","misterkilgore","Hi everyone. I was wondering, what‚Äôs the best way to approach a long text for gpt2 training? Suppose the text is 10000 tokens long and the max length for the tokenizer is 2048. It‚Äôs better to split the corpus in 5 sequential samples (first sample from token 0 to 2047, second sample from token 2048 to 4095‚Ä¶) or in 7952 overlapping samples (first sample form token 0 to token 2047, second sample from token 1 to 2048‚Ä¶ last sample from token 7952 to token 10000)? I‚Äôve seen people using both approaches.","Dec 20, 2022 10:54 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does Deberta tokenizer use wordpiece?","https://discuss.huggingface.co/t/does-deberta-tokenizer-use-wordpiece/21307","0","539","Created: Aug 6, 2022 9:05 am","Trunway","From the docs of hugging face: Constructs a DeBERTa tokenizer, which runs end-to-end tokenization: punctuation splitting + workpiece The answer is positive. However, when I checked results tokenized by other models‚Äô tokenizers, the results were confusing. I checked four models in total, respectively deberta, bert, roberta and albert. The tokenized results of the string ‚ÄúHugging face is of great help to implement models!‚Äù by the four models‚Äô tokenizers are respectively shown as follows. My code: tokenizer1 = AutoTokenizer.from_pretrained(""microsoft/deberta-base"") tokenizer2 = AutoTokenizer.from_pretrained(""bert-base-uncased"") tokenizer3 = AutoTokenizer.from_pretrained(""roberta-base"") tokenizer4 = AutoTokenizer.from_pretrained(""albert-xxlarge-v2"")  test_str = ""Hugging face is of great help to implement models!"" print(tokenizer1.tokenize(test_str)) print(tokenizer2.tokenize(test_str)) print(tokenizer3.tokenize(test_str)) print(tokenizer4.tokenize(test_str)) Results: ['Hug', 'ging', 'ƒ†face', 'ƒ†is', 'ƒ†of', 'ƒ†great', 'ƒ†help', 'ƒ†to', 'ƒ†implement', 'ƒ†models', '!'] ['hugging', 'face', 'is', 'of', 'great', 'help', 'to', 'implement', 'models', '!'] ['Hug', 'ging', 'ƒ†face', 'ƒ†is', 'ƒ†of', 'ƒ†great', 'ƒ†help', 'ƒ†to', 'ƒ†implement', 'ƒ†models', '!'] ['‚ñÅhugging', '‚ñÅface', '‚ñÅis', '‚ñÅof', '‚ñÅgreat', '‚ñÅhelp', '‚ñÅto', '‚ñÅimplement', '‚ñÅmodels', '!'] As you can see, the results of deberta is the same as roberta, which, as described in the hugging face docs, is derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding. So, does deberta use byte-level Byte-Pair-Encoding, instead of wordpiece like bert (the results of deberta and bert are different from each other .)? Besides, I also checked the codes for deberta tokenizer, and found that the _tokenize() method is derived from GPT2 tokenizer as well (class DebertaTokenizer(GPT2Tokenizer):). So, is there any problem with the content of the docs, or if anyone could help to explain this? Thank you for your reply!","Aug 6, 2022 9:05 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Building a custom Java tokenizer","https://discuss.huggingface.co/t/building-a-custom-java-tokenizer/71823","0","531","Created: Feb 4, 2024 5:40 pm","nimanthacooray","Hi, I am trying to build a custom tokenizer for tokenizing Java code using the tokenizers library. I have a set of tokens that should not be splitted into subwords (For example: Java keywords, operators, separators, common class names, etc). But I want identifiers in the Java token to split into subword tokens (For example: getAge, setName, etc). To achieve this, I have added the tokens that don‚Äôt need to be splitted to the tokenizer before training. Then the tokenizer is trained on a large Java code file. Below is the code for this: tokenizer = Tokenizer(models.WordPiece(unk_token='<unk>')) tokenizer.pre_tokenizer = PreTokenizer.custom(JavaPreTokenizer())  with open(IMPORTANT_TOKENS_FILE) as fp:     important_tokens = json.load(fp)  special_tokens = [""</s>"", ""<unk>"", ""<pad>""] num = tokenizer.add_tokens(important_tokens + special_tokens) # print(f""{num} tokens added"")  trainer = WordPieceTrainer(vocab_size=32000, special_tokens=special_tokens) tokenizer.decoder = decoders.WordPiece() tokenizer.train([DATASET_FILE], trainer=trainer) print(f""Vocab size: {tokenizer.get_vocab_size()}"") The custom pre-tokenizer is implemented using the javalang library. It is a python library for parsing and tokenizing Java code. Below is the custom pre-tokenizer: class JavaPreTokenizer:      def java_tokenize(self, i, normalized_string):         string = str(normalized_string)         javalang_tokens = list(javalang.tokenizer.tokenize(string))         splits = []         original_pos = 0         for javalang_token in javalang_tokens:             length = len(javalang_token.value)             while str(javalang_token.value) != string[original_pos:original_pos+length] and original_pos < len(string):                 original_pos += 1             if original_pos >= len(string):                 raise ValueError(f""Could not find token \""{javalang_token.value}\"" in string \""{string}\"""")             splits.append(normalized_string[original_pos:original_pos+length])             original_pos += length         return splits      def pre_tokenize(self, pretok: PreTokenizedString):         pretok.split(self.java_tokenize) The issue I am facing is, when I try to tokenize ‚Äúclass Person implements Comparable<Person> {\n String name = \""Hello World!\"";‚Äù, the pre-tokenizer is getting only the ""Hello Worl to pre-tokenize and fails with an error message Exception: LexerError: Unterminated character/string literal at "" "", line 1: ""Hello Worl. The failure is understandable because javalang can not tokenize a unterminated string. But why this happens? I mean why pre-tokenizer is given only part of the input? After some debugging, I saw that the pre-tokenizer is applied only on some parts of the input. Those parts are [' Person ', ' ', 'Person', ' ', '\n ', ' name ', ' ""Hello World']. I thought the pre-tokenizer is applied to the whole input. When I checked my vocabulary, I saw those tokens which were passed to pre-tokenizer are not in the added tokens section of the saved tokenizer (I saved the tokenizer after wrapping it using the transformers library). It seems like the tokens only which are not in the added tokens are passed to the pre-tokenizer. How to fix this issue?","Feb 4, 2024 5:40 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Are the slow and fast tokenizer results the same output for the same input?","https://discuss.huggingface.co/t/are-the-slow-and-fast-tokenizer-results-the-same-output-for-the-same-input/52743","0","528","Created: Aug 30, 2023 4:01 am Latest: Aug 30, 2023 5:50 am","mengban","I found some information but couldn‚Äôt find a definitive statement. But I found an example where tokenization results are different: from transformers import LlamaTokenizer, LlamaTokenizerFast  model_name_or_path = ""ziqingyang/chinese-alpaca-2-7b"" sentence = ""ÂêÉ‰∫ÜÂêó</s>""  llama_t = LlamaTokenizer.from_pretrained(model_name_or_path) llama_t_fast = LlamaTokenizerFast.from_pretrained(model_name_or_path)  print(llama_t(sentence)) print(llama_t_fast(sentence)) the output is: {‚Äòinput_ids‚Äô: [1, 29871, 35302, 32013, 2], ‚Äòattention_mask‚Äô: [1, 1, 1, 1, 1]} {‚Äòinput_ids‚Äô: [1, 29871, 32050, 34353, 2], ‚Äòattention_mask‚Äô: [1, 1, 1, 1, 1]} and here is my env: transformers                  4.31.0 tokenizers                    0.13.3","Aug 30, 2023 4:01 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I set up a different batch_size, but the time of data processing has not changed","https://discuss.huggingface.co/t/i-set-up-a-different-batch-size-but-the-time-of-data-processing-has-not-changed/9668","0","528","Created: Sep 1, 2021 1:10 pm","ccfeidao","def tokenize_function(example):     return tokenizer(example[""sentence1""], truncation=True, max_length = 512) tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, batch_size = 1024) tokenized_datasets = tokenized_datasets.remove_columns([""sentence1""]) tokenized_datasets.set_format(""torch"")","Sep 1, 2021 1:10 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Should cls_token be [CLS] or <cls>?","https://discuss.huggingface.co/t/should-cls-token-be-cls-or-cls/58140","3","265","Created: Oct 11, 2023 7:48 am Latest: Oct 11, 2023 9:23 am","raptorkwok","If I want to create a tokenizer from scratch, should the special tokens use square brackets with uppercase letters, or angle brackets with lowercase letters? That is, should cls_token be [CLS] or <cls>? BertTokenizerFast uses the former one: github.com huggingface/transformers/blob/599db139f921f3af535052c860cb685cadae6fcd/src/transformers/tokenization_bert.py#L547 def __init__(     self,     vocab_file,     do_lower_case=True,     do_basic_tokenize=True,     never_split=None,     unk_token=""[UNK]"",     sep_token=""[SEP]"",     pad_token=""[PAD]"",     cls_token=""[CLS]"",     mask_token=""[MASK]"",     tokenize_chinese_chars=True,     max_length=None,     pad_to_max_length=False,     stride=0,     truncation_strategy=""longest_first"",     add_special_tokens=True,     **kwargs ):     super(BertTokenizerFast, self).__init__( while many other examples on the Internet uses the latter one.","Oct 11, 2023 7:48 am","Sandy1857","It could be anything you want it to be since you‚Äôre making the tokenizer from scratch.","Oct 11, 2023 8:34 am","raptorkwok","thanks. Also, is bos_token and eos_token necessary? In Bert‚Äôs implementation, these two tokens are not defined.","Oct 11, 2023 8:44 am","Sandy1857","It‚Äôs not needed for Bert, which does not generate text auto-regressively. So it depends on your application I guess.","Oct 11, 2023 9:23 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using a BertTokenizer when training a RobertaForMaskedLM","https://discuss.huggingface.co/t/using-a-berttokenizer-when-training-a-robertaformaskedlm/27456","0","512","Created: Dec 8, 2022 10:51 am","Chrode","Hello, I want to train from scrath a RobertaForMaskedLM model. But I need a character level tokenizer and I found one already, perfect for me. So I am wondering, can I re-use it? This is my RobertaForMaskedLM config: {""architectures"": [""RobertaForMaskedLM""],  ""attention_probs_dropout_prob"": 0.1,  ""hidden_act"": ""gelu"",  ""hidden_dropout_prob"": 0.1,  ""hidden_size"": 768, ""initializer_range"": 0.02,  ""intermediate_size"": 3072,  ""layer_norm_eps"": 1e-05,  ""max_position_embeddings"": 40000,  ""model_type"": ""roberta"",  ""num_attention_heads"": 12,  ""num_hidden_layers"": 12,  ""type_vocab_size"": 1,  ""vocab_size"": 30} # edited to be the same as the tokenizer This is the BertTokenizer config: {   ""version"": ""1.0"",   ""truncation"": {     ""direction"": ""Right"",     ""max_length"": 512,     ""strategy"": ""LongestFirst"",     ""stride"": 0   },   ""padding"": null,   ""added_tokens"": [     {       ""id"": 0,       ""content"": ""[PAD]"",       ""single_word"": false,       ""lstrip"": false,       ""rstrip"": false,       ""normalized"": false,       ""special"": true     },     {       ""id"": 1,       ""content"": ""[UNK]"",       ""single_word"": false,       ""lstrip"": false,       ""rstrip"": false,       ""normalized"": false,       ""special"": true     },     {       ""id"": 2,       ""content"": ""[CLS]"",       ""single_word"": false,       ""lstrip"": false,       ""rstrip"": false,       ""normalized"": false,       ""special"": true     },     {       ""id"": 3,       ""content"": ""[SEP]"",       ""single_word"": false,       ""lstrip"": false,       ""rstrip"": false,       ""normalized"": false,       ""special"": true     },     {       ""id"": 4,       ""content"": ""[MASK]"",       ""single_word"": false,       ""lstrip"": false,       ""rstrip"": false,       ""normalized"": false,       ""special"": true     }   ],   ""normalizer"": {     ""type"": ""BertNormalizer"",     ""clean_text"": true,     ""handle_chinese_chars"": true,     ""strip_accents"": null,     ""lowercase"": false   },   ""pre_tokenizer"": {     ""type"": ""BertPreTokenizer""   },   ""post_processor"": {     ""type"": ""TemplateProcessing"",     ""single"": [       {         ""SpecialToken"": {           ""id"": ""[CLS]"",           ""type_id"": 0         }       },       {         ""Sequence"": {           ""id"": ""A"",           ""type_id"": 0         }       },       {         ""SpecialToken"": {           ""id"": ""[SEP]"",           ""type_id"": 0         }       }     ],     ""pair"": [       {         ""SpecialToken"": {           ""id"": ""[CLS]"",           ""type_id"": 0         }       },       {         ""Sequence"": {           ""id"": ""A"",           ""type_id"": 0         }       },       {         ""SpecialToken"": {           ""id"": ""[SEP]"",           ""type_id"": 0         }       },       {         ""Sequence"": {           ""id"": ""B"",           ""type_id"": 1         }       },       {         ""SpecialToken"": {           ""id"": ""[SEP]"",           ""type_id"": 1         }       }     ],     ""special_tokens"": {       ""[CLS]"": {         ""id"": ""[CLS]"",         ""ids"": [           2         ],         ""tokens"": [           ""[CLS]""         ]       },       ""[SEP]"": {         ""id"": ""[SEP]"",         ""ids"": [           3         ],         ""tokens"": [           ""[SEP]""         ]       }     }   },   ""decoder"": {     ""type"": ""WordPiece"",     ""prefix"": ""##"",     ""cleanup"": true   },   ""model"": {     ""type"": ""WordPiece"",     ""unk_token"": ""[UNK]"",     ""continuing_subword_prefix"": ""##"",     ""max_input_chars_per_word"": 100,     ""vocab"": {       ""[PAD]"": 0,       ""[UNK]"": 1,       ""[CLS]"": 2,       ""[SEP]"": 3,       ""[MASK]"": 4,       ""L"": 5,       ""A"": 6,       ""G"": 7,       ""V"": 8,       ""E"": 9,       ""S"": 10,       ""I"": 11,       ""K"": 12,       ""R"": 13,       ""D"": 14,       ""T"": 15,       ""P"": 16,       ""N"": 17,       ""Q"": 18,       ""F"": 19,       ""Y"": 20,       ""M"": 21,       ""H"": 22,       ""C"": 23,       ""W"": 24,       ""X"": 25,       ""U"": 26,       ""B"": 27,       ""Z"": 28,       ""O"": 29     }   } } Thank you very much! If you now how to create a character based tokenizer let me know please.","Dec 8, 2022 10:51 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Continuation token in pertained tokenizer bert-base-chinese","https://discuss.huggingface.co/t/continuation-token-in-pertained-tokenizer-bert-base-chinese/218","0","510","Created: Jul 11, 2020 10:21 am","jiayu","I am a bit confused on the different tokens (with and without ## i.e. continuation prefix) in bert-base-chinese. In [1]: from transformers import AutoTokenizer  In [2]: tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')  In [3]: tokenizer.encode('ÊÅãÁà±ing', add_special_tokens=False) Out[3]: [2605, 4263, 10139]  In [4]: tokenizer.save_pretrained('tokenizer')  Out[4]: ('tokenizer/vocab.txt',  'tokenizer/special_tokens_map.json',  'tokenizer/added_tokens.json')  In [5]: !grep -n ing tokenizer/vocab.txt 8222:##ing 9108:##ting 9310:booking 9383:king 9427:##ling 9536:##ning 9663:shopping 9741:##king 9756:##ding 10062:ling 10070:ÔΩóedding 10140:ing ...  In [6]: !grep -n Áà± tokenizer/vocab.txt 4264:Áà± 17321:##Áà± Here it shows that Áà± is tokenized into 4263 instead of 17320, indicating that for this Chinese pre-trained model the two character word is split separately; but then why do we still need the ##Áà± token in the vocab?","Jul 11, 2020 10:21 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5v1.1 tokenizer legacy=False","https://discuss.huggingface.co/t/t5v1-1-tokenizer-legacy-false/74250","0","508","Created: Feb 22, 2024 3:36 am","Nevermetyou","Hello, I am using t5v1.1 right now and have a question about the arg legacy. The only thing I know is it involves fixing some bugs. legacy (bool, optional) ‚Äî Whether or not the legacy behaviour of the tokenizer should be used. Legacy is before the merge of #24622 and #25224 which includes fixes to properly handle tokens that appear after special tokens. A simple example: legacy=True: But should I set it to false to use new behavior??","Feb 22, 2024 3:36 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False","https://discuss.huggingface.co/t/how-to-make-tokenizer-add-the-spaces-correctly-when-decoding-a-sequence-when-set-add-prefix-space-false/57930","0","502","Created: Oct 9, 2023 4:05 pm","TurboPascal","How to make tokenizer add the spaces correctly when decoding a sequence when set add_prefix_space=False","Oct 9, 2023 4:05 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Using Tokenizer for integer data","https://discuss.huggingface.co/t/using-tokenizer-for-integer-data/28835","0","501","Created: Jan 3, 2023 10:42 am","HasnainKhan","Hey guys, I am working on a problem where I am having tabular data having more than 90 features and one target and all the features are in integers (continuous). I want to use pre-trained BERT, GPT2 but when it comes to the tokenizer the tokenizer is expecting the input in the text format. I can change the integer data in the text format like this; original_data = [1,2,3,4,5,‚Ä¶,94] transformed_data = [‚Äú1 2 3 4 5 ‚Ä¶94‚Äù] Now if I pass the transformed_data to the tokenizer then surely it will work but I wanna know if someone tried to use transformers for this purpose and if yes, then what was the outcome, and how did the results look like?","Jan 3, 2023 10:42 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"TemplateProcessing for encoder-decoder","https://discuss.huggingface.co/t/templateprocessing-for-encoder-decoder/26135","0","500","Created: Nov 16, 2022 12:30 am","astariul","I‚Äôve seen the docs on how to define the encoding format with TemplateProcessing. For example for BERT it would be : from tokenizers.processors import TemplateProcessing  tokenizer.post_processor = TemplateProcessing(     single=""[CLS] $A [SEP]"",     pair=""[CLS] $A [SEP] $B:1 [SEP]:1"",     special_tokens=[(""[CLS]"", 1), (""[SEP]"", 0)], ) But what about encoder-decoder ? What I want to achieve is : The encoder get a sentence with CLS and SEP : <cls> sen1 <sep> The decoder get a sentence with BOS and EOS : <s> sen2 </s> I call my tokenizer like : x = tokenizer([sen1], text_target=[sen2]) So my question is : how can I define TemplateProcessing to achieve this kind of format ?","Nov 16, 2022 12:30 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Token Offsets in Rust vs. Python","https://discuss.huggingface.co/t/token-offsets-in-rust-vs-python/37949","1","354","Created: Apr 27, 2023 7:21 pm Latest: Apr 27, 2023 8:17 pm","sbrunk","Hi, I‚Äôm working on Scala bindings for the (Rust) Tokenizers library. While writing tests, I realized that the offsets for certain strings seem to differ when using the Python bindings and when calling the underlying Rust implementation directly. This can also be seen directly in the quicktour docs when looking at the offsets of a ‚Äú‚Äù: print(output.offsets[9]) # (26, 27) println!(""{:?}"", output.get_offsets()[9]); // (26, 30) I suspect the difference is due to how strings are represented as bytes vs. characters, i.e. calling len() on a rust str returns the number of bytes: let input = ""üòÅ""; println!(""{}"", input.len()); // 4 There‚Äôs also OffsetType which might be related but I haven‚Äôt been able to figure out yet where this is handled on the Python side. Any hints appreciated. Thanks!","Apr 27, 2023 7:21 pm","sbrunk","I just realized that there‚Äôs also encode_char_offsets and the the encode method in Python calls it.","Apr 27, 2023 8:17 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How do we customize the number of entites for NER pretrained model?","https://discuss.huggingface.co/t/how-do-we-customize-the-number-of-entites-for-ner-pretrained-model/24085","1","348","Created: Oct 6, 2022 5:50 pm Latest: Oct 6, 2022 6:59 pm","dineshmane","Hi, I am trying to re-train the model with custom dataset with X entites. I found the documentation link: AutoModels ‚Äî transformers 3.0.2 documentation AutoModelForTokenClassification.from_pretrained(‚Äúbert-base-multilingual-cased‚Äù) Anyone knows what is the configuration to set the pre-trained huggingface model to X entities? Thank you, Dinesh","Oct 6, 2022 5:50 pm","nbroad","AutoModelForTokenClassification.from_pretrained(‚Äúbert-base-multilingual-cased‚Äù, num_labels=X)","Oct 6, 2022 6:59 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Running train_new_from_iterator to train a tokenizer is very slow","https://discuss.huggingface.co/t/running-train-new-from-iterator-to-train-a-tokenizer-is-very-slow/79333","1","345","Created: Mar 29, 2024 5:02 pm Latest: Apr 13, 2024 10:04 am","tehranixyz","Hi everyone, I‚Äôm running train_new_from_iterator on top of microsoft/unixcoder-base tokenizer, which itself is a RobertaTokenizer to train the tokenizer (it‚Äôs a BPE tokenizer). The problem is that the training is very slow. The tokenizer training process takes ages to finish the Count pair part. At first, it shows it is able to count around 1M pairs out of 14M pairs in about 1 hour, but as it continues, it slows down. It‚Äôs now running for two days and only counted 9M pairs, and still, 5M pairs are left. Is there any way to speed up the tokenizer training process? Here is the code that I‚Äôm using for training the tokenizer: from dataset import concatenate_datasets  def get_training_corpus(train_set):     for start_idx in range(0, len(train_set), 1000):         samples = train_set[start_idx: start_idx + 1000]         yield samples['text']          def train_tokenizer(tokenizer, datasets):     trainset = concatenate_datasets([dataset['train'] for dataset in datasets]).shuffle()     # remove non text columns     trainset = trainset.remove_columns([                                                                      col for col in trainset.column_names if col != ""text""                                             ])       training_corpus = get_training_corpus(train_set=trainset)     tokenizer = tokenizer.train_new_from_iterator(training_corpus, len(tokenizer))     return tokenizer It seems like not all processors are being used for training the tokenizer. But I‚Äôm not sure what I can do to utilize all processors. Any help is appreciated.","Mar 29, 2024 5:02 pm","beachbuggyracing","(Beach Buggy Racing Mod APK) amps up the excitement of beach racing with unlocked features. Similar to a speeding train, it accelerates your gaming adventure. Yet, like the slow process of training a tokenizer, unlocking levels and customizing your buggy may take time. But once you‚Äôre set, the thrill of racing on sandy tracks or coding swiftly is unmatched.","Apr 13, 2024 10:04 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Huggingface inference API issue","https://discuss.huggingface.co/t/huggingface-inference-api-issue/29307","0","486","Created: Jan 10, 2023 4:01 pm","lukasweber","Error: Can‚Äôt load tokenizer using from_pretrained, please update its configuration: No such file or directory (os error 2) These are the files that were uploaded in the huggingface models. I also checked the files of other models (like BERT-base-NER) and they also have similar files. I am not sure why this error raises‚Ä¶ Hoping for help! Thanks!","Jan 10, 2023 4:01 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Padding not transferring when loading a tokenizer trained via the tokenizers library into transformers","https://discuss.huggingface.co/t/padding-not-transferring-when-loading-a-tokenizer-trained-via-the-tokenizers-library-into-transformers/42872","0","485","Created: Jun 12, 2023 12:27 am","abhinavp","Reposting this here from the transformers forum because I got no answer there: Hi, I trained a simple WhitespaceSplit/WordLevel tokenizer using the tokenizers library. I added padding by calling enable_padding(pad_token=""<pad>"") on the Tokenizer instance. Then I saved it to a JSON file and then loaded it into transformers using the instructions here: fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=""tokenizer.json"") When using the tokenizers.Tokenizer object directly, encode correctly adds the padding tokens. However, if I try padding when tokenizing using the PreTrainedTokenizerFast instance, I get the exception: ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`. Sure enough, if I follow the instructions and add the pad token as a special token, it works. But I want the tokenizer to work out of the box, exactly as the equivalent tokenizer.Tokenizer instance does, including in terms of padding behavior. Why is this not the case? Why do I have to enable padding for the tokenizer.Tokenizer instance, and then again for the PreTrainedTokenizerFast instance? Am I doing something wrong or missing something? To reproduce the problem, you can use the code below. Most of it is from the tokenizers Quicktour, so you‚Äôll need to download the data files as per the instructions there (or modify files if using your own files). The rest is from the official transformers docs on how to load a tokenizer from tokenizers into transformers: from tokenizers import BpeTrainer, Tokenizer from tokenizers.models import BPE from tokenizers.pre_tokenizers import Whitespace from transformers import PreTrainedTokenizerFast  files = [f""data/wikitext-103-raw/wiki.{split}.raw"" for split in [""test"", ""train"", ""valid""]] sentences = [""Hello, y'all!"", ""How are you üòÅ ?""]  tokenizer = Tokenizer(BPE(unk_token=""[UNK]"")) trainer = BpeTrainer(special_tokens=[""[UNK]"", ""[CLS]"", ""[SEP]"", ""[PAD]"", ""[MASK]""]) tokenizer.pre_tokenizer = Whitespace()  tokenizer.train(files, trainer)  # Enable padding tokenizer.enable_padding(pad_id=3, pad_token=""[PAD]"")  # Now use this tokenizer to tokenize a couple of sentences. output = tokenizer.encode_batch(sentences)  # The output is padded, as it should be: print(output[0].tokens) # ['Hello', ',', 'y', ""'"", 'all', '!'] print(output[1].tokens) # ['How', 'are', 'you', '[UNK]', '?', '[PAD]']  # But now let's say we load the tokenizer into transformers- let's try loading it directly from the tokenizer object:  fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)  # Tokenize two strings of different token length with padding fast_output = fast_tokenizer(sentences, padding=True) This gives us the error: Using pad_token, but it is not set yet. Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2548, in __call__     encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)   File ""/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2634, in _call_one     return self.batch_encode_plus(   File ""/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2816, in batch_encode_plus     padding_strategy, truncation_strategy, max_length, kwargs = self._get_padding_truncation_strategies(   File ""/Users/apatil/anaconda3/envs/lm-training/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 2453, in _get_padding_truncation_strategies     raise ValueError( ValueError: Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`. We can resolve the issue by explicitly specifying the special tokens when initializing the PreTrainedTokenizerFast: fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer,  pad_token=""[PAD]"", unk_token=""[UNK]"")  # Now padding works as expected fast_output = fast_tokenizer(sentences, padding=True)  print(fast_output[0].tokens) # ['Hello', ',', 'y', ""'"", 'all', '!'] print(fast_output[1].tokens) # ['How', 'are', 'you', '[UNK]', '?', '[PAD]'] The code above uses the tokenizer_object parameter to load the fast tokenizer as a PreTrainedTokenizerFast instance, but as you can confirm for yourselves, the same behavior occurs if you first save the tokenizer to file, then load it into PreTrainedTokenizerFast using the tokenizer_file parameter instead. Bottom line: I don‚Äôt understand, if the padding information is already in the tokenizer (or in the saved tokenizer config file), why I should need to explicitly specify the padding token again when transferring the tokenizer. This introduces a lot of totally unnecessary friction into what should be a painless process. The tokenizer object/config should be self-contained. I should not have to re-hardcode what the padding token is in the code that loads it into transformers, if that information is already encapsulated in the tokenizer object or its saved config file, any more than I should need to specify the vocab file, or the pretokenizers to use, etc. That‚Äôs the whole point of the tokenizer object/config file: to uniquely determine the behavior the tokenizer. Am I doing something wrong, or is this just how this works?","Jun 12, 2023 12:27 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BERT WordPiece Tokenizer: some matras missing after tokenization for Hindi Language #572","https://discuss.huggingface.co/t/bert-wordpiece-tokenizer-some-matras-missing-after-tokenization-for-hindi-language-572/2936","0","482","Created: Dec 23, 2020 12:39 pm","akshat311","We trained our Bert WordPiece Tokenizer using the following dataset : https://drive.google.com/file/d/12MbWKERa7QPfI9F-xnMgIgAhjbN8EaQK/view?usp=sharing The words ending with certain matras (eg. ‡§è - ‡•á ) are missing these matras in the tokens. For Eg : for the sentence ""‡§Ö‡§™‡§®‡•á ‡§™‡•ã‡§∏‡•ç‡§ü ‡§ë‡§´‡§ø‡§∏ ‡§Æ‡•á‡§Ç 420 ‡§™‡§¶‡•ã‡§Ç ‡§™‡§∞ ‡§≠‡§∞‡•ç‡§§‡•Ä "" the tokens were as follows : [‚Äò‡§Ö‡§™‡§®‚Äô, ‚Äò‡§™‡•ã‡§∏‡•ç‡§ü‚Äô , ‚Äò‡§ë‡§´‡§ø‡§∏‚Äô , ‚Äò‡§Æ‚Äô , ‚Äò420‚Äô , ‚Äò‡§™‡§¶‡•ã‚Äô , ‚Äò‡§™‡§∞‚Äô , ‚Äò‡§≠‡§∞‡•ç‡§§‡•Ä‚Äô ] The first word (‡§Ö‡§™‡§®‡•á) and third word (‡§Æ‡•á‡§Ç ) are missing the ‡§è ki matra after tokenization. Even for the pretrained huggingface tokenizers, all the uncased tokenizers have the exact same issue. Words ending with ‡§è ki matra are missing the matra after tokenization. However, cased pretrained tokenizers are working fine. (‚Äúbert-base-multilingual-cased‚Äù is working perfectly fine, however, ‚Äúbert-base-multilingual-uncased‚Äù has the same issue mentioned above.) Tokenization result for ‚Äúbert-base-multilingual-cased‚Äù : [[‚Äò‡§Ö‡§™‡§®‡•á‚Äô, ‚Äò‡§™‚Äô, ‚Äò##‡•ã‚Äô, ‚Äò##‡§∏‡•ç‡§ü‚Äô, ‚Äò‡§ë‡§´‚Äô, ‚Äò##‡§ø‡§∏‚Äô, ‚Äò‡§Æ‡•á‡§Ç‚Äô, ‚Äò420‚Äô, ‚Äò##0‚Äô, ‚Äò‡§™‡§¶‚Äô, ‚Äò##‡•ã‡§Ç‚Äô, ‚Äò‡§™‡§∞‚Äô, ‚Äò‡§≠‡§∞‚Äô, ‚Äò##‡•ç‡§§‡•Ä‚Äô] Tokenization result for ‚Äúbert-base-multilingual-uncased‚Äù : [‚Äò‡§Ö‡§™‡§®‚Äô, ‚Äò‡§™‚Äô, ‚Äò##‡•ã‚Äô, ‚Äò##‡§∏‡§ü‚Äô, ‚Äò‡§ë‡§´‚Äô, ‚Äò##‡§ø‡§∏‚Äô, ‚Äò‡§Æ‚Äô, ‚Äò420‚Äô, ‚Äò##0‚Äô, ‚Äò‡§™‡§¶‚Äô, ‚Äò##‡•ã‚Äô, ‚Äò‡§™‡§∞‚Äô, ‚Äò‡§≠‡§∞‚Äô, ‚Äò##‡§§‡•Ä‚Äô] Why are these matras getting omitted after tokenization for our own tokenizer and the uncased bert tokenizers?","Dec 23, 2020 12:39 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"<unk> token in the output instead curly braces","https://discuss.huggingface.co/t/unk-token-in-the-output-instead-curly-braces/34653","0","475","Created: Mar 25, 2023 6:49 pm","MihoZaki","Hello everyone! hope you all are well. i need some help because to be honest im super beginner and it got a bit confusing. I‚Äôm fine-tuning T5 for the task of generating mongodb queries as an ouput. i finished the training and when i loaded the model to test it, the output was missing Curly braces. inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=""pt"") output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64) decoded_output = tokenizer.batch_decode(output, skip_special_tokens=False)[0] print(decoded_output) predicted_Query = nltk.sent_tokenize(decoded_output.strip())[0]  print(predicted_Query) the code above would output this: <pad> db.movies.find(<unk>""title"": ""The Poor Little Rich Girl""<unk>, <unk>""writers"": 1<unk>)</s> <pad> db.movies.find(<unk>""title"": ""The Poor Little Rich Girl""<unk>, <unk>""writers"": 1<unk>)</s> The special token is supposed to be ‚Äú{‚Äù or ‚Äú}‚Äù . is there anyway to fix that ? also does that mean that during training it also outputted <unk> tokens? Thanks for you time!","Mar 25, 2023 6:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why Bert-chinese use do_lower_case=False?","https://discuss.huggingface.co/t/why-bert-chinese-use-do-lower-case-false/2952","0","474","Created: Dec 24, 2020 1:38 pm","Fei-Wang","Some Chinese Text has some English words, for example: ‚ÄúApplesÊòØËãπÊûúÁöÑÂ§çÊï∞ÂΩ¢Âºè„ÄÇ‚Äù. I have questions about how to tokenize the text: why Chinese Bert Case sensitive, but I can‚Äôt find even ‚ÄòA‚Äô in vocab.txt Because English words in Chinese vocab.txt is few, should I use wordpiece tokenizer as default, like ""[‚Äòapple‚Äô, ‚Äò##s‚Äô, ‚ÄòÊòØ‚Äô, ‚ÄòËãπ‚Äô, ‚Ä¶]""or split to char to tokenize, like ‚Äú[‚Äòa‚Äô, ‚Äòp‚Äô, ‚Äòp‚Äô, ‚Äòl‚Äô, ‚Äòe‚Äô, ‚Äòs‚Äô, ‚ÄòÊòØ‚Äô, ‚ÄòËãπ‚Äô, ‚Ä¶]‚Äù?","Dec 24, 2020 1:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Need clarity on ‚Äúpadding‚Äù parameter in Bert Tokenizer","https://discuss.huggingface.co/t/need-clarity-on-padding-parameter-in-bert-tokenizer/27444","0","469","Created: Dec 8, 2022 7:37 am","shivammavihs","Hi All, I have been fine tuning a BERT model for sentence classification. In training, while tokenization I had passed these parameters padding=""max_length"", truncation=True, max_length=150 but while inferencing it is still predicting even if padding=""max_length"" parameter is not being passed. Surprisingly, predictions are same in both the cases when padding=""max_length"" is passed or not but if padding=""max_length"" is not being passed, inferencing is much faster. So, I need some clarity on the parameter ‚Äúpadding‚Äù in Bert Tokenizer. Can someone help me to understand how bert is able to predict even without the padding since the length of the sentences will differ and does it have any negative consequences If padding=""max_length"" is not passed while inferencing? Any help would be highly appreciated. Thanks","Dec 8, 2022 7:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Scala/JVM Bindings for Tokenizers","https://discuss.huggingface.co/t/scala-jvm-bindings-for-tokenizers/39393","0","468","Created: May 10, 2023 10:57 am","sbrunk","I‚Äôve just published Scala bindings for the Rust Tokenizers library to be able to use them on the JVM. Currently, it can only load and run pre-trained tokenizers. Training is not yet possible. While it is currently focused on Scala it should be straightforward to add pure Java support as well if there is interest in that. Here‚Äôs the project: GitHub - sbrunk/tokenizers-scala: Scala bindings for Hugging Face Tokenizers","May 10, 2023 10:57 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"EM training on unigram tokenizer taking way longer than predicted","https://discuss.huggingface.co/t/em-training-on-unigram-tokenizer-taking-way-longer-than-predicted/19502","0","467","Created: Jun 23, 2022 11:39 am Latest: Jun 23, 2022 11:39 am","ddeerreekk","I am trying to train my own Sentencepiece unigram tokenizer, but the EM training step is taking nearly ~2x predicted iterations to prune the vocabulary. I was wondering if this is normal and whether I should retry with larger shrink factor/vocabulary. Any hints? Thanks! [02:22:46] Pre-processing sequences                 ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 0        /        0 [00:11:52] Suffix array seeds                       ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 43102066 / 43102066 [1d 00:50:16] EM training                              ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 72       /       34","Jun 23, 2022 11:39 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MBart50Tokenizer vs XLMRobertaTokenizer","https://discuss.huggingface.co/t/mbart50tokenizer-vs-xlmrobertatokenizer/8498","0","466","Created: Jul 19, 2021 4:15 pm","yoshi","Hello, I am currently working on the MBART50 many-to-one model for translation. I noticed that the tokenizer took a long time to encode the input when the length of the sequence became relatively important (cf attached picture). So, instead of using the MBart50TokenizerFast I used to use the XLMRobertaTokenizerFast. Actually, both are based on sentencepiece.bpe.model and have almost the same vocabulary. Using XLMRobertaTokenizerFast, I got practically the same translation as if I had used MBart50TokenizerFast BUT with a much shorter execution time. Does anyone have any idea why there is so much difference in execution time between these two tokenizers? (One can find the evolution of the execution time - encoding time - depending on the input-string length of the two tokenizers in attached picture) Thank you ! Code used : mbart_tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50-many-to-one-mmt"") xlm_tokenizer = XLMRobertaTokenizerFast.from_pretrained(""xlm-roberta-base"") mbart_token = mbart_tokenizer(input, return_tensors='pt') xlm_token = xlm_tokenizer(input, return_tensors='pt')","Jul 19, 2021 4:15 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to create a HF tokenizer‚Äôs vocab file from a BPE model‚Äôs merges.txt file?","https://discuss.huggingface.co/t/how-to-create-a-hf-tokenizers-vocab-file-from-a-bpe-models-merges-txt-file/39737","0","464","Created: May 13, 2023 9:37 am","junliu44","I have a BPE merges file that has been trained by another trainer. How can I convert it to the vocab format of the hf tokenizer? Because I don‚Äôt want to spend a lot of time retraining the BPE model.","May 13, 2023 9:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Converting JSON/dict to flatten string with indicator tokens","https://discuss.huggingface.co/t/converting-json-dict-to-flatten-string-with-indicator-tokens/37387","1","325","Created: Apr 21, 2023 7:41 pm Latest: Apr 21, 2023 7:42 pm","alvations","Given an input like: {'example_id': 0,  'query': ' revent 80 cfm',  'query_id': 0,  'product_id': 'B000MOO21W',  'product_locale': 'us',  'esci_label': 'I',  'small_version': 0,  'large_version': 1,  'split': 'train',  'product_title': 'Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan',  'product_description': None,  'product_bullet_point': 'WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air\nDesigned to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace\nDetachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation\nThis Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan\n0.35 amp',  'product_brand': 'Panasonic',  'product_color': 'White'} The goal is to output something that looks like: Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan [TITLE] Panasonic [BRAND] White [COLOR] WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air [SEP] Designed to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace [SEP] Detachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation [SEP] This Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan [SEP] 0.35 amp [BULLETPOINT] There‚Äôs a few operations going on to generate the desired output following the rules: If the values in the dictionary is None, don‚Äôt add the content to the output string If the values contains newline \n substitute them with [SEP] tokens Concatenate the strings with in order that user specified, e.g. above follows the order [""product_title"", ""product_brand"", ""product_color"", ""product_bullet_point"", ""product_description""] I‚Äôve tried this that kinda works but the function I‚Äôve written looks a little to hardcoded to look through the wanted keys and concatenate and manipulate the strings. item1 = {'example_id': 0,  'query': ' revent 80 cfm',  'query_id': 0,  'product_id': 'B000MOO21W',  'product_locale': 'us',  'esci_label': 'I',  'small_version': 0,  'large_version': 1,  'split': 'train',  'product_title': 'Panasonic FV-20VQ3 WhisperCeiling 190 CFM Ceiling Mounted Fan',  'product_description': None,  'product_bullet_point': 'WhisperCeiling fans feature a totally enclosed condenser motor and a double-tapered, dolphin-shaped bladed blower wheel to quietly move air\nDesigned to give you continuous, trouble-free operation for many years thanks in part to its high-quality components and permanently lubricated motors which wear at a slower pace\nDetachable adaptors, firmly secured duct ends, adjustable mounting brackets (up to 26-in), fan/motor units that detach easily from the housing and uncomplicated wiring all lend themselves to user-friendly installation\nThis Panasonic fan has a built-in damper to prevent backdraft, which helps to prevent outside air from coming through the fan\n0.35 amp',  'product_brand': 'Panasonic',  'product_color': 'White'}  item2 = {'example_id': 198,  'query': '# 2 pencils not sharpened',  'query_id': 6,  'product_id': 'B08KXRY4DG',  'product_locale': 'us',  'esci_label': 'S',  'small_version': 1,  'large_version': 1,  'split': 'train',  'product_title': 'AHXML#2 HB Wood Cased Graphite Pencils, Pre-Sharpened with Free Erasers, Smooth write for Exams, School, Office, Drawing and Sketching, Pack of 48',  'product_description': ""<b>AHXML#2 HB Wood Cased Graphite Pencils, Pack of 48</b><br><br>Perfect for Beginners experienced graphic designers and professionals, kids Ideal for art supplies, drawing supplies, sketchbook, sketch pad, shading pencil, artist pencil, school supplies. <br><br><b>Package Includes</b><br>- 48 x Sketching Pencil<br> - 1 x Paper Boxed packaging<br><br>Our high quality, hexagonal shape is super lightweight and textured, producing smooth marks that erase well, and do not break off when you're drawing.<br><br><b>If you have any question or suggestion during using, please feel free to contact us.</b>"",  'product_bullet_point': '#2 HB yellow, wood-cased pencils:Box of 48 count. Made from high quality real poplar wood and 100% genuine graphite pencil core. These No 2 pencils come with 100% Non-Toxic latex free pink top erasers.\nPRE-SHARPENED & EASY SHARPENING: All the 48 count pencils are pre-sharpened, ready to use when get it, saving your time of preparing.\nThese writing instruments are hexagonal in shape to ensure a comfortable grip when writing, scribbling, or doodling.\nThey are widely used in daily writhing, sketching, examination, marking, and more, especially for kids and teen writing in classroom and home.#2 HB wood-cased yellow pencils in bulk are ideal choice for school, office and home to maintain daily pencil consumption.\nCustomer service:If you are not satisfied with our product or have any questions, please feel free to contact us.',  'product_brand': 'AHXML',  'product_color': None}   def product2str(row, keys):     key2token = {'product_title': '[TITLE]',       'product_brand': '[BRAND]',       'product_color': '[COLOR]',      'product_bullet_point': '[BULLETPOINT]',       'product_description': '[DESCRIPTION]'}          output = """"     for k in keys:         content = row[k]         if content:             output += content.replace('\n', ' [SEP] ') + f"" {key2token[k]} ""      return output.strip()  product2str(item2, keys=['product_title', 'product_brand', 'product_color',                         'product_bullet_point', 'product_description']) Q: Is there some sort of native CPython JSON to str flatten functions/recipes that can achieve similar results to the product2str function? Q: Or is there already some function/pipeline in tokenizers library tokenizers ¬∑ PyPI that can flatten a JSON/dict into tokens?","Apr 21, 2023 7:41 pm","alvations","Also asked on python - Converting JSON/dict to flatten string with indicator tokens - Stack Overflow","Apr 21, 2023 7:42 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Enhaced word_ids() API for Chinese or CJK languages?","https://discuss.huggingface.co/t/enhaced-word-ids-api-for-chinese-or-cjk-languages/18662","0","454","Created: Jun 2, 2022 5:52 pm","cramraj8","Is there an API like tokenizer.word_ids() to map/align sub-word to whole-word in CJK languages ? The word_ids() is useful for white-space tokenizable languages like Farsi and Russian. But I have difficulty in mapping Chinese to get the whole-word vocabulary embeddings.","Jun 2, 2022 5:52 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Encode_plus Pretokenized input seuqence must be Union","https://discuss.huggingface.co/t/encode-plus-pretokenized-input-seuqence-must-be-union/26449","0","453","Created: Nov 21, 2022 2:19 pm","Adelija","Hello, I have a problem with arguments types for the encode_plus method. The code works when we put, for example, df[‚Äòlabels‚Äô][0] or any other index (so when it is list of strings), but not with the whole column (df[‚Äòlabels‚Äô].to_list(), so when it is list of lists of strings). We need to tokenize the whole data frame, so our question is - is there another function that can do that, or we need to iterate through the data frame and call encode_plus() on each separate cell value? Thanks","Nov 21, 2022 2:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Is there a way to save a pre-compiled AutoTokenizer?","https://discuss.huggingface.co/t/is-there-a-way-to-save-a-pre-compiled-autotokenizer/70581","1","318","Created: Jan 25, 2024 12:25 pm Latest: Jan 25, 2024 12:26 pm","alvations","Sometimes, we‚Äôll have to do something like this to extend a pre-trained tokenizer: from transformers import AutoTokenizer  from datasets import load_dataset   ds_de = load_dataset(""mc4"", 'de') ds_fr = load_dataset(""mc4"", 'fr')  de_tokenizer = tokenizer.train_new_from_iterator(     ds_de['text'],vocab_size=50_000 )  fr_tokenizer = tokenizer.train_new_from_iterator(     ds_fr['text'],vocab_size=50_000 )  new_tokens_de = set(de_tokenizer.vocab).difference(tokenizer.vocab) new_tokens_fr = set(fr_tokenizer.vocab).difference(tokenizer.vocab) new_tokens = set(new_tokens_de).union(new_tokens_fr)   tokenizer = AutoTokenizer.from_pretrained(     'moussaKam/frugalscore_tiny_bert-base_bert-score' )  tokenizer.add_tokens(list(new_tokens))  tokenizer.save_pretrained('frugalscore_tiny_bert-de-fr') And then when loading the tokenizer, tokenizer = AutoTokenizer.from_pretrained(   'frugalscore_tiny_bert-de-fr', local_files_only=True ) It takes pretty long to load from %%time in a Jupyter cell: CPU times: user 34min 20s Wall time: 34min 22s I guess this is due to regex compilation for the added tokens which was also raised in Loading of Tokenizer is really slow when there are lots of additional tokens ¬∑ Issue #914 ¬∑ huggingface/tokenizers ¬∑ GitHub I think it‚Äôs okay since it‚Äôll load once and the work can be done without redoing the regex compiles. But, is there a way to just save the tokenizer in binary form and avoid the whole regex compilation the next time?","Jan 25, 2024 12:25 pm","alvations","Also asked on nlp - Is there a way to save a pre-compiled AutoTokenizer? - Stack Overflow","Jan 25, 2024 12:26 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issues with BPE tokenizer","https://discuss.huggingface.co/t/issues-with-bpe-tokenizer/70381","2","260","Created: Jan 24, 2024 8:49 am Latest: Jan 24, 2024 4:09 pm","Imran1","I attempted to train the BPE Hugging Face tokenizer for Pashto from scratch, but it is not decoding words correctly.","Jan 24, 2024 8:49 am","malikarumi","No one can help you with such limited information. I am unfamiliar with Pashto. Does the official documentation say anything about BPE? Is it possible that Pashto has already been pre trained with a different tokenizer? Post your code so we can understand what the software is trying to tell you.","Jan 24, 2024 3:56 pm","Imran1","I solve the issues.","Jan 24, 2024 4:09 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to concatenate an answer to multiple choices after padded tokenization","https://discuss.huggingface.co/t/how-to-concatenate-an-answer-to-multiple-choices-after-padded-tokenization/26110","0","441","Created: Nov 15, 2022 2:52 pm","drt","I am evaluating a LM model on multiple choice questions. I want to dynamically concatenate each choice to the question after tokenization. An tokenization example: Q: How many hours are there in a day? ‚Äîtokenize‚Äî> 23 67 52 13 78 [PAD] [PAD] [PAD] [PAD] [PAD] C1: 12 hours. ‚Äîtokenize‚Äì> 312 [PAD] [PAD] C2: 24 hours. ‚Äîtokenize‚Äî> 89 [PAD] [PAD] I‚Äôd like to make two question choice pairs: 23 67 52 13 78 312 [PAD] [PAD] [PAD] [PAD] 23 67 52 13 78 89 [PAD] [PAD] [PAD] [PAD] I currently do this at tokenization time ‚Äì I concatenate the texts and feed to tokenizers. But this is very inconvenient when, i.e. I also want question and answers separately. Also, I think this is clumsy and will cause extra GPU memory consumption and extra space to save tokenized dataset on disk. Therefore, is there an elegant way to do this concatenation after tokenization?","Nov 15, 2022 2:52 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why does tokenization take so long?","https://discuss.huggingface.co/t/why-does-tokenization-take-so-long/102098","0","79","Created: Aug 13, 2024 4:13 am","Jason-Hwang1","I found that tokenization steps before training takes longer time than training itself. Yes, Training involves GPU, but I thought tokenization is not that compute-intensive (just splitting the sentence into words and mapping words to ID and other substeps‚Ä¶) so I thought it should be bounded to the IO time for loading the raw dataset. But it takes much time to tokenize subset of Wikipedia for more than 2 hours. Can someone give me the reason why tokenization steps takes so long? The code that I used for tokenization is as below. I also tried multiprocessing but it doesn‚Äôt make meaningful difference. tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')      def encode_example(example):         return tokenizer(example['text'], truncation=True, padding='max_length', max_length=64)  # Reduce max_length to save memory      # Tokenize dataset     dataset = dataset.map(encode_example, batched=True)","Aug 13, 2024 4:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Application of TFBertTokenizer","https://discuss.huggingface.co/t/application-of-tfberttokenizer/26437","0","435","Created: Nov 21, 2022 10:31 am","realonbebeto","I wish to use the TFBertTokenizer as a layer in my custom model where there is: Input Layer - Receiving String Tokenizer to process string tensor Bert model Other layers of my custom layer The model is for classification problem and the intentions are to have a model that has both preprocessing as part of the model. I‚Äôm thinking this is possible since TFBertTokenizer is an in-graph tokenizer. Please help.","Nov 21, 2022 10:31 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Change bpe-dropout value on the fly?","https://discuss.huggingface.co/t/change-bpe-dropout-value-on-the-fly/1721","0","427","Created: Oct 24, 2020 11:39 am","esceptico","Hello! Is there‚Äôs any way to change bpe-dropout value of tokenizer without reinitialization tokenizer from files? Many thanks!","Oct 24, 2020 11:39 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Overflowing Tokens in MarkupLM","https://discuss.huggingface.co/t/overflowing-tokens-in-markuplm/35217","0","425","Created: Mar 31, 2023 8:31 pm Latest: Mar 31, 2023 8:31 pm","jl2010","When using the MarkupLMTokenizerFast there is an argument to return_overflowing_tokens. This makes sense since the model can only handle 512 tokens at a time. For example, if my data has 1024 tokens the tokenizer would return a tensor of size(2,512). My question is, does the model consider the overflowing tokens when it‚Äôs training? If so can you point me to where it‚Äôs actually training on that data? It‚Äôs not clear to me that this is happening and want to make sure either way. Thank you!","Mar 31, 2023 8:31 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unigram vocab_size doesn‚Äôt fit","https://discuss.huggingface.co/t/unigram-vocab-size-doesnt-fit/26856","0","419","Created: Nov 28, 2022 11:06 am Latest: Nov 28, 2022 11:33 am","Axllm","Hi there, thanks for your amazing work ! I encounter an issue when I‚Äôm using Unigram and UnigramTrainer. In fact I want to build a vocabulary of size 10 (based on vocabulary built with BPE) But I don‚Äôt understand why, using both train_from_iterator (with list and with iterator) and with train (from file name containing same information as in list) I can‚Äôt retrieve a vocab of wished size. Here is some code to work with: corpus = [""table"", ""bleu"", ""cable""] # tokenizers.__version__ == 0.13.2 from tokenizers import Tokenizer from tokenizers.models import Unigram  vocab = [""a"", ""b"", ""c"", ""e"", ""l"", ""t"", ""u"", ""bl"", ""ble"", ""able"", ""cable""] tokenizer = Tokenizer(Unigram()) tokenizer.add_tokens(vocab) print(""With added tokens"") print(tokenizer.get_vocab(with_added_tokens=True)) print(tokenizer.get_vocab_size(with_added_tokens=True)) print(""Without added tokens"") print(tokenizer.get_vocab(with_added_tokens=False)) print(tokenizer.get_vocab_size(with_added_tokens=False))  from tokenizers.trainers import UnigramTrainer trainer = UnigramTrainer(vocab_size=10)#, initial_alphabet=[""a"", ""b"", ""c"", ""e"", ""l"", ""t"", ""u""]) tokenizer.train_from_iterator(corpus, trainer=trainer) #tokenizer.train([""corpus.txt""], trainer=trainer) #tokenizer.train_from_iterator(iterator(), trainer=trainer)  print(""With added tokens"") print(dict(sorted(tokenizer.get_vocab(with_added_tokens=True).items(), key=lambda item: item[1]))) print(tokenizer.get_vocab_size(with_added_tokens=True) )  print(""Without added tokens"") print(dict(sorted(tokenizer.get_vocab(with_added_tokens=False).items(), key=lambda item: item[1]))) print(tokenizer.get_vocab_size(with_added_tokens=False) ) And results associated :","Nov 28, 2022 11:06 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"I/O error calling ToenizersLibrary.createTokenizer in container","https://discuss.huggingface.co/t/i-o-error-calling-toenizerslibrary-createtokenizer-in-container/73204","1","296","Created: Feb 14, 2024 6:35 pm Latest: Feb 23, 2024 4:33 pm","wnmills3","I‚Äôm using code like:     static String DJL_MODEL = ""intfloat/multilingual-e5-base"";     static String DJL_PATH = ""djl://ai.djl.huggingface.pytorch/"" + DJL_MODEL;     static private HuggingFaceTokenizer huggingFaceTokenizer; ...     static private HuggingFaceTokenizer getHuggingFaceTokenizer() {         if (huggingFaceTokenizer == null) {             huggingFaceTokenizer = HuggingFaceTokenizer.newInstance(DJL_MODEL,                 getDJLConfig());         }         return huggingFaceTokenizer;     } and it works fine running from the command line. However, when executing the same code in a Docker container I am getting an error: I/O error Permission denied (os error 13) RuntimeException: I/O error Permission denied (os error 13) ai.djl.huggingface.tokenizers.jni.TokenizersLibrary.createTokenizer(Native Method) ai.djl.huggingface.tokenizers.HuggingFaceTokenizer.newInstance(HuggingFaceTokenizer.java: 109) I‚Äôm sure the issue has to do with permissions in the container, but I have no idea what happens when loading the instance so I don‚Äôt know which directory is needed to have its attributes changed. Unfortunately, there method resolves to a native library so I can‚Äôt figure out what is causing this error. Any help is greatly appreciated.","Feb 14, 2024 6:35 pm","wnmills3","Set the HF_HOME environment variable went starting the container to a directory that can be written to by the default user in the container.","Feb 23, 2024 4:33 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Get intermediate tokens and merges used in tokenization","https://discuss.huggingface.co/t/get-intermediate-tokens-and-merges-used-in-tokenization/64256","0","415","Created: Dec 1, 2023 1:40 pm","cwallenwein","Hi Friends Is there a way to get intermediate tokens and merges used during BPE tokenization? Example: vocab: ‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúc‚Äù, ‚Äúab‚Äù, ‚Äú‚Äù, ‚Äúabc‚Äù merges: ‚Äúa b‚Äù, ‚Äúb c‚Äù, ‚Äúab c‚Äù What I want: tokenize(‚Äúabc‚Äù): {‚Äúintermediate_tokens‚Äù: [‚Äúa‚Äù, ‚Äúb‚Äù, ‚Äúab‚Äù, ‚Äúc‚Äù], ‚Äúintermediate_merges‚Äù: [‚Äúa b‚Äù]} I currently solve this by manually implementing BPE in Python, but my implementation is too slow","Dec 1, 2023 1:40 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Bug in Offset generation for Rupee symbol","https://discuss.huggingface.co/t/bug-in-offset-generation-for-rupee-symbol/19655","0","412","Created: Jun 27, 2022 7:32 am Latest: Jun 27, 2022 7:34 am","Giriteja","Hi, When I am using the rupee symbol in a sentence Offset is dividing that symbol into 3 different symbols but instead of having (0,1)(1,2)(2,3), it is giving (0,1)(0,1)(0,1) which is causing issues in a mismatch between actual words and generated labels.For example from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(‚Äòdistilroberta-base‚Äô, add_prefix_space=True) sent=‚Äútotal amount that need to be paid is ‚Çπ 500‚Äù words=sent.split() output=tokenizer(words, is_split_into_words=True,return_offsets_mapping=True) tokens=output.tokens() offset=output[‚Äòoffset_mapping‚Äô] for token,offset in zip(tokens,offset): print(token,‚Äú----->‚Äù,offset) I am getting the following output -----> (0, 0) ƒ†total -----> (0, 5) ƒ†amount -----> (0, 6) ƒ†that -----> (0, 4) ƒ†need -----> (0, 4) ƒ†to -----> (0, 2) ƒ†be -----> (0, 2) ƒ†paid -----> (0, 4) ƒ†is -----> (0, 2) ƒ†√¢ -----> (0, 1) #problem ƒ§ -----> (0, 1)#problem ¬π -----> (0, 1)#poblem ƒ†500 -----> (0, 3) -----> (0, 0) As you can see above rupee symbol got divided in to 3 different labels but offset is still (0,1) for all three symbols","Jun 27, 2022 7:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with XLM-RoBERTa tokenizer","https://discuss.huggingface.co/t/issue-with-xlm-roberta-tokenizer/38311","1","290","Created: May 1, 2023 2:57 pm Latest: Aug 15, 2024 7:11 pm","ozanarmagan","Hi; I have a problem with encoding with XLM-RoBERTa sentencepiece tokenizer. Why is the hugging face encoding 1 greater compared to the google sentencepiece encoding? Example ## Hugging Face: tokenizer_xlmroberta.encode(""I don't understand why"",add_special_tokens=False) Output: [87, 2301, 25, 18, 28219, 15400] ## Sentencepiece: tokenizer_xlmroberta_.encode_as_ids(""I don't understand why"") Output: [86, 2300, 24, 17, 28218, 15399]","May 1, 2023 2:57 pm","stryptsTester99","Hey I am having the same issue, have you found any solution? Kind regards","Aug 15, 2024 7:11 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Create a simple tokenizer","https://discuss.huggingface.co/t/create-a-simple-tokenizer/31677","0","404","Created: Feb 14, 2023 6:57 am","beannn","I have a preprocessed dataset. The tokens are split by whitespace. So I need a very simple tokenizer to load this. Is there any advice about how to create this?","Feb 14, 2023 6:57 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ByteLevelBPETokenizer inconsistent behavior","https://discuss.huggingface.co/t/bytelevelbpetokenizer-inconsistent-behavior/442","0","402","Created: Jul 23, 2020 3:47 pm Latest: Jul 23, 2020 3:50 pm","mstekel","Hi, I encountered a weird behavior of the ByteLevelBPETokenizer: this publicly available notebook is parameterized to run on two almost identical text files. The first one is a transliteration of the Hebrew Bible text while the second one is the same transliteration with 2 modifications - the ‚Äò and ‚Äô characters are replaced by the Hebrew letters ◊¢ and ◊ê accordingly (the transliteration used these 2 types of apostrophes for denoting these Hebrew consonants). After training on the first file(tanach_translit_orig) the tokenizer is given a test sentence for encoding and results with 19 tokens but when running the same process on the second file and the same test sentence modified accordingly (by replacing the apostrophes by the Hebrew letters) the tokenizer results with 9 tokens. I assumed the ByteLevelBPETokenizer to be agnostic to the character meanings and I cannot understand why the results vary between the experiments. Can anyone shed some light please? P.S. for toggling between the files all you need is to un-comment the appropriate line in the second step of the notebook: Thank you in advance, Moshe.","Jul 23, 2020 3:47 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A question about the DataCollator for LM","https://discuss.huggingface.co/t/a-question-about-the-datacollator-for-lm/85273","2","228","Created: May 5, 2024 3:34 pm Latest: May 6, 2024 9:06 am","dydavide","I have a question about DataCollatorForLanguageModeling, when training a LM. I saw this video, which explains very well, how the training process works. Starting from minute 5:12 it says ‚Äúthe datacollator shifts the input, such that the label is the next token in the sequence for every single token in the input‚Äù It make sense to me and it is a nice explanation about what is happening behind the scene. But then, looking into the documentation for understanding the mlm parameter I found the following: mlm (bool, optional, defaults to True) ‚Äî Whether or not to use masked language modeling. If set to False, the labels are the same as the inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for non-masked tokens and the value to predict for the masked token. So, now I‚Äôm totally confused. Is DataCollator shifting the tokens to the left? Or it is controlling only the behavior of the padding tokens? Thanks","May 5, 2024 3:34 pm","nielsr","Hi, No the data collator itself is not shifting any tokens. Typically the labels are just a copy of the input_ids, with padding tokens replaced by -100 (which is the ignore index of the cross-entropy loss in PyTorch). The shifting of the tokens one position happens inside the model (hence the user doesn‚Äôt need to take care of that). This can be seen here for Llama for instance.","May 6, 2024 9:06 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"1 line code for NER data set preparation using tokenizer library!","https://discuss.huggingface.co/t/1-line-code-for-ner-data-set-preparation-using-tokenizer-library/22816","0","391","Created: Sep 9, 2022 3:40 am Latest: Sep 9, 2022 3:40 am","Imran1","you can see this code is hard to read and understand. Is there any easy way in which we can simplify 15 line of code to 1 line??? def tokenize_and_align_labels(examples):     tokenized_inputs = tokenizer(examples[""tokens""], truncation=True, is_split_into_words=True)      labels = []     for i, label in enumerate(examples[f""{task}_tags""]):         word_ids = tokenized_inputs.word_ids(batch_index=i)         previous_word_idx = None         label_ids = []         for word_idx in word_ids:             # Special tokens have a word id that is None. We set the label to -100 so they are automatically             # ignored in the loss function.             if word_idx is None:                 label_ids.append(-100)             # We set the label for the first token of each word.             elif word_idx != previous_word_idx:                 label_ids.append(label[word_idx])             # For the other tokens in a word, we set the label to either the current label or -100, depending on             # the label_all_tokens flag.             else:                 label_ids.append(label[word_idx] if label_all_tokens else -100)             previous_word_idx = word_idx          labels.append(label_ids)      tokenized_inputs[""labels""] = labels     return tokenized_inputs","Sep 9, 2022 3:40 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Configure RobertaTokenizer","https://discuss.huggingface.co/t/configure-robertatokenizer/23969","0","388","Created: Oct 4, 2022 4:26 am","srishti-hf1110","Hi, I am willing to configure RobertaTokenizer such that it outputs token_type_ids that it doesn‚Äôt by default. Is there a way to do that? I have changed the model configuration and updated its type_vocab_size to 2, like so: model = RobertaModel.from_pretrained('roberta-base')  # Update config to finetune token type embeddings model.config.type_vocab_size = 2   # Create a new Embeddings layer, with 2 possible segments IDs instead of 1 model.embeddings.token_type_embeddings = nn.Embedding(2, model.config.hidden_size)                  # Initialize it model.embeddings.token_type_embeddings.weight.data.normal_(mean=0.0, std=model.config.initializer_range) I want to input token_type_ids to the model instance like so: model(token_ids, attn_masks, token_type_ids)","Oct 4, 2022 4:26 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BertTokenizer‚Äôs encode_plus returns 2d tensor when printing ‚Äòinput_ids‚Äô/ ‚Äòattention_mask‚Äô","https://discuss.huggingface.co/t/berttokenizers-encode-plus-returns-2d-tensor-when-printing-input-ids-attention-mask/3530","0","384","Created: Feb 7, 2021 11:40 pm","AnkitaDas","When using BertTokenizer‚Äôs encode_plus, why does tokenized[‚Äòinput_ids‚Äô] return a 2d tensor? I understand when batch_encode_plus does the same(an extra dimension for containing the batch size). But what‚Äôs the purpose of returning a 2d tensor in encode_plus? The returned tensor size is of the form [1,N].","Feb 7, 2021 11:40 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Seeking an end-to-end example of grouping, tokenization and padding to construct preprocessed data in HF","https://discuss.huggingface.co/t/seeking-an-end-to-end-example-of-grouping-tokenization-and-padding-to-construct-preprocessed-data-in-hf/44602","0","383","Created: Jun 26, 2023 12:19 pm Latest: Jun 26, 2023 12:25 pm","mdrpanwar","Hi, I am writing the code for training an LM from scratch on a custom dataset following the run_clm_no_trainer.py file. Please help me with the following questions. I could not find clear answers in the docs. Please point me to references in case I may have missed them. An example in the batch should look like this: BOS example_1 EOS BOS example_2 EOS ‚Ä¶ BOS example_n EOS PAD ‚Ä¶ PAD Q1. Is there an efficient method for grouping and padding data examples like above? My use cases: (1) pad s.t. the maximum number of data examples fit in context, (2) pad s.t. k data examples fit in the context (where k is a fixed natural number). run_clm_no_trainer.py has a custom method group_texts that does grouping (no padding) that can be modified. However, I think my use cases are pretty standard and some built-in method should exist. Q2. [Design Choice] Is using a SEP token better than using the BOS and EOS? What are the considerations here? Q3. What tokens do I need to update in the tokenizer and model? I have BOS, EOS, PAD and UNK (or SEP in place of BOS and EOS depending on the answer to Q2). UNK goes into the tokenizer while instantiating the tokenizer‚Äôs model (WordLevel in my case). BOS and EOS token IDs go in the model config (I am using GPT-2). What about PAD and SEP? Also, what changes are needed from my end if I want to use EOS as PAD? Q4. HF won‚Äôt compute the loss for the predictions at special tokens, right? I read somewhere that tokenizer sets labels as -100 for special tokens which are ignored while computing the loss. Please confirm if that is correct. Q5. What should the token_type_ids be? Since I am using GPT-2, should I care about token_type_ids? If yes, what does GPT-2 expect ‚Äì same token_type_ids for all tokens or different token_type_ids for each segment (i.e. ‚ÄúBOS example_k EOS‚Äù gets k-th token_type_id) in a batch sentence like in BERT? If the questions are more appropriate for the Transformers forum, please let me know. Thanks.","Jun 26, 2023 12:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Customization of Wav2Vec2CTCTokenizer with rules","https://discuss.huggingface.co/t/customization-of-wav2vec2ctctokenizer-with-rules/21912","0","382","Created: Aug 22, 2022 10:52 am Latest: Aug 22, 2022 10:52 am","spicci","Hi, my goal is to fine-tune an ASR model, WavLM, that relies on the pretrained tokenizer Wav2Vec2CTCTokenizer. I want to fine-tune this ASR model with another language and to perform the tokenization according to phonological rules, such as syllable segmentation. Providing a vocabulary with all the possible syllables (aka my tokens), is it possible to customize the Wav2Vec2CTCTokenizer segmentation so that it will respect syllable segmentation rules? Example: Original sentence: Il tentativo era cosi bello Segmentation made by Wav2Vec2CTCTokenizer (not respecting syllabification rules): [‚Äòil‚Äô, ‚Äòten‚Äô, ‚Äòtat‚Äô, ‚Äòiv‚Äô, ‚Äòo‚Äô, ‚ÄòEr‚Äô, ‚Äòa‚Äô, ‚Äòkos‚Äô, ‚Äòi‚Äô, ‚ÄòbEl‚Äô, ‚Äòlo‚Äô] Expected segmentation according to syllabification rules: [‚Äòil‚Äô, ‚Äòten‚Äô, ‚Äòta‚Äô, ‚Äòti‚Äô, ‚Äòvo‚Äô, ‚ÄòE‚Äô, ‚Äòra‚Äô, 'ko, ‚Äòsi‚Äô, ‚ÄòbEl‚Äô, ‚Äòlo‚Äô] Basically, I need to state and include some rules in the tokenizer, for example to give priority to tokens with a consonant in the onset position instead of in the coda of the syllable. Is it possible to insert this kind of rules in the tokenizer? If so, where can I modify these parameters? If not, if I train a new tokenizer, will it be ok to implement it in the pre-trained WavLm model that I need to fine-tune? Thanks in advance!","Aug 22, 2022 10:52 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GPT2Tokenizer not working in Kaggle Notebook","https://discuss.huggingface.co/t/gpt2tokenizer-not-working-in-kaggle-notebook/41508","0","373","Created: May 30, 2023 2:58 pm","admarcosai","Hello I have been trying to tokenize the WMT14 en-de. I first started in a free google colab and the following code worked there; I then switched to Kaggle notebook since it is a better environment but it doesn‚Äôt work there for some reason. The error it throws is below: from transformers import GPT2Tokenizer bpe_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  def tokenization(examples):     source, target = [], []     for example in examples:         trgt = bpe_tokenizer(example['de'])         src = bpe_tokenizer(example['en'])         target.append(trgt)         source.append(src)              return {'de': target,             'en': source}  train_dataset = dataset_de_en['train'].map(lambda examples: tokenization(examples['translation']), batched=True) test_dataset = dataset_de_en['test'].map(lambda examples: tokenization(examples['translation']), batched=True) val_dataset = dataset_de_en['val'].map(lambda examples: tokenization(examples['translation']), batched=True) ArrowInvalid: Could not convert {'input_ids': [54, 798, 263, 559, 22184, 993, 1326, 4587, 311, 4224, 2150, 82, 525, 72, 1098], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} with type BatchEncoding: did not recognize Python value type when inferring an Arrow data type Is there an explanation to this?","May 30, 2023 2:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Skip-gram tokens","https://discuss.huggingface.co/t/skip-gram-tokens/5294","0","368","Created: Apr 4, 2021 6:02 am","vitali","Is there any way to produce skip-gram tokens with tokenizers library?","Apr 4, 2021 6:02 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Training tokenizers with padding in between tokens","https://discuss.huggingface.co/t/training-tokenizers-with-padding-in-between-tokens/59173","0","367","Created: Oct 19, 2023 6:43 pm","akanakia","Hi folks, I am trying to train a set of tokenizers (BPE, WordPiece, and Unigram) on a dataset containing antibody sequences. The issue is that antibody sequences are usually pre-aligned using some aligment scheme like cabat or IMGT. This results in padding tokens being introduced in the middle of the sequence which never happens when tokenizing text. E.g., An aligned antibody sequence can look something like this ‚ÄúQVQT‚ÄìTYHHH ASTR-MTPY Q-----QY‚Äù, with ‚Äú-‚Äù being the pad token introduced during sequence alignment. I would like the tokenizers to essentially treat the pad token as an ‚Äúunk‚Äù token during pretraining but there is already an unk token in the initial vocab, usually denoted by ‚ÄúX‚Äù representing an unknown amino acid in a sequence. Is there some way to enforce learned tokens do no contain any padding characters in them using huggingface.tokenizers? Any help would be really appreciated. Thanks.","Oct 19, 2023 6:43 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BpeTrainer implementation in Python","https://discuss.huggingface.co/t/bpetrainer-implementation-in-python/8625","0","368","Created: Jul 23, 2021 12:58 pm","amankhandelia","Hi there, So I was perusing through the tokenizer library in order pick some knowledge about the nitty gritty of the BpeTrainer, although I was only able to find the rust implementation (github.com), after bit more of digging I came across the fact that the tokenizer library only offers bindings for other languages not any native implementations, so my question is, is there any good implementation of the BpeTrainer or similar functionality in python, which one can refer Thanks in advance.","Jul 23, 2021 12:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Map tokenization and posterior to smaller substrings","https://discuss.huggingface.co/t/map-tokenization-and-posterior-to-smaller-substrings/23792","0","365","Created: Sep 29, 2022 5:38 pm","mdelas","Dear community, My objective is to have sequences of 1024 tokens ready to GPT-2. As input data, I have DNA sequences of 2E5 length, and I have built a BPE tokenizer using this data. I am trying to build a function which concatenates: Tokenize the full sequences (2E5 max_length) Chunk data into 1024 sequence length and enlarge the number of rows of my Huggingface Dataset For now I am trying to use this function, and map it posterior to the Dataset: def tokenize_and_chunk(examples):     chunks = []     tokenized_inputs = fast_tokenizer(         examples['text'],         max_length=200000,         truncation=True     )      for sentence in examples['text']:         chunks += [sentence[i:i + 1024] for i in range(0, len(sentence), 1024)]      return {""chunks"": chunks}   chunked_data = dna_data.map(tokenize_and_chunk, remove_columns=dna_data.column_names) The problem is that I receive a list of lists just like this: [[‚ÄòC‚Äô, ‚ÄòC‚Äô, ‚ÄòA‚Äô, ‚ÄòG‚Äô, ‚Ä¶, ],‚Ä¶,] where this list is a list where for each example I have a inner-list with length of 91170 chars. I can‚Äôt figure out how should I correctly map this function. Does someone have an idea of which should be the best practice for this?","Sep 29, 2022 5:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Are special_tokens the only tokens guaranteed to be atomic?","https://discuss.huggingface.co/t/are-special-tokens-the-only-tokens-guaranteed-to-be-atomic/4124","0","365","Created: Mar 3, 2021 4:53 pm","jncasey","I‚Äôm building a BPE tokenizer from scratch, and I‚Äôd like to add some tokens to its vocabulary that are never broken apart and never included in merges. The second part of that (no tokens in the merges) is easy ‚Äì the tokens don‚Äôt appear in the training text I give the tokenizer. But for the first part (never breaking my special token, e.g. [Y], into its [,Y, and ] components), I‚Äôm less sure of the correct process. I had though I could just add the tokens to the tokenizer using add_tokens() before training, and when testing using the Tokenizers library version directly, everything seems fine. But when I saved out that tokenizer json and load it as the tokenizer_file param in a Transformers RobertTokenizerFast and used it to train a BART model, I was getting unexpected results ‚Äì the added tokens were sometimes getting broken apart into their components. The problem stopped and tokenization went as expected when I shifted to adding my tokens using add_special_tokens() instead. But that‚Äôs not ideal, because I want my tokens to be part of my output, and not stripped away by the handy skip_special_tokens=True param in the tokenizer‚Äôs decode method. It‚Äôs entirely possible that in my first attempt, I wasn‚Äôt loading the saved tokenizer correctly into RobertaTokenizerFast ‚Äì it‚Äôs a little confusing what can be loaded from the json and what needs to be explicitly passed as a parameter. But after re-reading the docs more carefully, it seems that maybe only special tokens are guaranteed to be atomic? Is there any way to define a token to be atomic but also not in the same class as control tokens like eos and pad?","Mar 3, 2021 4:53 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unable to convert output to interpretable format","https://discuss.huggingface.co/t/unable-to-convert-output-to-interpretable-format/8882","0","363","Created: Jul 31, 2021 7:39 pm","markrogersjr","I‚Äôm not able to convert model outputs to a human-interpretable format. I went ahead and created an issue along with a PR that adds the feature.","Jul 31, 2021 7:39 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unable to upload custom Pytorch model in huggingface","https://discuss.huggingface.co/t/unable-to-upload-custom-pytorch-model-in-huggingface/35545","0","362","Created: Apr 4, 2023 4:46 pm","shivam21mishra08","I created a custom generative pytorch model . but while uploading to hugging face i cannot access hosted API ‚Ä¶ help me fix it!!","Apr 4, 2023 4:46 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Incorporate SARI score into run_summarization.py example script","https://discuss.huggingface.co/t/incorporate-sari-score-into-run-summarization-py-example-script/29511","0","350","Created: Jan 13, 2023 6:02 pm","brooklynsheppard","I am working with the example summarization script provided by Huggingface here. My task is not quite summarization, but a different seq2seq task where I would like to use the SARI metric to evaluate my models. The compute_metrics function in this script takes only the predictions and labels as input, making it easy to calculate ROUGE or BLEU sores, but does not include the source text, which is required for computing the SARI score. The compute_metrics function is then given to the Seq2SeqTrainer as an argument, and I‚Äôm not sure how it is used from there. Does anyone have any guidance on how I can edit this function to be able to take the source text as an argument as well? Thanks!","Jan 13, 2023 6:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Phi model giving extra ids than vocab size of tokenizer so Phi-2 tokenizer.batch_decode() giving error: expected string got NoneType","https://discuss.huggingface.co/t/phi-model-giving-extra-ids-than-vocab-size-of-tokenizer-so-phi-2-tokenizer-batch-decode-giving-error-expected-string-got-nonetype/74581","0","349","Created: Feb 24, 2024 6:18 am","deshwalmahesh","Issue 1: tokenizer.vocab_size has a size of 50257 when printed but the Phi-2 model gives me an output shape (5, 1256, 51200) during evaluation and the error below while decoding I‚Äôm working on callback like: def decode_predictions(tokenizer, predictions):     print(type(predictions), predictions.predictions.shape, predictions.label_ids.shape) # (5, 1256, 51200)     labels = tokenizer.batch_decode(predictions.label_ids)      prediction_text = tokenizer.batch_decode(predictions.predictions.argmax(axis=-1)) # HERE COMES THE ERROR     return {""labels"": labels, ""predictions"": prediction_text}   def on_evaluate(self, args, state, control,  **kwargs):         super().on_evaluate(args, state, control, **kwargs)          predictions = self.trainer.predict(self.sample_dataset)# generate predictions         predictions = decode_predictions(self.tokenizer, predictions) # decode predictions and labels         predictions_df = pd.DataFrame(predictions) # add predictions to a wandb.Table         predictions_df[""epoch""] = state.epoch         records_table = self._wandb.Table(dataframe=predictions_df)         self._wandb.log({""sample_predictions"": records_table}) # log the table to wandb Issue-2: When I create a random example, the tokenizer works till 50300 and after that, I‚Äôm getting: --------------------------------------------------------------------------- TypeError                                 Traceback (most recent call last) Cell In[19], line 1 ----> 1 tokenizer.batch_decode(predictions.argmax(axis=-1), skip_special_tokens = True)  File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3742, in PreTrainedTokenizerBase.batch_decode(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)    3718 def batch_decode(    3719     self,    3720     sequences: Union[List[int], List[List[int]], ""np.ndarray"", ""torch.Tensor"", ""tf.Tensor""],    (...)    3723     **kwargs,    3724 ) -> List[str]:    3725     """"""    3726     Convert a list of lists of token ids into a list of strings by calling decode.    3727     (...)    3740         `List[str]`: The list of decoded sentences.    3741     """""" -> 3742     return [    3743         self.decode(    3744             seq,    3745             skip_special_tokens=skip_special_tokens,    3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,    3747             **kwargs,    3748         )    3749         for seq in sequences    3750     ]  File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3743, in <listcomp>(.0)    3718 def batch_decode(    3719     self,    3720     sequences: Union[List[int], List[List[int]], ""np.ndarray"", ""torch.Tensor"", ""tf.Tensor""],    (...)    3723     **kwargs,    3724 ) -> List[str]:    3725     """"""    3726     Convert a list of lists of token ids into a list of strings by calling decode.    3727     (...)    3740         `List[str]`: The list of decoded sentences.    3741     """"""    3742     return [ -> 3743         self.decode(    3744             seq,    3745             skip_special_tokens=skip_special_tokens,    3746             clean_up_tokenization_spaces=clean_up_tokenization_spaces,    3747             **kwargs,    3748         )    3749         for seq in sequences    3750     ]  File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:358, in CodeGenTokenizer.decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, truncate_before_pattern, **kwargs)     331 """"""     332 Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special     333 tokens and clean up tokenization spaces.    (...)     353     `str`: The decoded sentence.     354 """"""     356 token_ids = to_py_obj(token_ids) --> 358 decoded_text = super()._decode(     359     token_ids=token_ids,     360     skip_special_tokens=skip_special_tokens,     361     clean_up_tokenization_spaces=clean_up_tokenization_spaces,     362     **kwargs,     363 )     365 if truncate_before_pattern is not None and len(truncate_before_pattern) > 0:     366     decoded_text = self.truncate(decoded_text, truncate_before_pattern)  File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/tokenization_utils.py:1024, in PreTrainedTokenizer._decode(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)    1022         current_sub_text.append(token)    1023 if current_sub_text: -> 1024     sub_texts.append(self.convert_tokens_to_string(current_sub_text))    1026 if spaces_between_special_tokens:    1027     text = "" "".join(sub_texts)  File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/codegen/tokenization_codegen.py:284, in CodeGenTokenizer.convert_tokens_to_string(self, tokens)     282 def convert_tokens_to_string(self, tokens):     283     """"""Converts a sequence of tokens (string) in a single string."""""" --> 284     text = """".join(tokens)     285     text = bytearray([self.byte_decoder[c] for c in text]).decode(""utf-8"", errors=self.errors)     286     return text  TypeError: sequence item 31: expected str instance, NoneType found This is the code I‚Äôm using: import numpy as np from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""microsoft/phi-2"", use_fast = False)  print(tokenizer.vocab_size)  tokenizer.add_tokens([""<|im_start|>"", ""<PAD>""]) tokenizer.pad_token = ""<PAD>"" tokenizer.add_special_tokens(dict(eos_token=""<|im_end|>""))  print(tokenizer.vocab_size)  predictions = np.random.uniform(size = (5, 1256, 50300)) # [No of samples, sequence_length, Vocab] preds = predictions.argmax(axis=-1)  tokenizer.batch_decode(preds) # Works till 50300","Feb 24, 2024 6:18 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"`add_tokens` with argument `special_tokens=True` vs `add_special_tokens`","https://discuss.huggingface.co/t/add-tokens-with-argument-special-tokens-true-vs-add-special-tokens/35648","0","345","Created: Apr 5, 2023 12:54 pm","Buxian","There are many tutorial using add_tokens(special_tokens=True), but I read the source code, and find that add_special_tokens will do more thing than add_tokens, Which is prefer?","Apr 5, 2023 12:54 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Is it okay to split ids sequence when it is encoded using Byte-level BPE","https://discuss.huggingface.co/t/is-it-okay-to-split-ids-sequence-when-it-is-encoded-using-byte-level-bpe/8129","0","339","Created: Jul 7, 2021 8:59 am","jisng-prk","TLDR; So, When I use byte level BPE tokenizer, Is it possible to split the encoded sequences into sub list without loss of information. I think when the sublist is decoded into string, It should be generate error at the tail of the sequence because the last ids has only part of the bytes of a string token I‚Äôm using BART BPE , which is same with that of RoBERTa and thus, it is byte-level BPE When using the tokenizer, it split the sequence based on the byte sequence. But, when I should split the sequence into limited sequence length such as 128, maybe, some byte sequence could be split into different documents, thus it should makes error when decode the byte into string because the part of the byte is not provided in the tail of ids sequence For example, when I encode the following sequence ‚ÄúThis string will be encoded as byte level‚Äù => [0, ‚Ä¶ , 2] And I split the ids into two lists, [0, ‚Ä¶ , 2] => [0, ‚Ä¶], [‚Ä¶, 2] the boundary ids between two lists can miss some information about the rest byte of the token string So, When I use byte level BPE, Is it possible to split the encoded sequences into sub list without loss of information?","Jul 7, 2021 8:59 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can‚Äôt get to the source code of `tokenizer.convert_tokens_to_string`","https://discuss.huggingface.co/t/cant-get-to-the-source-code-of-tokenizer-convert-tokens-to-string/32714","0","338","Created: Feb 28, 2023 3:07 pm","Shaike04","I have a list of tokens and I want to understand the logic behind tokenizer.convert_tokens_to_string but I can‚Äôt seem to find the source code for this function or debug my way to it. Can you help?","Feb 28, 2023 3:07 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to use a trained tokenizer for semantic search?","https://discuss.huggingface.co/t/how-to-use-a-trained-tokenizer-for-semantic-search/61091","0","337","Created: Nov 5, 2023 3:17 am","panigrah","I trained a new tokenizer with vocabulary from my domain. I started with sentence-transformers/multi-qa-mpnet-base-dot-v1. I don‚Äôt understand what model to use in order to use the new tokenizer with for semantic search. Do I need to train the model on my vocabulary also before I can use it to create embeddings? I read through the tutorial on FAISS here Semantic search with FAISS - Hugging Face NLP Course. I get reasonably good matches on my dataset using the base tokenizer. But if I swap the base tokenizer with the trained one - I don‚Äôt get matches. So does the model also need to be trained on the new dataset and if so would I be looking to train from scratch or is there a way to just fine tune from this checkpoint? thanks","Nov 5, 2023 3:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BertTokenizer.decode not understanding new vocabulary","https://discuss.huggingface.co/t/berttokenizer-decode-not-understanding-new-vocabulary/64254","0","335","Created: Dec 1, 2023 1:34 pm Latest: Dec 1, 2023 1:38 pm","rberga","I added 3 new items to the BertTokenizer vocabulary (2 emojis and a made-up word), and saved the new vocabulary. Then I instantiated a new BertTokenizer using the new vocabulary file and checked that the tokenizer understood the new words. That worked fine. Then I ran ‚Äòencode‚Äô to see the tokens encodings and verified that the new encodings were used. Then I ran ‚Äòdecode‚Äô on the encoded tokens and did NOT get the original words back. The new items added to the vocabulary were NOT decoded and were left as [UNK] even though the ‚Äòencode‚Äô generated the correct encodings. The code below illustrates the problem: # Importing transformers tokenizer from transformers import BertTokenizer  # Adding 3 new words or symbols to tokenizer vocabulary: thumbs-up and down emojis and a made-up word newvocab = [ 'üëç', 'üëé', 'babalu' ] print(newvocab)  # Get basic Bert Tokenizer from pretrained tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # quick check to make sure new vocabulary is not yet present in existing tokenizer print(tokenizer.tokenize('babalu'))    # ['baba', '##lu'] print(tokenizer.tokenize('üëç'))          # ['[UNK]'] print(tokenizer.tokenize('üëé'))          # ['[UNK]']  # Get base vocabulary (to add newvocab to) bert_vocab = tokenizer.get_vocab()  # add newvocab to bert_vocab print(f'bert_vocab before = {len(bert_vocab)}') for i, k in enumerate(newvocab, len(bert_vocab)):     print(f'new item {k} : {i}')     bert_vocab[k] = i print(f'bert_vocab after = {len(bert_vocab)}')  # The above lines print: # bert_vocab before = 30522 # new item üëç : 30522 # new item üëé : 30523 # new item babalu : 30524 # bert_vocab after = 30525  # save new vocab file with open('/tmp/newvocab.tmp', 'w', encoding = 'utf-8') as tmp_vocab_file:     tmp_vocab_file.write('\n'.join(bert_vocab))  # Get new tokenizer using the new vocabulary file                         new_bert = BertTokenizer.from_pretrained('bert-base-uncased',vocab_file = '/tmp/newvocab.tmp' )  # Does the new tokenizer understand the new items added to the vocabulary? new_bert.tokenize('thumbs-up üëç, thumbs-down üëé,  new word babalu.')  # This produces: # ['thumbs',  '-',  'up',  'üëç',  ',',  'thumbs',  '-',  'down',  'üëé',  ',',  'new',  'word',  'babalu',  '.'] # which shows that the new tokenizer does understand the new items added to the vocabulary  # Checking the encoding and decoding # It seems that the ENCODING is using all the new vocabulary entries (i.e., mapping emojis to their encodings) # But the DECODING is not mapping them back to their original representation.  tokens = new_bert.encode('thumbs-up üëç, thumbs-down üëé,  new word babalu.',                          add_special_tokens=True,                          max_length=32,                          truncation=True                         ) print(f'tokens after encode:\n{tokens}') tokens_decoded = tokenizer.decode(tokens) print(f'tokens after decoding it back:\n{tokens_decoded}')  # this prints: # tokens after encode: # [101, 16784, 1011, 2039, 30522, 1010, 16784, 1011, 2091, 30523, 1010, 2047, 2773, 30524, 1012, 102] # tokens after decoding it back: # [CLS] thumbs - up [UNK], thumbs - down [UNK], new word [UNK]. [SEP]  # It seems that the new items are being mapped to their correct encodings (30522, 30523, 30524), # but are not being decoded back to their original representation. Am I doing anything wrong here? Thanks!","Dec 1, 2023 1:34 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue in loading the saved tokenizer","https://discuss.huggingface.co/t/issue-in-loading-the-saved-tokenizer/67978","1","234","Created: Jan 4, 2024 1:54 pm Latest: Jan 4, 2024 1:54 pm","hadnoor","","Jan 4, 2024 1:54 pm","hadnoor","i was using the following code : from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(‚ÄúSalesforce/xgen-7b-8k-base‚Äù,trust_remote_code=True) tokenizer.pad_token = tokenizer.eos_token tokenizer.save_pretrained(‚Äútoksav‚Äù) loaded_tokenizer = AutoTokenizer.from_pretrained(‚Äútoksav‚Äù)","Jan 4, 2024 1:54 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can‚Äôt load tokenizer for ‚Äòsshleifer/student_blarge_12_3‚Äô","https://discuss.huggingface.co/t/cant-load-tokenizer-for-sshleifer-student-blarge-12-3/6027","0","330","Created: May 6, 2021 11:24 am","jyotsna2893","","May 6, 2021 11:24 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why I‚Äôm getting same result with or without using Wav2Vec2Processor?","https://discuss.huggingface.co/t/why-im-getting-same-result-with-or-without-using-wav2vec2processor/32482","0","320","Created: Feb 25, 2023 3:56 am","laro1","I‚Äôm running simple wav2vec2 code on short without noise voice: #processor = Wav2Vec2Processor.from_pretrained(""facebook/wav2vec2-base-960h"") model     = Wav2Vec2ForCTC.from_pretrained(""facebook/wav2vec2-base-960h"")       FILE_NAME        = ""tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav"" SPEECH_FILE      = download_asset(FILE_NAME)  speech, sr       = librosa.load(SPEECH_FILE, sr=16000)  speech           = torch.tensor(speech) speech           = speech.reshape(1, -1)  logits           = model(speech).logits  predicted_ids    = torch.argmax(logits, dim=-1) transcription    = processor.decode(predicted_ids[0]) transcription results: 'I HAD THAT CURIOSITY BESIDE ME AT THIS MOMENT' As you see, I didn‚Äôt use processor. The examples on the net always used processor So: What is the benefit of using processor ? When do we need to use it ?","Feb 25, 2023 3:56 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Many ambiguous unicode characters for trained tokenizer","https://discuss.huggingface.co/t/many-ambiguous-unicode-characters-for-trained-tokenizer/67553","0","317","Created: Dec 31, 2023 1:06 pm","KellRa","Hey, I trained a tokenizer on english wikipedia texts. Everything works fine, however, looking into the vocabulary of the saved tokenizer, I find a multitude of unicode characters which I would not expect to be included in the corpus, e.g., many chinese symbols. You can take a look at the vocabulary here: finroberta/dicts_and_tokenizers/wikipedia_tokenizer.json at main ¬∑ RalfKellner/finroberta ¬∑ GitHub Odd symbols start at id 68 and go until approximately 10,000 (out of 40,000 tokens). I use unicode normalization when training the tokenizer which I thought would prevent this behavior. More concrete, I use NFD normalization, however, I also tried other forms of normalization. This also happens when using a domain specific corpus from the financial markets area, so I was wondering if I am doing something wrong here? The training script can be found here: finroberta/00_train_wikipedia_tokenizer.py at main ¬∑ RalfKellner/finroberta ¬∑ GitHub I would appreciate any help or clarification! Many thanks in advance! Cheers, Ralf","Dec 31, 2023 1:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5Tokenizer add a whitespace token after added special tokens","https://discuss.huggingface.co/t/t5tokenizer-add-a-whitespace-token-after-added-special-tokens/63101","0","317","Created: Nov 22, 2023 6:44 am","xianf","I am using the mt5 Tokenizer and I want to add ‚Äú\n‚Äù to the origin tokenizer. I add ""additional_special_tokens"": [""\n""] to the tokenizer_config.json. But the output is not what I want. The script is like: tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True) print(tokenizer.tokenize(""‰Ω†Â•Ω\nÂïä"", add_special_tokens=False)) and the output is ['‚ñÅ', '‰Ω†', 'Â•Ω', '\n', '‚ñÅ', 'Âïä'] I want to remove this '‚ñÅ' token after '\n'. Of course, I can do it with some more codes with Python. But can I do this with the tokenizer itself?","Nov 22, 2023 6:44 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer not found","https://discuss.huggingface.co/t/tokenizer-not-found/757","0","314","Created: Aug 18, 2020 8:40 pm","huggingfans","when I try to use patrickvonplaten/longformer-random-tiny model and download the tokenizer, I get this error: OSError: Model name ‚Äòpatrickvonplaten/longformer-random-tiny‚Äô was not found in tokenizers model name list There is the same problem on patrickvonplaten/reformer-tiny-random But the model can be downloaded, and only the tokenizer was not found.","Aug 18, 2020 8:40 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can‚Äôt load tokenizer using from_pretrained, Interface API","https://discuss.huggingface.co/t/cant-load-tokenizer-using-from-pretrained-interface-api/87639","0","310","Created: May 21, 2024 11:06 am","yewo","Can‚Äôt load tokenizer using from_pretrained, please update its configuration: data did not match any variant of untagged enum PyPreTokenizerTypeWrapper at line 2102 column 3 This is my project file link project File hugginceface as a model, it doesn‚Äôt seem to work anymore. Could anyone please help me with that!","May 21, 2024 11:06 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What is required to create a fast tokenizer? For example for a Marian model","https://discuss.huggingface.co/t/what-is-required-to-create-a-fast-tokenizer-for-example-for-a-marian-model/33941","0","310","Created: Mar 16, 2023 12:49 pm","pejrich","I notice that there are no fast tokenizers available for the Opus Marian translation models. The library I‚Äôm using to run my models currently only supports fast tokenizers from HF, but they‚Äôre not available for Marian, is this due to a limitation unique to Marian, or would it be possible for me to create one? What would be involved in the process. I couldn‚Äôt find any information specific to creating a fast tokenizer from an existing slow tokenizer. Thanks!","Mar 16, 2023 12:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Word tokenizers for text generators","https://discuss.huggingface.co/t/word-tokenizers-for-text-generators/23464","0","307","Created: Sep 21, 2022 11:02 pm Latest: Sep 22, 2022 9:49 pm","PeterB123A","Word based text generators make use of tokenizations, they scan a text, build a word-vector table. I wonder how this is done. ea does it learn each verb, walk walking walked walks, etc as different words. Or is there an indexer, stored with a base verb, with only a lookup table for irregular verbs? In essence, To Walk Conjugation - All English Verb Forms does already contains short base sentences, that are part of everyday chat. "" I‚Äôve been walking."" ( maybe just stored as some number- that could become walk 24 walk or Sandra 27 walk. I was thinking could such a scheme optimize (reduce footprint) a smaller amount of total word vectors. by making the tokenizer / de-tokenizer smart (supply it with most language rules, firt word capitalize add . at the end etc)","Sep 21, 2022 11:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to see contents of a normalizer","https://discuss.huggingface.co/t/how-to-see-contents-of-a-normalizer/6045","0","300","Created: May 7, 2021 2:11 am","sumsaw","Hello all, I have just started exploring the tokenizer library and I want to know what know what are the basic normalizer steps which are performed in BPE tokenizer . Is there any way I can list out the contents of tokenizer.normarlizer Regards Sumeet","May 7, 2021 2:11 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Encoding and then decodeing text is not equal","https://discuss.huggingface.co/t/encoding-and-then-decodeing-text-is-not-equal/101851","2","98","Created: Aug 11, 2024 9:08 am Latest: Aug 12, 2024 7:59 am","ron5569","I wonder why in some cases, encdoing the text and then decoding it, is not the same the original text For example very simple code from transformers import AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Meta-Llama-3.1-8B-Instruct"")  texts = ['  A LoyaltyImport is a ...', '  You can ...', '    .withVariants(""true"")'] for row in texts:     encoded = tokenizer(row,                         add_special_tokens=False,                         truncation=True,                         padding=False,                         max_length=2000,                         return_overflowing_tokens=False,                          return_length=False, )     decoded = tokenizer.decode(encoded[""input_ids""])     if decoded != row:         print(""Different"")         print(row)         print(decoded) Then I see the output Different   A LoyaltyImport is a ...   A LoyaltyImport is a... Different   You can ...   You can... Different     .withVariants(""true"")    .withVariants(""true"") This is happen especially when using of dots and spaces. Why does it happen and how can I fix that Thanks!","Aug 11, 2024 9:08 am","mapama247","Just add this line after loading the tokenizer: tokenizer.clean_up_tokenization_spaces = False Llama3 has it to true by default, as you can see on its tokenizer_config.json file.","Aug 12, 2024 7:59 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Initialize Vocabulary for Unigram Tokenizer","https://discuss.huggingface.co/t/initialize-vocabulary-for-unigram-tokenizer/46371","0","296","Created: Jul 11, 2023 6:28 am","dotan1111","I have reviewed the documentation and code, but I was unable to find the specific method used for initializing the set of tokens in the Unigram tokenizer. Typically, it is initialized by running a BPE tokenizer, but I would like to confirm this information. If anyone could provide a link to the code or documentation or offer an answer regarding this matter, it would be greatly appreciated.","Jul 11, 2023 6:28 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer producing token index greater than size of the dictionary","https://discuss.huggingface.co/t/tokenizer-producing-token-index-greater-than-size-of-the-dictionary/39989","0","295","Created: May 15, 2023 7:58 pm","surya-narayanan","I am using a tokenizer with a vocab size of 30522, but the tokenized dataset has a token with id 50,000 and above. Is that possible? What might I be doing wrong?","May 15, 2023 7:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with pushing tokenizer to hub","https://discuss.huggingface.co/t/issue-with-pushing-tokenizer-to-hub/24120","0","294","Created: Oct 7, 2022 8:14 pm","hadiqa123","Can anyone help me with how to resolve this issue?","Oct 7, 2022 8:14 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Exploring the Majestic Temples in Karnataka","https://discuss.huggingface.co/t/exploring-the-majestic-temples-in-karnataka/40952","0","292","Created: May 25, 2023 12:02 pm","justinpaul","Karnataka, a southwestern state in India, is renowned for its rich cultural heritage and magnificent architecture. Among its many treasures, the state boasts an impressive collection of temples that showcase the artistic and spiritual prowess of ancient dynasties. From intricately carved stone structures to grandiose monuments, Karnataka‚Äôs temples offer a mesmerizing glimpse into the region‚Äôs history and devotion. One of the most iconic temples in Karnataka is the Virupaksha Temple in Hampi. Built in the 7th century, this UNESCO World Heritage Site is dedicated to Lord Shiva and stands as a marvel of Dravidian architecture. Its towering gopuram (gateway tower), pillared halls, and beautifully adorned sanctum make it a must-visit for history enthusiasts and spiritual seekers alike.","May 25, 2023 12:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Loading BPE modeled Tokenizer results in empty tokenizer","https://discuss.huggingface.co/t/loading-bpe-modeled-tokenizer-results-in-empty-tokenizer/81849","0","291","Created: Apr 15, 2024 12:18 pm","timdadum","I am creating a GPT-like model from scratch to improve my understanding of tokenization, encoding and the architecture in itself. As such, I tried to create my own, fairly na√Øve BPE encoding using for-loops, but as you could guess, it was terribly slow for large vocabularies with many merges. I decided to use the Huggingface Tokenizer class from the tokenizers library with the following function to create a tokenizer: def create_tokenizer(corpus_path, tokenizer_path, vocab_size):     # Initialize a tokenizer with BPE model     tokenizer = Tokenizer(BPE(unk_token='<UNK>'))     tokenizer.pre_tokenizer = Whitespace()      # Initialize the trainer     trainer = BpeTrainer(vocab_size=vocab_size, special_tokens=['<UNK>'])      # List of files to train on     files = [corpus_path]      # Train the tokenizer     tokenizer.train(files, trainer)      # Saving the tokenizer     tokenizer.save(tokenizer_path)     print(f""Succesfully saved tokenizer at {tokenizer_path}"")     return tokenizer Which actually works well to create a tokenizer. The resulting .json file also looks good: {   ""version"": ""1.0"",   ""truncation"": null,   ""padding"": null,   ""added_tokens"": [     {       ""id"": 0, (......)     ""vocab"": {       ""<UNK>"": 0,       ""'"": 1,       "","": 2,       ""-"": 3,       ""."": 4,       ""_"": 5,       ""a"": 6,       ""b"": 7,       ""c"": 8,       ""d"": 9,       ""e"": 10,       ""f"": 11,       ""g"": 12,       ""h"": 13,       ""i"": 14, Then, when I try to load it using the following method: def load_model(model_class, config):     model = model_class(**config['Hyperparameters'])     tokenizer = Tokenizer(BPE(unk_token='<UNK>'))     tokenizer.from_file(config['Files']['tokenizer'])          # DEBUG     print(""Loaded tokenizer from file:"", config['Files']['tokenizer'])     vocab = tokenizer.get_vocab()     print(""Vocabulary size:"", len(vocab))     print(""Sample entries from vocabulary:"", dict(list(vocab.items())[:10]))     # GUBED      model.set_tokenizer(tokenizer, config)     model.load_state_dict(torch.load(config['Files']['model'], map_location=config['Hyperparameters']['device']))     model.eval()     return model  trained_model = load_model(gpt.GPT, config) Loaded tokenizer from file: tiny-llm/tokenizers/nature.json Vocabulary size: 0 Sample entries from vocabulary: {} Tokenizer succesfully set Yields an empty vocabulary, as-if the tokenizer is not instantiated correctly. Yet, I do not receive any errors. I checked The Huggingface quicktour for tokenizers, but to no avail. I can‚Äôt spot any issues. Does anyone know what may cause my vocabulary to be empty?","Apr 15, 2024 12:18 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Authorization header is correct, but the token seems invalid","https://discuss.huggingface.co/t/authorization-header-is-correct-but-the-token-seems-invalid/111177","3","84","Created: Oct 10, 2024 12:02 pm Latest: Oct 10, 2024 2:02 pm","DiegoHexagon","Hello, I read more solutions for this problem, but no one is valid for me. I use huggingface with nodejs. Also with a simple code I have the error. const { HfInference } = require(‚Äò@huggingface/inference‚Äô); const hf = new HfInference({ token: ‚Äòhf_‚Ä¶‚Äô }); // await hf.featureExtraction({ model: ‚Äúsentence-transformers/distilbert-base-nli-mean-tokens‚Äù, inputs: ‚ÄúThat is a happy person‚Äù, }); I checked Write access to contents/settings of all repos under your personal namespace Make calls to the serverless Inference API in ther option of the AccessToken Maybe someone can help me.","Oct 10, 2024 12:02 pm","John6666","I‚Äôm not familiar with JavaScript, but let‚Äôs follow the sample anyway. Sometimes a model is bad, so you can try a different one. huggingface.co Hugging Face JS libraries We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science. huggingface.co Accessing Private/Gated Models We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.","Oct 10, 2024 12:38 pm","DiegoHexagon","Thanks for reply. I have to use the first model for my solution","Oct 10, 2024 1:25 pm","DiegoHexagon","Solved. The problem was that I used ""const hf "", and this is noa accepted. I changed in ‚Äúconst inference‚Äù and now it works.","Oct 10, 2024 2:02 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What file type should my training data be?","https://discuss.huggingface.co/t/what-file-type-should-my-training-data-be/32075","0","289","Created: Feb 20, 2023 11:07 am","TheaMT","Hi, I want to fine tune the opus nt ar-en model using my own dataset, but I‚Äôm not sure what type of files my training data should be in? In the huggingface Marian tutorial (MarianMT) they just pass in lists of sentences, but I also read somewhere that I‚Äôm supposed to preprocess the data with Sentencepiece first, since the Marian tokenizer takes spm files. Or is sentencepiece ‚Äúbuilt in‚Äù into the Marian tokenizer? As for now, my data is a csv file. I‚Äôm a beginner, so all help is much appreciated.","Feb 20, 2023 11:07 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to instantiate a XLMRobertaTokenizer object using a locally trained SentencePiece tokenizer","https://discuss.huggingface.co/t/how-to-instantiate-a-xlmrobertatokenizer-object-using-a-locally-trained-sentencepiece-tokenizer/39843","0","287","Created: May 14, 2023 11:49 pm","mehran","I have trained a SentencePiece tokenizer locally and I would like to instantiate an XLMRobertaTokenizer object based on it. Can someone please help me understand how this is done? What I have at the moment are two files: my_sp.model and my_sp.vocab","May 14, 2023 11:49 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Easy way to compare tokenizers","https://discuss.huggingface.co/t/easy-way-to-compare-tokenizers/38347","0","285","Created: May 1, 2023 10:08 pm","surya-narayanan","Is there any easy way to compare tokenizers? I want to see if tokenizer x is compatible with model y","May 1, 2023 10:08 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Pretokenise on punctuation except hyphens","https://discuss.huggingface.co/t/pretokenise-on-punctuation-except-hyphens/36691","0","286","Created: Apr 15, 2023 2:46 pm","Bauwens","I need to preprocess some sentences so that an existing module can split them into words based on single spaces. Currently, I have: import tokenizers.normalizers as tn import tokenizers.pre_tokenizers as tp  normalizer   = tn.Sequence([tn.NFD(), tn.StripAccents()]) pretokeniser = tp.Whitespace()  # Combines WhitespaceSplit and Punctuation  def preprocess(line: str):     pretokens = pretokeniser.pre_tokenize_str(normalizer.normalize_str(line))     return "" "".join([w for w,_ in pretokens]) The problem is that my language (Dutch) has hyphenation within words for some compounds, and these are split into separate words by this process. As an example: Energie-effici√´ntie, i.e. zuinig omgaan met stroomverbruik, wordt steeds belangrijker bij het trainen van transformer-architecturen ‚Äì zoveel is zeker! now becomes Energie - efficientie , i . e . zuinig omgaan met stroomverbruik , wordt steeds belangrijker bij het trainen van transformer - architecturen ‚Äì zoveel is zeker ! Is there a way to exclude hyphens as punctuation mark?","Apr 15, 2023 2:46 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Why is BertNormalizer not exposed on the tokenizers library?","https://discuss.huggingface.co/t/why-is-bertnormalizer-not-exposed-on-the-tokenizers-library/23340","0","279","Created: Sep 19, 2022 12:03 pm","ndamulelonemakh","Hi all I am trying to use the BertNormalizer() to train a ByteLevelBPETokenizer(). But it seems this normaliser is not exposed looking at the source(see attached image). I am using tokenizers==0.12.1 Was this intentional or is it a feature that will be added going forward?","Sep 19, 2022 12:03 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Custom training - tokenization via collate fn or __getitem__?","https://discuss.huggingface.co/t/custom-training-tokenization-via-collate-fn-or-getitem/81711","0","278","Created: Apr 14, 2024 6:55 pm","malba96","hello, I am finetuning clip with 1.1TB of image and text pairs using pytorch lighting fabric and mosaic-ml streaming datasets to load the data from multiple shards. When loading I have the option to apply the tokenizer and processor for text and images respectively in __getitem__ or using them via colllate_fn to do it batch-wise staking them afterwards. My question is, what is more recommended, apply transformations/tokenizers in collate_fn or __getitem__? I have seen very few examples online of people applying tokenizers/processors via collate_fn (theoretically it should be faster than in __getitem__) I tried to look the ViT example and it seems the transformations are done on the fly and then collate fn to stack, should I follow the same to gain training speed and reduce memory footprint? thank you in advance best,","Apr 14, 2024 6:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Use sentence-transformers/all-MiniLM-L6-v2 fully local","https://discuss.huggingface.co/t/use-sentence-transformers-all-minilm-l6-v2-fully-local/90685","1","196","Created: Jun 6, 2024 1:37 pm Latest: Jun 6, 2024 4:24 pm","Riadrfm","Hello, i want to use the sentence-transformers/all-MiniLM-L6-v2 totaly localy, need your support Br.","Jun 6, 2024 1:37 pm","MattiLinnanvuori","https://www.sbert.net SentenceTransformers Documentation has local use instructions.","Jun 6, 2024 4:24 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Getting Wholeword corresponding to a subword in a text?","https://discuss.huggingface.co/t/getting-wholeword-corresponding-to-a-subword-in-a-text/24152","0","275","Created: Oct 8, 2022 8:32 pm","cramraj8","I am using AutoTokenizer to tokenize a text, and then for a certain subword mention index, I am trying to find which whole word it belongs to. I was using word_ids() to find the whole word index and white-space split the text to find the corresponding wholeword. However, if the text has puncutations, the whole word count after white-space splitting is getting mismatched. How can I handle it ? whole_word_ids = encoded_passage.word_ids(0) max_wholeword_id = np.nanmax(whole_word_ids[1:-1])             whitespace_tokens = passage_text.split(' ') assert max_wholeword_id+1 == len(whitespace_tokens), ""Mismatched length""","Oct 8, 2022 8:32 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Help defining tokenizer","https://discuss.huggingface.co/t/help-defining-tokenizer/38091","0","275","Created: Apr 28, 2023 11:33 pm","surya-narayanan","Im just curious- i want to train multiple models on the same dataloader, kinda similar to vision- is there any way to train a new tokenizer that‚Äôs not specific to a model, such that I can run the following workflow? dataset = load_dataset('wiki') model1 = AutoModelForMaskedLM.from_pretrained(""model1name"") model2 = AutoModelForMaskedLM.from_pretrained(""model2name"")  ### help me with code to tokenize the dataset here tokenizer = (...) # I would have done a from_pretained here, but am not sure what to do, since model1 and model2 might have different tokenizers  def tokenize_function(examples):     return tokenizer.encode(examples[""text""], padding=""max_length"", truncation=True)  dataset = dataset.map(tokenize_function, batched=True)  dataloader = Dataloader(dataset) ####  for x in dataloader:    y1 = model1(x)    y2 = model2(x)","Apr 28, 2023 11:33 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Train tokenizer for seq2seq model","https://discuss.huggingface.co/t/train-tokenizer-for-seq2seq-model/82524","0","267","Created: Apr 19, 2024 1:14 am","Nevermetyou","Hi all, I am trying to train the seq2seq model using t5v1.1 My task is simple; I want to map text from format A to format B for example, 19 July 2020 ‚Üí 19/07/2020. This is not my real data, just an example of what I am doing. I want to train the tokenizer for this data, but for the seq2seq model, the tokenizer needs to tokenize both input data and label, right? So, I am a bit confused about arranging the data to train the tokenizer. Should I concatenate input data and label together and then pass it old_tokenizer.train_new_from_iterator I have read the doc Training a new tokenizer from an old one - Hugging Face NLP Course But I am still confused about seq2seq setting Thanks","Apr 19, 2024 1:14 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Customize FlauBERT tokenizer to split line breaks","https://discuss.huggingface.co/t/customize-flaubert-tokenizer-to-split-line-breaks/33011","0","265","Created: Mar 4, 2023 10:45 am","rapminerz","Hello, I want to train FlauBERT model on french music lyrics and I want to adapt the tokenizer to my usecase : for example I‚Äôve seen the tokenizer is actually ignoring line breaks How can I make it tokenize them ? I‚Äôve also seen FlauBERT Tokenizer is a ‚Äúslow‚Äù tokenizer so it cannot be trained with the .train_from_iterator() method Should I preprocess data myself ? Can I train another tokenizer ? I‚Äôm pretty blocked Thanks !","Mar 4, 2023 10:45 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Get Problem with Doubled tokens in NLLB Tokenizer After load new vocab!","https://discuss.huggingface.co/t/get-problem-with-doubled-tokens-in-nllb-tokenizer-after-load-new-vocab/62940","0","263","Created: Nov 21, 2023 2:36 am Latest: Nov 21, 2023 6:55 am","ulinnuha","I load NLLB Tokenizer with new vocab after adding some vocabs using sentencepiece. tokenizer = NllbTokenizer.from_pretrained('facebook/nllb-200-distilled-600M', vocab_file=NEW_SPM_NAME) However, I have a problem with several doubled tokens in the lang_code. print(tokenizer.convert_ids_to_tokens([256001])) ['ace_Arab'] print(tokenizer.convert_ids_to_tokens([270130])) ['ace_Arab'] in addition, the length of tokenizer and vocab size are different print(len(tokenizer)), print(tokenizer.vocab_size) 270130 270333 It looks like that tokenizer.added_tokens_encoder is still in the old config before loading the new vocab print(tokenizer.added_tokens_encoder) {'<s>': 0,  '<pad>': 1,  '</s>': 2,  '<unk>': 3,  'ace_Arab': 256001,  'ace_Latn': 256002,  'acm_Arab': 256003,  'acq_Arab': 256004,  'aeb_Arab': 256005,  'afr_Latn': 256006,  'ajp_Arab': 256007,  'aka_Latn': 256008,  'amh_Ethi': 256009,  'apc_Arab': 256010,  'arb_Arab': 256011,  'ars_Arab': 256012,  'ary_Arab': 256013,  'arz_Arab': 256014,  'asm_Beng': 256015, ....  print(tokenizer.fairseq_tokens_to_ids) {'<s>': 0,  '<pad>': 1,  '</s>': 2,  '<unk>': 3,  '<mask>': 270332,  'ace_Arab': 270130,  'ace_Latn': 270131,  'acm_Arab': 270132,  'acq_Arab': 270133,  'aeb_Arab': 270134,  'afr_Latn': 270135,  'ajp_Arab': 270136,  'aka_Latn': 270137,  'amh_Ethi': 270138,  'apc_Arab': 270139,  'arb_Arab': 270140,  'ars_Arab': 270141, How to solve it?","Nov 21, 2023 2:36 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"`additional_special_tokens` are not added","https://discuss.huggingface.co/t/additional-special-tokens-are-not-added/93192","1","186","Created: Jun 20, 2024 12:55 pm Latest: Jun 20, 2024 12:58 pm","Yao-Lirong","Hi Hugging Face Community, I have the following questions regarding special tokens: Why doesn‚Äôt tokenizer.all_special_tokens include <image> token? I‚Äôm using the LLaVA model that has <image> as a special token (as defined in added_tokens_decoder of tokenizer_config.json). The tokenizer encodes and decodes it indeed as a special token. However, when I load in its tokenizer and call tokenizer.all_special_tokens or tokenizer.additional_special_tokens, <image> token is not included. Where is <image> token loaded? I looked into the tokenizer.from_pretrained function but there doesn‚Äôt seem to be a place to actually read in the added_tokens_decoder where in config file this special token is defined ? Where is tokenizer.decode function defined as a special token? I tried to break-point into it to find how it skipped <image> as a special token but I seem to get into a loop call between tokenization_utils_base.py and tokenization_utils_fast.py It would be really helpful if you give an answer to any of these questions. Thank you very much!","Jun 20, 2024 12:55 pm","Yao-Lirong","I‚Äôm not allowed to paste more than 2 links in a post so I will provide the codes related to question 3 where I got into a loop. tokenization_utils_base.py and tokenization_utils_fast.py","Jun 20, 2024 12:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unmasking adds an extra whitespace for BPE tokenizer","https://discuss.huggingface.co/t/unmasking-adds-an-extra-whitespace-for-bpe-tokenizer/69100","0","258","Created: Jan 14, 2024 5:03 pm Latest: Jan 14, 2024 6:13 pm","Eghbal","I created a custom BPE tokenizer for pre-training a Roberta model, utilizing the following parameters (I tried to align it with the default parameters of BPE for RoBERTa.): from tokenizers.models import BPE from tokenizers import ByteLevelBPETokenizer from tokenizers.processors import RobertaProcessing      tokenizer = ByteLevelBPETokenizer() tokenizer.normalizer = normalizers.BertNormalizer(lowercase = False) tokenizer.train_from_iterator(Data_full, vocab_size = 50264, min_frequency = 2, special_tokens = [""<s>"", ""<pad>"", ""</s>"", ""<unk>""]) tokenizer.add_special_tokens([""<mask>""]) tokenizer.post_processor = RobertaProcessing(sep = (""</s>"", 2), cls = (""<s>"", 0), trim_offsets = False, add_prefix_space = False) tokenizer.enable_padding(direction = 'right', pad_id = 1, pad_type_id = 1, pad_token = ""<pad>"", length = 512) When pre-training a Roberta model with this tokenizer, I observe unusual behavior during the unmasking process: from tokenizers import Tokenizer from transformers import pipeline from transformers import RobertaTokenizerFast tokenizer_in = Tokenizer.from_file('tokenizer_file') tokenizer_m = RobertaTokenizerFast(tokenizer_object=tokenizer_in, clean_up_tokenization_spaces=True)  unmasker = pipeline('fill-mask', model=model_m, tokenizer = tokenizer_m) unmasker(""Capital of France is <mask>."") The output consistently appears as follows: Capital of France is(two whitespaces)Paris. I‚Äôm curious about the persistent extra space before ‚ÄòParis‚Äô. I believe activating the clean_up_tokenization_spaces option might resolve this. Could there be an error in my code leading to this issue? This happens for all unmasking tasks. Also, when I conduct a test with a command like unmasker(""Capital of France is<mask>.""), the quality improves and the issue seems to be resolved.","Jan 14, 2024 5:04 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unable to load saved tokenizer","https://discuss.huggingface.co/t/unable-to-load-saved-tokenizer/86631","0","244","Created: May 14, 2024 9:05 am","willsaliba","I‚Äôm able to successfully train and save my tokenizer but then i cant reload it. tokenizer.save(tokenizer_save_path+""tokenizer.json"") #works newTokenizer = Tokenizer.from_file(tokenizer_save_path+""tokenizer.json"") #breaks I always get this error: Exception: data did not match any variant of untagged enum ModelWrapper at line 3258 column 3","May 14, 2024 9:05 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Leaving unknown words untokenized like in OpenMNT","https://discuss.huggingface.co/t/leaving-unknown-words-untokenized-like-in-openmnt/58969","0","243","Created: Oct 18, 2023 9:32 am Latest: Oct 18, 2023 9:56 am","KhaiKit","Hi Is there a way to collate the OOV tokens for future finetuning? I wanted to leave unknown words untokenized instead of being replaced by <unk> but couldn‚Äôt figure out how to. For example the text ""Hi there hello word"" is sent to the tokenizer and outputs [Hi, <unk>, hello, word] But I want the tokenizer to output  [Hi, there, hello, word] even if the word ‚Äúthere‚Äù is OOV. Seems like OpenNMT (https://forum.opennmt.net/t/leave-unknown-words-untranslated/2790) has it implemented and I was wondering if HF has it as I would want to stick to the HF framework.","Oct 18, 2023 9:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RobertaTokenizer decode and tokenize do not have the same output","https://discuss.huggingface.co/t/robertatokenizer-decode-and-tokenize-do-not-have-the-same-output/59709","0","239","Created: Oct 24, 2023 2:43 pm","BettyFbr","I use a RobertaTokenizer to tokenize sentences that contains french characters like √© or √ß. I need the generated tokens with the ƒ† character and the french characters well formatted. For instance with the input input = ""3 all√©es paris, 75000"" [tokenizer.decode([token]) for token in tokenizer.encode(input)] outputs ['<s>', ' 3', ' all', '√©es', ' paris', ',', ' 7', '5000', '</s>'] so the ƒ† are replaced by spaces. And tokenizer.tokenize(input) outputs ['ƒ†3', 'ƒ†all', '√É¬©es', 'ƒ†paris', ',', 'ƒ†7', '5000'] so the french characters are not well formatted. I used to do this, and it used to work: inputs = self.tokenizer.encode_plus(input, return_tensors=""pt"") ids = inputs['input_ids'].cpu().tolist() clean_tokens = [self.tokenizer.decode([token]) for token in ids[0]] But for some reasons I cannot understand, it does not output the tokens with the ƒ† characters anymore and I cannot figure out what was the breaking change. Do you have any idea ?","Oct 24, 2023 2:43 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Custom Tokenizing?","https://discuss.huggingface.co/t/custom-tokenizing/78027","0","237","Created: Mar 19, 2024 4:15 am","thedarkknight7","I have already tokenized my dataset in the desired format for the problem, so I don‚Äôt want to tokenize it again. I‚Äôm working with nucleotides so I want single, paired and triple sequences. However, I do want to pass this into a BERT model and would like to preprocess the data. I‚Äôve looked at this link here: Preprocess (particularly the video). I currently have my tokens stored as a list. Is there anyway I can use that for the remaining steps mentioned in the video (i.e. converting to ids and preparing for model)? Thanks!","Mar 19, 2024 4:15 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Incorporating my tokenizer into huggingface","https://discuss.huggingface.co/t/incorporating-my-tokenizer-into-huggingface/73331","0","223","Created: Feb 15, 2024 2:46 pm","yuvalpinter","Hi there, About a year ago my lab released SaGe, a tokenizer that incorporates contextual signals from corpora and thus learns tokens which are more aligned with LM objectives. The paper is here: ACL Anthology Incorporating Context into Subword Vocabularies Shaked Yehezkel, Yuval Pinter. Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. 2023. Recently, we released a version that‚Äôs much faster than the original, better streamlining the corpus for training the vocab. The (python) implementation is here: GitHub GitHub - MeLeLBGU/SaGe: Code for SaGe subword tokenizer (EACL 2023) Code for SaGe subword tokenizer (EACL 2023). Contribute to MeLeLBGU/SaGe development by creating an account on GitHub. We were wondering if and how we can get support for porting SaGe into hf tokenizers and making it a first-class member of the codebase? Would any of your engineers be able to help? What would you need from us? Thanks, Yuval uvp@cs.bgu.ac.il","Feb 15, 2024 2:46 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Train Retry Tokenizer","https://discuss.huggingface.co/t/train-retry-tokenizer/37031","0","222","Created: Apr 18, 2023 10:28 pm","Q-bert","Hello, can I enlarge the tokenizer that I trained on a corpus before, by training it on another corpus? Thanks","Apr 18, 2023 10:28 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Creating tokenizer from counts file?","https://discuss.huggingface.co/t/creating-tokenizer-from-counts-file/31399","0","216","Created: Feb 9, 2023 7:37 pm","sabharadwaj","I would like to train a wordpiece tokenizer from scratch from a counts file with tokens and counts.","Feb 9, 2023 7:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"[NER][Japanese] labeled segment shorter than token","https://discuss.huggingface.co/t/ner-japanese-labeled-segment-shorter-than-token/63330","0","214","Created: Nov 23, 2023 7:38 pm Latest: Nov 24, 2023 12:52 am","ewfian","When dealing with Japanese NER tasks, I have a problem. the labeled segment ‚ÄúÊó•Êú¨‚Äù[29,31] is shorter than the token ‚ÄòÊó•Êú¨„ÅÆ‚Äô, how to deal with it? here are more cases: token labeled segment WRN END Áî∫„ÅÆ Â§ßÊâãÁî∫ WRN START „ÅÆ‰∏ñÁïå ‰∏ñÁïå„ÉÅ„Çß„ÇπÈÅ∏ÊâãÊ®© WRN END „É´„ÅÆ „Ç¢„ÉÉ„Éó„É´ WRN END ÈßÖ„Åã„Çâ „Ç¨„Ç§„É´„Éâ„É´„ÉïË•øÈßÖ WRN END Êó•Êú¨„ÅÆ Êó•Êú¨ WRN END „ÅÑ„Åß È≥•ÁæΩ„Éª‰ºèË¶ã„ÅÆÊà¶„ÅÑ WRN END ÂõΩ„ÅÆ „Ç¢„É°„É™„Ç´ÂêàË°ÜÂõΩ WRN END !„Äç „ÇΩ„Éº„Éª„Éï„Ç°„Éº„ÄÅ„ÇΩ„Éº„Éª„Ç∞„ÉÉ„Éâ‚Ä¶„ÇΩ„Éº„Éª„Éõ„ÉØ„ÉÉ„Éà! WRN END Á±≥ÂõΩ Á±≥ WRN END „Çπ„ÅÆ „Çπ„Éé„Éº„Éó„Çπ WRN END ÂÜÖ„Åß Áü•ÂÜÖ WRN END Â∏Ç„ÅÆ Êú≠ÂπåÂ∏Ç WRN END „Çπ„ÅÆ „Çº„Ç¶„Çπ WRN END Â∏Ç„ÅÆ Á¶èÂ≥∂Áúå„ÅÑ„Çè„ÅçÂ∏Ç xlmr_tokenizer = AutoTokenizer.from_pretrained(""xlm-roberta-base"") text= ""Ê†™Âºè‰ºöÁ§æ„Ç∏„Çß„Ç§„Ç¢„Éº„É´Êù±Êó•Êú¨‰ºÅÁîª„ÅØ„ÄÅÊù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫„Å´Êú¨Á§æ„ÇíÁΩÆ„ÅèÊó•Êú¨„ÅÆÁ∑èÂêàÂ∫ÉÂëä‰ª£ÁêÜÂ∫ó„ÄÇ"" embed = xlmr_tokenizer(text) print(xlmr_tokenizer.convert_ids_to_tokens(embed.input_ids))  ## ['<s>', '‚ñÅ', 'Ê†™Âºè‰ºöÁ§æ', '„Ç∏„Çß', '„Ç§', '„Ç¢', '„Éº„É´', 'Êù±', 'Êó•Êú¨', '‰ºÅÁîª', '„ÅØ', '„ÄÅ', 'Êù±‰∫¨ÈÉΩ', 'Ê∏ãË∞∑', 'Âå∫', '„Å´', 'Êú¨', 'Á§æ', '„Çí', 'ÁΩÆ', '„Åè', 'Êó•Êú¨„ÅÆ', 'Á∑èÂêà', 'Â∫ÉÂëä', '‰ª£ÁêÜ', 'Â∫ó', '„ÄÇ', '</s>'] ""entities"": [             {                 ""name"": ""Ê†™Âºè‰ºöÁ§æ„Ç∏„Çß„Ç§„Ç¢„Éº„É´Êù±Êó•Êú¨‰ºÅÁîª"",                 ""span"": [                     0,                     15                 ],                 ""type"": ""Ê≥ï‰∫∫Âêç""             },             {                 ""name"": ""Êù±‰∫¨ÈÉΩÊ∏ãË∞∑Âå∫"",                 ""span"": [                     17,                     23                 ],                 ""type"": ""Âú∞Âêç""             },             {                 ""name"": ""Êó•Êú¨"",                 ""span"": [                     29,                     31                 ],                 ""type"": ""Âú∞Âêç""             }         ]","Nov 23, 2023 7:38 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Caching tokenization","https://discuss.huggingface.co/t/caching-tokenization/69072","0","213","Created: Jan 14, 2024 12:16 pm","Kason123","Hi I tokenize my data as follows but every time I try to run it, the code does the mapping scratch although there is a cached one in the respective folder. Can anyone help to avoid this redundant process? tokenizer=AutoTokenizer.from_pretrained(script_args.model_name, cache_dir=""hf_cache_dir"", local_files_only=True) def tokenize_function(example):     return tokenizer(example[""text""], truncation=True)   tokenized_datasets = dataset.map(tokenize_function, batched=True, load_from_cache_file=True)","Jan 14, 2024 12:16 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Questions re: Tokenizer pipeline composability / reuse outside of the HF ecosystem","https://discuss.huggingface.co/t/questions-re-tokenizer-pipeline-composability-reuse-outside-of-the-hf-ecosystem/66207","0","209","Created: Dec 18, 2023 1:30 pm","namnnumbr","I‚Äôd like to use portions of the tokenizer pipeline (Normalizer, Pre-tokenizer) separately for some initial preprocessing/cleaning, do some external functions for additional preprocessing, then hand back to (a new?) tokenizer pipeline for normalizer pre-tokenizer ‚Äì custom (non-tokenizer pipeline) functions ‚Äì tokenizer.normalizer tokenizer.pre-tokenizer tokenizer.tokenize Is there a way to create a Tokenizer pipeline object that doesn‚Äôt tokenize? Or should I just do something like nzr = normalizers.Sequence(...) ptok = pre_tokenizer(...)  def custom_fn(text: str):     # custom preprocessing     ...     return txt  cleaned = custom_fn(     ptok.pre_tokenize_str(        nzr.normalize_str(text)     ) ) Further, if I hope to apply these to a Huggingface Dataset, should I just map the function to the dataset? my_ds = load_dataset(...) nzr = normalizers.Sequence(...) ptok = pre_tokenizer(...)  my_ds = my_ds.map(nzr.normalize_str) my_ds = my_ds.map(ptok.pre_tokenize_str)","Dec 18, 2023 1:30 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Doubts about the tokenization strategy and the explanation of models through SHAP","https://discuss.huggingface.co/t/doubts-about-the-tokenization-strategy-and-the-explanation-of-models-through-shap/87803","0","204","Created: May 22, 2024 8:04 am Latest: May 23, 2024 6:20 am","MattBlue92","Hi there! I have a doubt about the tokenizers of the transformers models, and to simplify I will restrict the case to BERT. BERT uses the wordpiece tokenizer therefore its features are word pieces and some whole words, the advantages of this approach are: a small dictionary, robustness to new words and computational efficiency. I‚Äôm doing a project in text-classification and an important module of this project is about XAI, I was required to use SHAP to explain BERT‚Äôs classification. We discovered without much surprise that SHAP highlights word pieces to explain BERT‚Äôs classification, but for my boss this isn‚Äôt good for him. He wanted that SHAP highlights words and not piece of words but with wordpiece tokenizer it‚Äôs impossible because that algorithm works for split words! Now he has asked me to train a new tokenizer that processes word-level features for BERT, but I am not very convinced about this strategy because hugginface does not provide a word-level tokenizer to make from scracht; it would result in a huge dictionary that would have to be built from the domain documents, which are very few (about a hundred documents), plus a huge generic data corpus like the one used by BERT and roBERTa to try to have as many words as possible and increase the resilience to new words; it is a computationally expensive operation in terms of time, effort and resources, because BERT would then have to be retrained because its knowledge was based on the features extracted from the word piece tokeniser, and to get things right it would then have to redo a hyperparametisation, which would be very challenging as I do everything. I don‚Äôt found papers (IEEE, elsever journals) or on web no one train word-level tokenizer to make from scracht for BERT and co. I am convinced that the best way forward is to try a post-processing strategy where it is possible to reconstruct the information by retrieving the word embeddings of the split words and obtain the word embeddings of the reconstructed word and then understand how this can be used by SHAP. Did anyone train from scratch a word-level tokenizer? Did anyone use a post processing tecnique to retrive the correct embedding and rappresentation of a split word for SHAP?","May 22, 2024 8:04 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Does AutoTokenizer uploads data to HuggingFace","https://discuss.huggingface.co/t/does-autotokenizer-uploads-data-to-huggingface/59829","0","198","Created: Oct 25, 2023 8:51 am","javigallego4","I want to use AutoTokenizer in a project, but I‚Äôm a bit concerned about whether it‚Äôll upload to HuggingFace any kind of data from my texts. Can anyone confirm if it does so ?","Oct 25, 2023 8:51 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FastTokenizer add 10 more tokens in Avg","https://discuss.huggingface.co/t/fasttokenizer-add-10-more-tokens-in-avg/69854","0","196","Created: Jan 20, 2024 5:48 am","Hennara","Hi, I want to ask a question about the LlamaTokenizerFast, or TokenizerFast in general. I am working on a project to create LLM in support of the Arabic language, we‚Äôve decided to extend the LlamaTokenizer with Arabic tokens following the Chinese-LLama approach. We add the tokens and get a new tokenizer. When converting it to TokenizerFast just by reading by ‚ÄúAutoTokenizer‚Äù "" the Tokenizer fast generates on average 8 more extra tokens for Arabic text. Can any please explain what is going on?","Jan 20, 2024 5:48 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Problem with AutoTokenizer","https://discuss.huggingface.co/t/problem-with-autotokenizer/93626","1","139","Created: Jun 23, 2024 3:03 pm Latest: Jun 24, 2024 7:37 am","agnavale","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  model_path = ""./raphael"" tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForSeq2SeqLM.from_pretrained(model_path)  # Example prompts prompts = [     ""What is your name?"",     ""Who are you?"",     ""Do you know Raphael"", ]  # Tokenize and generate responses for prompt in prompts:     inputs = tokenizer(prompt, return_tensors=""pt"")     input_ids = inputs[""input_ids""]     outputs = model.generate(input_ids, max_length=100)     response = tokenizer.decode(outputs[0], skip_special_tokens=True)     print(f""Prompt: {prompt}"")     print(f""Response: {response}\n"") This my code when using AutoTokenizer. and It gives error. Exception: Error while initializing BPE: Token _</w> out of vocabulary but same program works fine when I use BlenderbotSmallTokenizer in place of AutoTokenizer. from transformers import BlenderbotSmallForConditionalGeneration, BlenderbotSmallTokenizer  model_path = ""./raphael"" model = BlenderbotSmallForConditionalGeneration.from_pretrained(model_path) tokenizer = BlenderbotSmallTokenizer.from_pretrained(model_path) What is exactly the problem?","Jun 23, 2024 3:03 pm","agnavale","Is it even possible to write errorless code for this library. I‚Äôm always getting some or the other errors. tired of all this shit.","Jun 24, 2024 7:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T5 tokenizer vs t51.1 tokenizer","https://discuss.huggingface.co/t/t5-tokenizer-vs-t51-1-tokenizer/75421","0","195","Created: Mar 1, 2024 2:35 am","Nevermetyou","Hello, there I wonder if the tokenizer of t51.1 is the same as that of normal t5. Or is the pre-tokenization step not the same?","Mar 1, 2024 2:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to deal SQL query in tabular dataset?","https://discuss.huggingface.co/t/how-to-deal-sql-query-in-tabular-dataset/73570","0","192","Created: Feb 17, 2024 12:11 pm","Iamexperimenting","Hi, I have a dataset which contains numerical columns, categorical columns and SQL Query as a separate column. Can someone recommend me how to deal with it? I would like to get suggestion on how to deal with SQL Query column? what sort of feature engineering should I apply for SQL Query column? Is there any specific model to deal with this? Datasets Beginners Models Intermediate Tokenizers Research Community Calls","Feb 17, 2024 12:11 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Modifying normalizer for pretrained tokenizers don‚Äôt consistently work","https://discuss.huggingface.co/t/modifying-normalizer-for-pretrained-tokenizers-dont-consistently-work/91729","2","111","Created: Jun 12, 2024 11:07 am Latest: Jun 12, 2024 11:10 am","alvations","I‚Äôm not sure if it‚Äôs a bug/feature but sometimes modifying the normalizer of a pretrained tokenizer works but sometimes it doesn‚Äôt. For example, it works for ""mistralai/Mistral-7B-v0.1"" but not ""mistralai/Mistral-7B-v0.3"": from transformers import AutoTokenizer from tokenizers.normalizers import Sequence, Replace, Prepend  tokenizer_name = ""mistralai/Mistral-7B-v0.1"" old_tok = AutoTokenizer.from_pretrained(tokenizer_name)  assert old_tok.backend_tokenizer.normalizer != None  new_normalizer = Sequence(     [Prepend('‚ñÅ'), Replace('‚ñÅ', ' '), Replace(""foo"", ""bar""), Replace('<br>', '\n')] )  old_tok.backend_tokenizer.normalizer = new_normalizer new_tokenizdr_name = f""new_tokenizer-{tokenizer_name}"" old_tok.save_pretrained(new_tokenizdr_name)   old_tok = AutoTokenizer.from_pretrained(tokenizer_name) new_tok = AutoTokenizer.from_pretrained(new_tokenizdr_name) [out]: >>> print(' '.join(old_tok.batch_decode(old_tok(""I foo you<br>hello world"")['input_ids']))) <s> I foo you < br > hello world  >>> print(' '.join(new_tok.batch_decode(new_tok(""I foo you<br>hello world"")['input_ids']))) <s>  I  bar  you   hello  world The same process above won‚Äôt work for ""mistralai/Mistral-7B-v0.3"".","Jun 12, 2024 11:07 am","alvations","Also raised issue on Modifying normalizer for pretrained tokenizers don't consistently work ¬∑ Issue #1552 ¬∑ huggingface/tokenizers ¬∑ GitHub And asked on python - How do we add/modify the normalizer in a pretrained Huggingface tokenizer? - Stack Overflow","Jun 12, 2024 11:08 am","alvations","A few more follow-up questions: How do we add/modify the normalizer in a pretrained Huggingface tokenizer? Can any normalizer from a pretrained tokenizer be modified or just specific ones? If the latter, why and how do we know if a pretrained tokenizer‚Äôs normalizer can be extended or modified? Can we add a normalizer when the pretrained one doesn‚Äôt already have one?","Jun 12, 2024 11:10 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with german umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct","https://discuss.huggingface.co/t/issue-with-german-umlauts-python-in-deepseek-ai-deepseek-coder-1-3b-instruct/73459","0","188","Created: Feb 16, 2024 12:03 pm","Michael22","Hi, I have an issue with German umlauts python in deepseek-ai/deepseek-coder-1.3b-instruct tokenizer. If I run the following script I got ÔøΩ f√ºr every German umlaut. from transformers import AutoTokenizer import torch DEVICE = ""cuda"" if torch.cuda.is_available() else ""cpu"" print(""Target Device: "", DEVICE) MODEL = ""deepseek-ai/deepseek-coder-1.3b-instruct"" tokenizer = AutoTokenizer.from_pretrained(MODEL) print (tokenizer) inputs = tokenizer(""A"", return_tensors=""pt"").to(""cuda"") inputs.input_ids[0][1] = 32009 print (inputs) decoded = tokenizer.decode(inputs.input_ids[0]) print (decoded) How to fix this?","Feb 16, 2024 12:03 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Construct a Marian tokenizer. Based on huggingface tokenizers","https://discuss.huggingface.co/t/construct-a-marian-tokenizer-based-on-huggingface-tokenizers/85679","0","185","Created: May 7, 2024 5:27 pm","RaphaelKalandadze","I have a Hugging Face tokenizer with three files: tokenizer.json, tokenizer_config.json, and vocab.txt. However, according to the documentation, the Marian tokenizer requires files in the SentencePiece format (.model and .vocab files). Is there a way to construct a Marian tokenizer without these specific file formats? I have already tried converting the tokenizer into a .model file and then constructing the Marian tokenizer, but it raised new issues. Are there any alternative approaches or workarounds to use the existing tokenizer files with the Marian tokenizer?","May 7, 2024 5:27 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Where the introduction of tokenizers.implementations?","https://discuss.huggingface.co/t/where-the-introduction-of-tokenizers-implementations/38959","0","183","Created: May 7, 2023 8:51 am","Love2DL","I look at the doc in Tokenizers, i don‚Äôt find the introduction of tokenizers.implementations.Can give me a website, thank you!","May 7, 2023 8:51 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unable to register my own tokenizer","https://discuss.huggingface.co/t/unable-to-register-my-own-tokenizer/63024","0","179","Created: Nov 21, 2023 2:37 pm","RANITBAG","I have tried to register my own tokenization model based on sentencepiece using CustomAITokenizer.register_for_auto_class(‚ÄúAutoTokenizer‚Äù) . But I am falied to do so. RANITBAG/CustomAItokenizer at main . This the repo link. Can anyone help me in this?","Nov 21, 2023 2:37 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"How to handle translations one source language to many target sentences for the same language","https://discuss.huggingface.co/t/how-to-handle-translations-one-source-language-to-many-target-sentences-for-the-same-language/61669","0","178","Created: Nov 9, 2023 3:29 pm","sancelot","Hi, I am training a m2m100 model with more technical corpus data using IATE database (https://iate.europa.eu/) I don‚Äôt know how to handle a case , when a source text may have different significations for the same sentence. That means for one source language sentence , I will assign 3 targets language definitions What would happen ??? by example , I have these data for the same term, I have 3 associated definitions in english language : line[61163] = ['1448173', 'mechanical engineering', 'en', 'to jam', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z'] line[61164] = ['1448173', 'mechanical engineering', 'en', 'to seize', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z'] line[61165] = ['1448173', 'mechanical engineering', 'en', 'to get stuck', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z'] line[61166] = ['1448173', 'mechanical engineering', 'de', 'festlaufen', 'Term', 'Reliable', '', '0', '', '', 'COM', '2014-05-19T14:56:28.001Z']","Nov 9, 2023 3:29 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Is it safe to assume tokenizer does not change after initialization?","https://discuss.huggingface.co/t/is-it-safe-to-assume-tokenizer-does-not-change-after-initialization/79379","0","171","Created: Mar 30, 2024 3:46 am","youkaichao","Hi, I find many attributes in tokenizer are very expensive to compute, e.g.     def __len__(self):         """"""         Size of the full vocabulary with the added tokens. Counts the `keys` and not the `values` because otherwise if         there is a hole in the vocab, we will add tokenizers at a wrong index.         """"""         return len(set(self.get_vocab().keys())) Therefore, I want to cache the results of some attributes on tokenizer. I do see some functions that can change a tokenizer, but I find they are only used during initialization, and these methods are marked as private by an _ prefix, e.g. _update_trie/_add_tokens . Is it safe to assume a tokenizer does not change after initialization? If not, what are some typical use cases to change a tokenizer after initialization?","Mar 30, 2024 3:46 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Special_tokens_mask","https://discuss.huggingface.co/t/special-tokens-mask/69161","0","169","Created: Jan 15, 2024 4:53 am","suhg-203","Hi there, when training tokenizer from scratch, after encoding text, I caught a weird special_tokens_mask. Not as I expected, the special_tokens_mask is not [1, 0, 0, 0, 0, 0, 0, 0, 0, 1] . Bellow is my script. Can anyone help me out? Thank you very much!","Jan 15, 2024 4:53 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Preprocessing of dataset","https://discuss.huggingface.co/t/preprocessing-of-dataset/81086","0","166","Created: Apr 10, 2024 1:54 pm Latest: Apr 10, 2024 2:01 pm","boringblobking","I‚Äôm going through this notebook and below is a gist of some of it: datasets = load_dataset(""squad_v2"") tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-uncased"") example = datasets[""train""][i]  tokenized_example = tokenizer(     example[""question""],     example[""context""],     max_length=max_length,     truncation=""only_second"",     return_overflowing_tokens=True,     return_offsets_mapping=True,     stride=doc_stride )  sequence_ids = tokenized_example.sequence_ids()  answers = example[""answers""] start_char = answers[""answer_start""][0] end_char = start_char + len(answers[""text""][0]) # Start token index of the current span in the text. token_start_index = 0 while sequence_ids[token_start_index] != 1:     token_start_index += 1  # End token index of the current span in the text. token_end_index = len(tokenized_example[""input_ids""][0]) - 1 while sequence_ids[token_end_index] != 1:     token_end_index -= 1 So as you can see, return_overflowing_tokens is True so the tokenized_examples input_ids are a list of lists so tokenized_example[‚Äúinput_ids‚Äù][0] would contain just the first segment of the context for that sample. So then to get token_end_index, the code starts at len(tokenized_example[‚Äúinput_ids‚Äù][0]) - 1 and then moves backwards until it finds a 1. But surely len(tokenized_example[‚Äúinput_ids‚Äù][0]) - 1 isn‚Äôt going to be the end of the context because len(tokenized_example[‚Äúinput_ids‚Äù][0]) is only going to give you the length of the first segment of the context?","Apr 10, 2024 1:54 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Treat Hawaiian Glottal stop as consonant, not punctuation","https://discuss.huggingface.co/t/treat-hawaiian-glottal-stop-as-consonant-not-punctuation/82531","0","165","Created: Apr 19, 2024 2:33 am","HURIMOZ","I ªm struggling to get the  ªokina, the Hawaiian glottal stop character (U+02BB), treated as a letter and not as punctuation in SentencePiece subword tokenization, whether BPE or Unigram. Can someone tell me how I can achieve that please? This is what I attempted: spm_train --input=tgt-train.txt --model_prefix=data/tgt_spm --vocab_size=32000 --model_type=bpe --character_coverage=1.0 --output_format=piece --input_sentence_size=1000000 --user_defined_symbols= ªa, ªe, ªi, ªo, ªu, ªƒÅ, ªƒì, ªƒ´, ª≈ç, ª≈´ However, only these specific tokens above are listed in the created vocab file, when I want all tokens coming from a word that contains the glottal stop to carry the glottal stop.","Apr 19, 2024 2:33 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"The process for tokenizing concatenated dataset is slow st the end of tokenizing","https://discuss.huggingface.co/t/the-process-for-tokenizing-concatenated-dataset-is-slow-st-the-end-of-tokenizing/60412","0","163","Created: Oct 30, 2023 10:35 am","pkr7098","I am tokenizing wikipedia English and bookcorpus dataset, which is concatenated in one dataset for training GPT2. Tokenizing each of dataset is fast(i.e. not concatenated) but after concatenation, the tokenizing process is extremly slow at the end of tokenizing. I am using fast tokenizer option","Oct 30, 2023 10:35 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Issue with Loading Custom Tokenizer: Tokenizer class BaseTokenizer does not exist or is not currently imported Error","https://discuss.huggingface.co/t/issue-with-loading-custom-tokenizer-tokenizer-class-basetokenizer-does-not-exist-or-is-not-currently-imported-error/115874","6","36","Created: Nov 5, 2024 7:46 pm Latest: Nov 6, 2024 8:17 am","antoshka1608","I‚Äôm encountering an issue when trying to load my custom tokenizer from a model repository on the Hugging Face Hub. Despite following the documentation for custom tokenizers. huggingface.co antoshka1608/wordpiece-tokenizer-v1 at main We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science. To load the tokenizer, I‚Äôm using: from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(‚Äúantoshka1608/wordpiece-tokenizer-v1‚Äù, use_fast=False, trust_remote_code=True) When loading the tokenizer, it downloads tokenizer_config.json and vocab.json but then fails with the Tokenizer class BaseTokenizer does not exist or is not currently imported error. Has anyone else encountered this issue or have suggestions on what might be going wrong? Any guidance on troubleshooting this would be greatly appreciated! Thank you for your help!","Nov 5, 2024 7:46 pm","John6666","That error usually occurs when the transoformers library is out of date, but it‚Äôs hard to imagine that you‚Äôre using a version so old that BaseTokeniker isn‚Äôt defined. pip install -U transformers There could be some other cause, such as the manual now being out of date.","Nov 6, 2024 4:14 am","antoshka1608","Sorry, for, probably, misleading you, its not transformer‚Äôs tokenizer, but my custom ( i did it, inherited from PreTrainedTokenizer) And yes, i reinstalled transformers mant times, doesnt work. After your reply, i thought that this is might be issue with name duplication, but no. With new name of my tokenizer the same error(but of course with new tokenizer name ) .","Nov 6, 2024 7:21 am","John6666","huggingface.co CohereForAI/c4ai-command-r-plus ¬∑ ValueError: Tokenizer class CohereTokenizer... Running this example script gives an error Oh, I see. The name is the same as the existing one, and the tokenizer is also a custom one. Have you encountered this bug? Also, in the case of HF in general, there are cases where information that was true at the time is now false in the manual, so I think the easiest thing to do is to refer to the py or json of someone else‚Äôs model that is working. It‚Äôs tough if there is no similar model‚Ä¶ The next best thing is to refer to the code of the library itself.","Nov 6, 2024 7:53 am","antoshka1608","Yeah, thank you for you respond, anyway!! Actually, if i import my tokenizer directly from directory of my project. tokenizer = CustomBaseTokenizer.from_pretrained(hugging_face_name) It works good, but probably it takes just vocab file and doesnt use another files, so we cant check anything here. Ill try your idea about other‚Äôs works later.","Nov 6, 2024 8:03 am","John6666","Many of the HF libraries use hard-coded file names, so sometimes they work and sometimes they don‚Äôt. If it works locally but not online, the problem is often with the file names, their placement, or the YAML part of README.md (the de facto repository configuration file). Also, in your case, since you‚Äôre using a gated model, there‚Äôs a chance that the error is occurring because you‚Äôre failing to pass the token. Even when calling the tokenizer, you need a token to read the repo. If it seems like a bug in the library, you‚Äôll need to find a way to make the error more visible and identify the bug itself. Or you could find the part that‚Äôs causing the problem and bypass it. In this case, I‚Äôll go read the github‚Ä¶","Nov 6, 2024 8:10 am","antoshka1608","Yep, going to do a deep dive later.","Nov 6, 2024 8:17 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer: what function removes spaces between ‚Äò<‚Äô and ‚Äò>‚Äô?","https://discuss.huggingface.co/t/tokenizer-what-function-removes-spaces-between-and/130257","0","30","Created: Dec 9, 2024 7:46 pm","idruker","Here is an detokenized sequence: print(tokenizer.decode(generated_ids[0][input_ids.shape[-1]:], skip_special_tokens=False, clean_up_tokenization_space=False)) <tool_call> {‚Äúarguments‚Äù: {‚Äúsymbol‚Äù: ‚ÄúAAPL‚Äù}, ‚Äúname‚Äù: ‚Äúget_stock_fundamentals‚Äù} </tool_call><|im_end|> As you see, </tool_call> is a construct without spaces. The original sequence of token ids is 700 6462 28730 2845 28767 which is ‚Äú</‚Äù ‚Äútool‚Äù ‚Äú_‚Äù ‚Äúcall‚Äù ‚Äú>‚Äù What Transformers function implements the logic of removal of the spaces? How does it know that ‚Äòtool‚Äô, ‚Äò_‚Äô and ‚Äòcall‚Äô are part of one keyword? Would appreciate your guidance.","Dec 9, 2024 7:46 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Paligemma model Forward Method Not Returning Loss in Trainer #31045","https://discuss.huggingface.co/t/paligemma-model-forward-method-not-returning-loss-in-trainer-31045/88591","0","155","Created: May 26, 2024 2:55 pm","damerajee","I was trying to finetune goggle new vision lanaguge model and I keep getting this error I was running the code on kaggle notebook free T4 and also tried with P100 Google Colab `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`. --------------------------------------------------------------------------- ValueError                                Traceback (most recent call last) Cell In[14], line 1 ----> 1 trainer.train()  File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1876, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)    1873 try:    1874     # Disable progress bars when uploading models during checkpoints to avoid polluting stdout    1875     hf_hub_utils.disable_progress_bars() -> 1876     return inner_training_loop(    1877         args=args,    1878         resume_from_checkpoint=resume_from_checkpoint,    1879         trial=trial,    1880         ignore_keys_for_eval=ignore_keys_for_eval,    1881     )    1882 finally:    1883     hf_hub_utils.enable_progress_bars()  File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2216, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)    2213     self.control = self.callback_handler.on_step_begin(args, self.state, self.control)    2215 with self.accelerator.accumulate(model): -> 2216     tr_loss_step = self.training_step(model, inputs)    2218 if (    2219     args.logging_nan_inf_filter    2220     and not is_torch_xla_available()    2221     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))    2222 ):    2223     # if loss is nan or inf simply add the average of previous logged losses    2224     tr_loss += tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)  File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3238, in Trainer.training_step(self, model, inputs)    3235     return loss_mb.reduce_mean().detach().to(self.args.device)    3237 with self.compute_loss_context_manager(): -> 3238     loss = self.compute_loss(model, inputs)    3240 del inputs    3241 torch.cuda.empty_cache()  File /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3282, in Trainer.compute_loss(self, model, inputs, return_outputs)    3280 else:    3281     if isinstance(outputs, dict) and ""loss"" not in outputs: -> 3282         raise ValueError(    3283             ""The model did not return a loss from the inputs, only the following keys: ""    3284             f""{','.join(outputs.keys())}. For reference, the inputs it received are {','.join(inputs.keys())}.""    3285         )    3286     # We don't use .loss here since the model may return tuples instead of ModelOutput.    3287     loss = outputs[""loss""] if isinstance(outputs, dict) else outputs[0]  ValueError: The model did not return a loss from the inputs, only the following keys: logits. For reference, the inputs it received are input_ids,attention_mask,pixel_values,labels.","May 26, 2024 2:55 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Which file stores token frequency in SentencePieceBPETokenizer?","https://discuss.huggingface.co/t/which-file-stores-token-frequency-in-sentencepiecebpetokenizer/84954","0","155","Created: May 3, 2024 6:52 am","raptorkwok","After training a Tokenizer, there are several files generated: merges.txt special_tokens_map.json tokenizer.json tokenizer_config.json vocab.json However, none of these store the frequency of the tokens found in the training dataset. Does it mean the training process does not store such data? The reason for this question is that I found that some words that frequently appeared in the dataset are not included in the token list, while some words with fewer occurrences are included in the token list. How does a SentencePieceBPETokenizer choose tokens from a dataset?","May 3, 2024 6:52 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Translate from one tokenizer to another","https://discuss.huggingface.co/t/translate-from-one-tokenizer-to-another/81811","0","154","Created: Apr 15, 2024 9:03 am Latest: Apr 15, 2024 9:03 am","broadwayj","I got tokens encoded with one tokenizer which I want to yield to the LM with another tokenizer. problem seems natural so may be there exist some convenient and unified solution to do so? Here‚Äôs what I use rn def translate_to_other_tokenizer(ids, tokenizer_from, tokenizer_to):     text = tokenizer_from.batch_decode(         ids,         skip_special_tokens=True,         clean_up_tokenization_spaces=True     )      output = tokenizer_to(         text,         truncation=False,         padding=True,         return_attention_mask=True,         return_special_tokens_mask=True,         return_tensors='pt',     )      return text, output","Apr 15, 2024 9:03 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"OPT special tokens","https://discuss.huggingface.co/t/opt-special-tokens/78667","0","145","Created: Mar 25, 2024 8:56 am","aljazpotocnik","Hello, I can‚Äôt understand something about OPT tokenizer and its special tokens. I came across <unk> token in opt vocabulary. But when encoding it with opt_tokenizer it doesn‚Äôt find it as a single token but three tokens: <, unk, >. How does this make sense? Below is some code to reproduce my findings: opt_tokenizer = AutoTokenizer.from_pretrained(""facebook/opt-1.3b"")  print('<unk>' in opt_tokenizer.get_vocab())  ids = opt_tokenizer.encode(""<unk>"", add_special_tokens=False)  print(opt_tokenizer.convert_ids_to_tokens(ids)) Another funny thing is that <unk> token isn‚Äôt present in opt_tokenizer.special_tokens_map: print(opt_tokenizer.special_tokens_map) {‚Äòbos_token‚Äô: ‚Äò</s>‚Äô, ‚Äòeos_token‚Äô: ‚Äò</s>‚Äô, ‚Äòunk_token‚Äô: ‚Äò</s>‚Äô, ‚Äòpad_token‚Äô: ‚Äò<pad>‚Äô} Best, AP","Mar 25, 2024 8:56 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BUGs on offset-mapping","https://discuss.huggingface.co/t/bugs-on-offset-mapping/88313","0","143","Created: May 24, 2024 4:39 pm","Ruiton","Hi guys, I recently notice that the offset mapping returned by tokenizer seems to be problematic. I am working with the llama3-8b-Instruct model with dtype=fp16. text='''Suppose A represents a certain relation. Infer the relation based on certain examples.''' token_ranges = tokenizer(text,return_offsets_mapping=True)['offset_mapping']  and what it returns is: [(0, 0), (0, 3), (3, 7), (7, 7), (9, 9), (20, 20), (22, 22), (30, 30), (39, 39), (40, 40), (46, 46), (50, 50), (59, 59), (65, 65), (68, 68), (76, 76), (85, 85)] I assume that what it should return is the starting and end position of the token. Is there a bug or something that I misunderstand?","May 24, 2024 4:39 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Encode token without spaced between them","https://discuss.huggingface.co/t/encode-token-without-spaced-between-them/85955","0","141","Created: May 9, 2024 11:03 am Latest: May 9, 2024 11:07 am","ron5569","I‚Äôm working with an LLM that generates files in unified diff format. However, in some cases, the LLM generates invalid output due to spaces between tokens. For example --- EventsLiteTestKit.scala 2024-05-31 07:00:00 +++ EventsLiteTestKit.scala 2024-05-31 07:00:01 @@ -39,10 +39,10 @@   import java.util.UUID  import scala.collection.concurrent.TrieMap - import scala.concurrent.Future + import scala.concurrent.{Future, _}  import scala.concurrent.Future.{failed, successful}   class EventsLiteTestKit { This is not a valid patch file because of the space between the ‚Äò-‚Äô character and the word ‚Äòimport‚Äô. Any ideas on how to force the model to encode such that there are no spaces after the ‚Äò-‚Äô and ‚Äò+‚Äô symbols?","May 9, 2024 11:03 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AutoTokenizer.encode with multiThread and mutliProcess","https://discuss.huggingface.co/t/autotokenizer-encode-with-multithread-and-mutliprocess/110822","2","43","Created: Oct 9, 2024 3:48 am Latest: Oct 9, 2024 9:20 am","DaleMeng","Hello, every one, I want to convert the all texts to index tokens and save them in files. So I use the encode Function to do it. I tried it with the multiProcess way and multiThread way, tokenizer = AutoTokenizer.from_pretrained(""./llama2-tokenizer"", trust_remote_code=True)  #MultiThread way with ThreadPoolExecutor(max_workers=8) as pool:         result = list(pool.map(tokenizer,data))  #data is a list of str  #MultiProcess way with ProcessPoolExecutor(max_workers=8) as pool:         result = list(pool.map(tokenizer,data)) result shows the MultiProcess way is much slower than the MultiThread way. With total 1M tokens, MultiProcess way takes about 34s while MultiThread way takes about 2s. Isn‚Äôt the encode function computationally intensive? Since the low-level implementation of the encode is in the Tokenizers package with Rust. I can not figure out how the above result was caused. Can anybody offers some explanation ? Thanks!","Oct 9, 2024 3:48 am","samchain","Hey Best option would be to use the hugging face datasets class and use the ‚Äú.map()‚Äù method with an argument ‚Äúnum_proc‚Äù that enables parallel tokenization : Main classes","Oct 9, 2024 7:59 am","DaleMeng","Thanks for your answer! I know that it is one of a good way to use huggingface datasets.map function. But I also wonder how to do it in a more custom way without such module. And I find that by adding the chunksize parameter, the total time with MultiProcess way decrease a lot, about that same as MultiThread way.","Oct 9, 2024 9:20 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Escape symbol appearance","https://discuss.huggingface.co/t/escape-symbol-appearance/82015","0","125","Created: Apr 16, 2024 9:33 am","broadwayj","I have a dataset of prefixes which I would like to continue up with my LM. Further I want to build up a dataset of generated sentences and save it to csv file as raw text. Thereby I do not want delimiter to appear within generated symbols by no means. Currently I am using parameter bad_words_ids=DELIMITER_SYMBOL_ID in GenerationConfig of LM to ban delimiter symbol from being generated. However, theoretically this symbol could be a part of some other more complex token from tokenizer‚Äôs keys (e.g. if delimiter is !, then token !!! could still persist in tokenizer), which will be decoded to text with banned delimiter symbol. How do I completely erase this symbol from final text? I could only come up with filtration of resulted text to manually substitute all delimiters with some third party symbol. May be there exist some more convenient ways?","Apr 16, 2024 9:33 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Printing tokens array","https://discuss.huggingface.co/t/printing-tokens-array/81427","0","124","Created: Apr 12, 2024 1:06 pm","Mahmoodn","Hi, How can I see the tokens with tokenizer()? In the example: raw_inputs = [     ""I've been waiting for a HuggingFace course my whole life."",     ""I hate this so much!"", ] inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=""pt"") print(inputs) The inputs contains the unique numbers. I would like to see the tokens. For example, I want to know if ['I've'] is the token or ['i',''','ve'] is the token.","Apr 12, 2024 1:06 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Unusual input_id size for distilBERT tokenizer","https://discuss.huggingface.co/t/unusual-input-id-size-for-distilbert-tokenizer/86695","0","112","Created: May 14, 2024 5:29 pm","rishuhuhu","I am tokenizing a news article using DistilBERT tokenizer whose max token length supposed to be 512, but I a getting 280, I dont know what is the issue, I have worked with DistilBERT before and it used to give 512 as the max length‚Ä¶","May 14, 2024 5:29 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Create entirely new vocabulary for tokenizer","https://discuss.huggingface.co/t/create-entirely-new-vocabulary-for-tokenizer/89453","0","112","Created: May 30, 2024 3:48 pm","Svangorden13","I am building an encoder-decoder model based off facebook-bart-base for the purpose of solving math problems. I would like to train the model so that the decoder can only output a small set of words (e.g. ‚Äúmultiply‚Äù, ‚Äúdivide‚Äù, ‚Äúadd‚Äù, ‚Äúsubtract‚Äù, etc.) and numbers. Is it possible to completely redefine the corpus used by the decoder tokenizer, rather than just adding new tokens to it?","May 30, 2024 3:48 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Emojis poisoning tokenizer","https://discuss.huggingface.co/t/emojis-poisoning-tokenizer/92479","0","103","Created: Jun 17, 2024 2:09 am","Dts1","I have a problem with emojis ‚Äúpoisoning‚Äù tokenizers and I wonder if there is an existing solution to deal with this problem. So far, I was not able to find it by doing different web searches. I have some ideas how to solve it by myself but I‚Äôd like to know if somebody already dealt with it. The problem is the following: is an emoji is not separated from a word, then the whole word is marked as UNK token. Example: from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(‚Äúsentence-transformers/distiluse-base-multilingual-cased-v1‚Äù) text=‚Äúsky‚Äù print(tokenizer(text).input_ids) text=‚ÄúskyüôÇ‚Äù print(tokenizer(text).input_ids) text=‚Äúsky‚Äù print(tokenizer(text).input_ids) Output: [101, 62368, 102] [101, 100, 102] [101, 100, 102]","Jun 17, 2024 2:09 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Seq2SeqTrainer produces incorrect EvalPrediction after changing another Tokenizer","https://discuss.huggingface.co/t/seq2seqtrainer-produces-incorrect-evalprediction-after-changing-another-tokenizer/91435","0","94","Created: Jun 11, 2024 2:44 am","raptorkwok","I‚Äôm using Seq2SeqTrainer to train my model with a custom tokenizer. The base model is BART Chinese (fnlp/bart-base-chinese). If the original tokenizer of BART Chinese is used, the output is normal. Yet when I swap the tokenizer with another tokenizer that I made, the output of compute_metrics, specifically the preds part of EvalPrediction is incorrect (the decoded text becomes garbage). The codes are as follows: model = BartForConditionalGeneration.from_pretrained(checkpoint) model.resize_token_embeddings(len(tokenizer)) model.config.vocab_size = len(tokenizer)  steps = 500 # small value for debug purpose batch_size = 4 training_args = CustomSeq2SeqTrainingArguments(     output_dir = ""my_output_dir"",     evaluation_strategy = IntervalStrategy.STEPS,     optim = ""adamw_torch"",     eval_steps = steps,     logging_steps = steps,     save_steps = steps,     learning_rate = 2e-5,     per_device_train_batch_size = batch_size,     per_device_eval_batch_size = batch_size,     weight_decay = 0.01,     save_total_limit = 1,     num_train_epochs = 30,     predict_with_generate = True,     remove_unused_columns = False,      fp16 = True, # save memory     metric_for_best_model = ""bleu"",     load_best_model_at_end = True,     report_to = ""wandb"",     # HuggingFace Hub related     hub_token = hf_token,     push_to_hub = True,     save_safetensors = True, )  trainer = Seq2SeqTrainer(     model = model,     args = training_args,     train_dataset = tokenized_train_dataset,     eval_dataset = tokenized_eval_dataset,     tokenizer = tokenizer,     data_collator = data_collator,     compute_metrics = compute_metrics,     callbacks = [EarlyStoppingCallback(early_stopping_patience=3)], ) which the tokenizer is my custom tokenizer. The result is normal if my tokenizer uses the original tokenizer (tokenizer = BertTokenizer.from_pretrained(checkpoint)). For the compute_metrics, it is as follows: def postprocess_text(preds, labels):     preds = [pred.strip() for pred in preds]     labels = [[label.strip()] for label in labels]      return preds, labels  def compute_metrics(eval_preds):     preds, labels = eval_preds      print(""Preds and Labels:"", preds[0], labels[0])          if isinstance(preds, tuple):         preds = preds[0]     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)      labels = np.where(labels != -100, labels, tokenizer.pad_token_id)     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)      print(""Decoded Preds (before postprocess):"", decoded_preds[0])     print(""Decoded Labels (before postprocess):"", decoded_labels[0])      decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)     print(""Decoded Preds:"", decoded_preds[0])     print(""Decoded Labels:"", decoded_labels[0])      result_bleu = metric_bleu.compute(predictions=decoded_preds, references=decoded_labels, tokenize='zh')     result_chrf = metric_chrf.compute(predictions=decoded_preds, references=decoded_labels, word_order=2)     results = {""bleu"": result_bleu[""score""], ""chrf"": result_chrf[""score""]}      prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]     results[""gen_len""] = np.mean(prediction_lens)     results = {k: round(v, 4) for k, v in results.items()}     return results From the debug message, the output sentence does not make sense and consists of weird characters only. I think the model does not recognize the token IDs produced by my custom tokenizer. How should I tackle this problem? My goal is to train the model with my custom tokenizer.","Jun 11, 2024 2:44 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer method inference","https://discuss.huggingface.co/t/tokenizer-method-inference/114974","3","27","Created: Nov 1, 2024 6:31 am Latest: Nov 2, 2024 6:32 am","jinoooooooooo","This is a beginner question but How does huggingface infer what type of method to use for tokenizing the sentence? Can you point me where the implementation could be? Thanks","Nov 1, 2024 6:31 am","John6666","Maybe simply written in a json file in the same folder. huggingface.co tokenizer_config.json ¬∑ unsloth/Llama-3.2-1B-Instruct at main We‚Äôre on a journey to advance and democratize artificial intelligence through open source and open science.","Nov 2, 2024 1:39 am","jinoooooooooo","not really, i couldnt find it","Nov 2, 2024 6:14 am","John6666","Oh, you‚Äôre looking for code. The following is called based on the YAML set in the model repo and the json as above. in Llama‚Äôs case, this is it. github.com huggingface/transformers/blob/main/src/transformers/models/llama/tokenization_llama.py # coding=utf-8 # Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved. # # This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX # and OPT implementations in this library. It has been modified from its # original forms to accommodate minor architectural differences compared # to GPT-NeoX and OPT used by the Meta AI team that trained the model. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. This file has been truncated. show original","Nov 2, 2024 6:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Adding special tokens to LEDTokenizer","https://discuss.huggingface.co/t/adding-special-tokens-to-ledtokenizer/132987","0","17","Created: Dec 25, 2024 9:10 am Latest: Dec 25, 2024 11:34 am","KeertiPrem","Context I need to add 3 special section tokens to LED-base-16384 model vocabulary. (RoBERTa/LED uses ByteLevelBPE (Byte-Pair Encoding) and then finetune on a custom dataset. Current vocab size: 0 to 50264 i.e. 50265. Current Understanding Placeholder tokens exist: madeupword0000 (50261) madeupword0001 (50262) madeupword0002 (50263) Problem with mask token id in RoBERTa vocab ¬∑ Issue #1091 ¬∑ huggingface/transformers ¬∑ GitHub Temporary fix for RoBERTa's mismatch of vocab size and embedding size - issue #1091 by amirsaffari ¬∑ Pull Request #1096 ¬∑ huggingface/transformers ¬∑ GitHub Two Possible Approaches: A. Replace placeholder tokens (50261-50263) Maintains multiple of 8 (as mentioned in the above links) Uncertain about impact on model B. Add after (50265-50267) Safer but loses multiple of 8 New size would be 50268 Questions Which approach is recommended? What are the performance implications of: Replacing placeholder tokens Losing multiple-of-8 alignment Are there any documented cases of successfully replacing placeholder tokens? Will either approach affect the model‚Äôs learned patterns?","Dec 25, 2024 9:10 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer splits words with accents into separate subwords","https://discuss.huggingface.co/t/tokenizer-splits-words-with-accents-into-separate-subwords/93088","0","78","Created: Jun 20, 2024 1:26 am","leestevennz","Hi there, My aim is to finetune an existing pretrained LLM on a new language. My new language contains vowels with the following accents: [‚ÄòƒÄ‚Äô, ‚ÄòƒÅ‚Äô, ‚Äòƒí‚Äô, ‚Äòƒì‚Äô, ‚Äòƒ™‚Äô, ‚Äòƒ´‚Äô, ‚Äò≈å‚Äô, ‚Äò≈ç‚Äô, ‚Äò≈™‚Äô, ‚Äò≈´‚Äô]. I first train a new tokenizer on my target language. This tokenizer performs well on my target language. Common words that include accents are split into a new token. new_tokenizer = tokenizer.train_new_from_iterator(training_corpus,                                                            new_vocab_size) I then add the vocab from the the new tokenizer to the pretrained tokenizer: new_vocab = list(new_tokenizer.vocab.keys()) tokenizer.add_tokens(new_vocab) My issue is that when I use the tokenizer with the added tokens, it ignores the added tokens and always split accented vowels into a seperate token. Does anyone know how to solve this issue?","Jun 20, 2024 1:26 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Byte Level Tokenizer While Training","https://discuss.huggingface.co/t/byte-level-tokenizer-while-training/131009","0","16","Created: Dec 14, 2024 1:27 am Latest: Dec 14, 2024 2:19 am","dinesh-bk","Hi, I have trained the tokenizer using the model BPE and pre tokenizer as ByteLevel tokenizer = Tokenizer(models.BPE(unk_token=""[UNK]"")) tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel() tokenizer.decoder = decoders.ByteLevel()  Now, my vocabulary is saved in bytes and tokenizer.tokenize give me output in bytes too which is obvious. tokenizer.tokenize output is ['ƒ†√†¬§¬®',  '√†¬•ƒ©',  '√†¬§¬™',  '√†¬§¬æ',  '√†¬§¬≤',  '√†¬•ƒ¢',  'ƒ†√†¬§≈É',  '√†¬§¬æ',  '√†¬§¬∑',  '√†¬§¬æ',  '√†¬§¬Æ',  '√†¬§¬æ',  'ƒ†√†¬§¬Ø',  '√†¬•ƒ≠',  'ƒ†√†¬§ƒ±√†¬§ƒ∑',  'ƒ†√†¬§ƒ´√†¬§¬¶',  '√†¬§¬æ',  '√†¬§¬π√†¬§¬∞√†¬§¬£',  'ƒ†√†¬§¬π',  '√†¬•ƒ≠√†¬•¬§'] . Is there way to save my vocabulary in unicode character rather than bytes and show tokens in unicode characters too?","Dec 14, 2024 1:27 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"When I using the chat_template of llama 2 tokenizer the response of IT model is nothing","https://discuss.huggingface.co/t/when-i-using-the-chat-template-of-llama-2-tokenizer-the-response-of-it-model-is-nothing/97098","0","76","Created: Jul 13, 2024 3:52 am Latest: Jul 13, 2024 4:01 am","thaisonatk","My model was instruction data follow the template blow, I can try to generate some answer but the answer always return repeated of question without answer. There is my code : import torch  device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")  # from peft import PeftModel, PeftConfig  from transformers import AutoModelForCausalLM, AutoTokenizer  tokenizer = AutoTokenizer.from_pretrained(""/data/tqlong/project_gamma/Son/mem/checkpoint-9600"", trust_remote_code=True)  model = AutoModelForCausalLM.from_pretrained(""/data/tqlong/project_gamma/Son/mem/checkpoint-9600"", trust_remote_code=True).to(device)  EOS_TOKEN = tokenizer.eos_token def format_prompt(prompt):     text = [         {""role"": ""system"", ""content"": ""You are a helpful assistant.""},         {""role"": ""user"", ""content"": prompt},     ]     mess = tokenizer.apply_chat_template(text, tokenize=False) + EOS_TOKEN     return mess  def generate(prompt, tokenizer, max_new_tokens):     text = format_prompt(prompt)     text += ""\n<|im_start|>assistant\n""     input_ids = tokenizer(text, return_tensors=""pt"", truncation=True).input_ids.cuda()     outputs = model.generate(input_ids=input_ids, max_new_tokens=max_new_tokens,                              do_sample=False,pad_token_id=tokenizer.pad_token_id,                              forced_eos_token_id=tokenizer.eos_token_id)     outputs = outputs[0].tolist()     # Stop decoding when hitting the EOS token     return tokenizer.decode(outputs)      text = """"""Who is Bill Gates ?"""""" print(generate(text, tokenizer, 256)) The model is ELM and the tokenizer is llama-2 And the answer is : <s><s> [INST] <<SYS>> You are a helpful assistant. <</SYS>>  Who is Bill Gates [/INST]</s>  <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start|>assistant <|im_end|>  You are a helpful assistant. <|im_start</s> I don‚Äôt know how to sovle my problem. Please help me!! Thank you","Jul 13, 2024 3:52 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Update encode function slowTokenizer vs FastTokenizer","https://discuss.huggingface.co/t/update-encode-function-slowtokenizer-vs-fasttokenizer/96946","0","47","Created: Jul 12, 2024 8:04 am","hathyzazda","Hello, I am trying to build a personalized tokenizer on top on GPT2 Tokenizer, so that‚Äôs why I am modifying the code. I want to build encode words following a specific manner. The code for the slow tokenizer is here: github.com huggingface/transformers/blob/a695c18649fc6ab4b1fb1d9c8cfa9258c5908e2a/src/transformers/models/gpt2/tokenization_gpt2.py # coding=utf-8 # Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """"""Tokenization classes for OpenAI GPT.""""""  import json import os from functools import lru_cache from typing import List, Optional, Tuple This file has been truncated. show original The code for the fast tokenizer is here: github.com huggingface/transformers/blob/a695c18649fc6ab4b1fb1d9c8cfa9258c5908e2a/src/transformers/models/gpt2/tokenization_gpt2_fast.py # coding=utf-8 # Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team. # # Licensed under the Apache License, Version 2.0 (the ""License""); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # #     http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an ""AS IS"" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. """"""Tokenization classes for OpenAI GPT.""""""  import json from typing import Optional, Tuple  from tokenizers import pre_tokenizers This file has been truncated. show original To do pre-tokenization for the slowTokenizer is use ‚Äú_tokenize()‚Äù method. My question is how to do the same for the fast tokenizer?","Jul 12, 2024 8:04 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Errors with Tokenizers on Llama","https://discuss.huggingface.co/t/errors-with-tokenizers-on-llama/132264","1","20","Created: Dec 19, 2024 11:52 am Latest: Dec 19, 2024 1:19 pm","dristi","I tried downloading llama3 on my server and train. I downloaded it from meta and I tried to convert the raw meta data. I have protobuf and sentencepiece downloaded still it says I have to download it. ValueError: Failed to instantiate tokenizer. Please, make sure you have sentencepiece and protobuf installed.","Dec 19, 2024 11:52 am","John6666","Assuming that PyTorch is installed. This should be fine, but there is a possibility that the actual error is occurring elsewhere. pip install protobuf sentencepiece transformers","Dec 19, 2024 1:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HugginChat (Android App)","https://discuss.huggingface.co/t/hugginchat-android-app/101530","0","35","Created: Aug 8, 2024 3:19 pm","gabrielparca","I get this 403 error when attempting logging in: Invalid or expired CSRF token HuggingChat v.1.2 (Play Store) Samsung SM-A346E, Android 14, ONE UI 6.1","Aug 8, 2024 3:19 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Memory leaks when training Gemma or Phi 3 and 3.5 tokenizer","https://discuss.huggingface.co/t/memory-leaks-when-training-gemma-or-phi-3-and-3-5-tokenizer/104499","0","31","Created: Aug 29, 2024 12:58 pm Latest: Aug 29, 2024 1:01 pm","MilosKovacevic68","I have a problem when training a new tokenizer for Gemma 2 2B or Phi 3 and 3.5 models using the following code: def corpus_gen(dataset, batch_size=300, n=300_000):     current = []     tot = 0     for ex in dataset:         current.append(ex['txt'])         tot += 1         if tot == n: break         if len(current) == batch_size:             yield current             current = []     if current:         yield current  def train_tokenizer():     dataset = load_dataset(         ""json"",          split=""train"",         streaming=True,         data_files=[             ""../serlama/tokenizer/paragraphs_tokenizer.jsonl"",             ""../serlama/tokenizer/pdrs_tokenizer.jsonl"",             ""../serlama/tokenizer/macocu_tokenizer.jsonl"",     ])      existing_tokenizer = AutoTokenizer.from_pretrained(""google/gemma-2-2b-it"")           new_tokenizer = existing_tokenizer.train_new_from_iterator(         corpus_gen(dataset),          vocab_size=30000,          min_frequency=3     )     new_tokenizer.save_pretrained(""sr_tokenizer"")  train_tokenizer() After n= 100 000 (examples) my RAM steadily increases in blocks of few gigabytes and i cannot train the tokenizer. When i try the same code with Llama 3.1 tokenizer everything is ok and the RAM does not increase. My transformers version is 4.44.0 Why is that? What is the problem with Gemma 2 2B and Phi3 tokenizers. Do they have a memory leak problems?","Aug 29, 2024 12:58 pm","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenizer Error [AGAIN!]","https://discuss.huggingface.co/t/tokenizer-error-again/106104","0","32","Created: Sep 10, 2024 5:10 am","gorski","When running lucataco/flux-dev-multi-lora ‚Äì Run with an API on Replicate, I‚Äôm getting the following error: gorski/flux-avatar-3 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models' If this is a private repository, make sure to pass a token having permission to this repo with tokenor log in withhuggingface-cli login. Based on my searches, it looks like Hugging Faces tokenizer error that keeps returning since 2023. Does anyone have any suggestions?","Sep 10, 2024 5:10 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Tokenization compared to sentencepiece","https://discuss.huggingface.co/t/tokenization-compared-to-sentencepiece/106278","0","30","Created: Sep 11, 2024 8:43 am","marcin-ochman","Hi! Recently I tried to use microsoft/Phi-3-mini-4k-instruct tokenizer. I‚Äôve found that results of the transformers tokenizer is different than the output of sentencepiece tokenizer. To give an example, here‚Äôs the sample code reproducing that issue: tokenizer = AutoTokenizer.from_pretrained(""microsoft/Phi-3-mini-4k-instruct"") formatted_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False) # '<|user|>\nWhat is capital city of France?<|end|>\n<|assistant|>\n' tokenizer(formatted_text, return_tensors=""pt"") # {'input_ids': tensor([[32010,  1724,   338,  7483,  4272,   310,  3444, 29973, 32007, 32001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}  sp = spm.SentencePieceProcessor(model_file=""..."") sp.encode(formatted_text) # [32010, 13, 5618, 338, 7483, 4272, 310, 3444, 29973, 32007, 13, 32001, 13] Do you know why transformers modify the original text? Is this correct behaviour? It removes newlines and add spaces (13 is a newline character, while 5618 ‚Üí ‚ÄúWhat‚Äù and 1724 ‚Üí ‚Äú_What‚Äù.","Sep 11, 2024 8:43 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What does ‚Äútrim_offsets‚Äù do in tokenizer post-processor?","https://discuss.huggingface.co/t/what-does-trim-offsets-do-in-tokenizer-post-processor/103849","0","21","Created: Aug 25, 2024 2:32 am","cloudydory","In the documentation of ‚ÄúPost-processers‚Äù in Huggingface‚Äôs tokenizer library, many post-processors has a argument ‚Äútrim_offset‚Äù. And the explanation is like: It also takes care of trimming the offsets. By default, the ByteLevel BPE might include whitespaces in the produced tokens. If you don‚Äôt want the offsets to include these whitespaces, then this PostProcessor should be initialized with trim_offsets=True I am still confused by the explanation. Specifically: Why does the ‚Äúthe ByteLevel BPE might include whitespaces in the produced tokens‚Äù? Is it specific to Byte-Level BPE or also happens in BPE? What does the argument ‚Äútrim_offsets‚Äù actually do? Why do we ‚Äúdon‚Äôt want the offsets to include these whitespaces‚Äù?","Aug 25, 2024 2:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Fine tuning a T5 model for translation - How do I apply my trained tokenizer to the target sentences?","https://discuss.huggingface.co/t/fine-tuning-a-t5-model-for-translation-how-do-i-apply-my-trained-tokenizer-to-the-target-sentences/98442","0","19","Created: Jul 20, 2024 5:00 am","ourdearbenefactor","I am fine tuning a T5 model to translate between a custom pair of languages (Classical Nahuatl and English); the T5 was not trained on Classical Nahuatl, of course. I trained my own tokenizers for both the input language (Classical Nahuatl) and the target language (English). I‚Äôm following the tutorial for translation. From my own implementation, it seems that the target texts are tokenized with the input language‚Äôs tokenizer in the following line of code: model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True) in the preprocess_function function. That is, the target language‚Äôs tokenizer is not used at all, and only the input language‚Äôs tokenizer is used for both the input and target sentences. How do I apply the target language‚Äôs tokenizer on the target sentences? Will I need to tokenize the target sentences before implementing the code above?","Jul 20, 2024 5:00 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Decoding sequence of tokens produces question marks instead of actual tokens","https://discuss.huggingface.co/t/decoding-sequence-of-tokens-produces-question-marks-instead-of-actual-tokens/105087","1","12","Created: Sep 3, 2024 10:27 am Latest: Sep 3, 2024 10:28 am","souryadey","I am working with the mistralai/Mistral-7B-v0.1 model. I loaded the tokenizer via: tokenizer = transformers.AutoTokenizer.from_pretrained('mistralai/Mistral-7B-v0.1') then ran the following code: tokenizer.decode([69]) Produces output B. tokenizer.decode([198]) Produces output ÔøΩ. This is perhaps understandable since token ID 198 corresponds to token <0xC3>, which is the hex ASCII code for √É and may be unprint-able. But then: tokenizer.decode([69,198]) Produces output ÔøΩÔøΩ. I don‚Äôt know why it‚Äôs producing this instead of BÔøΩ. Any help will be appreciated!","Sep 3, 2024 10:27 am","souryadey","Additional details: My Python environment has tokenizers==0.19.1 and transformers==4.44.2.","Sep 3, 2024 10:28 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"What is based model of XLM-RoBERTa Tokenizer? SenetencePiece? XLNetTokenizer","https://discuss.huggingface.co/t/what-is-based-model-of-xlm-roberta-tokenizer-senetencepiece-xlnettokenizer/106443","0","16","Created: Sep 12, 2024 9:37 am","JoonseoHyeon","In the offcial document of XLM-RoBERTa:> Adapted from RobertaTokenizer and XLNetTokenizer. Based on SentencePiece. And each RobertaTokenizer and XLNetTokenizer are descripted below: Constructs a RoBERTa tokenizer, derived from the GPT-2 tokenizer, using byte-level Byte-Pair-Encoding. Construct an XLNet tokenizer. Based on SentencePiece. And the description in article XLM-RoBERTa about tokenization is like below: The different language-specific tokenization tools used by mBERT and XLM-100 make these models more difficult to use on raw text. Instead, we train a Sentence Piece model (SPM) and apply it directly on raw text data for all languages. We did not observe any loss in performance for models trained with SPM when compared to models trained with language-specific preprocessing and byte-pair encoding and hence use SPM for XLM-R. Conneau, A. ‚ÄúUnsupervised cross-lingual representation learning at scale.‚Äù arXiv preprint arXiv:1911.02116 (2019). I‚Äôm confused what is real XLM-RoBERTa Tokenizer builted. SentencePiece Tokenizer? XLNet Tokenizer? Or am I wrong?","Sep 12, 2024 9:37 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Can I retrain GPT-2 tokeniser on Chinese data and use it with GPT-2 XL or other models to create a Chinese-speaking model?","https://discuss.huggingface.co/t/can-i-retrain-gpt-2-tokeniser-on-chinese-data-and-use-it-with-gpt-2-xl-or-other-models-to-create-a-chinese-speaking-model/102333","0","14","Created: Aug 14, 2024 11:32 am","sidharthsajith7","Hi everyone, I‚Äôm trying to retrain the GPT-2 tokeniser on a vast amount of Chinese data to adapt it to the Chinese language. My goal is to use this retrained tokeniser with a pre-trained GPT-2 XL or other models to create a Chinese-speaking model. My question is: will this approach work? Can I simply retrain the GPT-2 tokeniser on Chinese data and then use it with a pre-trained GPT-2 XL or other models to create a Chinese-speaking model? Or are there additional steps I need to take to ensure the model can understand and generate Chinese text? I‚Äôve tried searching for similar questions and tutorials, but I couldn‚Äôt find any clear guidance on this specific topic. I‚Äôd appreciate any insights or advice from the community on whether this approach is feasible and what additional steps I might need to take to make it work. Some specific questions I have are: Will the retrained tokeniser be compatible with the pre-trained GPT-2 XL or other models? Are there any specific preprocessing steps I need to take when working with Chinese text? Are there any known issues or limitations when using a retrained tokeniser with pre-trained models? Any help or guidance would be greatly appreciated!","Aug 14, 2024 11:32 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Help to choose decoder for devnagari ocr","https://discuss.huggingface.co/t/help-to-choose-decoder-for-devnagari-ocr/107442","0","13","Created: Sep 20, 2024 5:13 am","nhujaw07","I am trying to train Devnagari handwritten OCR but i am unable to get proper txt output. Can some one suggest which encoder and decoder should I use?","Sep 20, 2024 5:13 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Call rust function in python","https://discuss.huggingface.co/t/call-rust-function-in-python/103446","0","8","Created: Aug 22, 2024 6:01 am","Boltzmachine","I want to write a rust function that takes the python tokenizer (the tokenizer might be set in python code). for example, define this in rust fn foo(tokenizer: PyTokenizer, ...) { ... } and call it in python tokenizer = AutoTokenizer.from_pretrained(...) # set tokenizer, e.g. pad_id foo(tokenizer, ...) Is there any guide to achieve this","Aug 22, 2024 6:01 am","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
